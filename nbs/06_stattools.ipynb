{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b13c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp statstools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b438c54",
   "metadata": {},
   "source": [
    "# Statstools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96f3ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from pyexpat import model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from numba import jit\n",
    "from scipy.stats import boxcox\n",
    "from scipy.special import inv_boxcox\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from numba import jit\n",
    "##Stationarity Check\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK, space_eval\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statistics import NormalDist\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "from statsmodels.tsa.seasonal import STL, MSTL\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "from statsmodels.tsa.stattools import ccf\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Unit Root Test and Serial Correlation Check\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def unit_root_test(series, method = \"ADF\", n_lag = None):\n",
    "    \"\"\"\n",
    "    Performs a unit root test on the given time series data to check for stationarity.\n",
    "    Args:\n",
    "        series (pd.Series): The time series data to be tested.\n",
    "        method (str): The method for the unit root test, either \"ADF\" for Augmented Dickey-Fuller or \"KPSS\" for Kwiatkowski-Phillips-Schmidt-Shin.\n",
    "        n_lag (int, optional): The number of lags to include in the test. If None, the default lag will be used.\n",
    "    Returns:\n",
    "        float: The p-value from the unit root test.\n",
    "    \"\"\"\n",
    "    if method == \"ADF\":\n",
    "        if n_lag ==None:\n",
    "            adf = adfuller(series)[1]\n",
    "        else:\n",
    "            adf = adfuller(series, maxlag = n_lag)[1]        \n",
    "        if adf < 0.05:\n",
    "            return adf, print('ADF p-value: %f' % adf + \" and data is stationary at 5% significance level\")\n",
    "        else:\n",
    "            return adf, print('ADF p-value: %f' % adf + \" and data is non-stationary at 5% significance level\")\n",
    "    elif method == \"KPSS\":\n",
    "        if n_lag == None:\n",
    "            kps = kpss(series)[1]\n",
    "        else:\n",
    "            kps = kpss(series, nlags = n_lag)[1]\n",
    "        if kps < 0.05:\n",
    "            return kps, print('KPSS p-value: %f' % kps + \" and data is non-stationary at 5% significance level\")\n",
    "        else:\n",
    "            return kps, print('KPSS p-value: %f' % kps + \" and data is stationary at 5% significance level\")\n",
    "    else:\n",
    "        return print('Enter a valid unit root test method')\n",
    "\n",
    "## Cross Corelation Check\n",
    "\n",
    "def cross_autocorrelation(x, y, nlags, adjusted=True, alpha=None, bartlett_confint=False):\n",
    "    \"\"\"\n",
    "    Compute the cross-autocorrelation between two time series.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array_like\n",
    "        First time series.\n",
    "    y : array_like\n",
    "        Second time series.\n",
    "    nlags : int\n",
    "        Number of lags to compute.\n",
    "    adjusted : bool, optional\n",
    "        Whether to apply the adjustment factor (default is True).\n",
    "    alpha : float, optional\n",
    "        Significance level for confidence intervals (default is None).\n",
    "    bartlett_confint : bool, optional\n",
    "        Whether to use Bartlett's method for confidence intervals (default is False).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cc : ndarray\n",
    "        Cross-autocorrelation values for each lag and confidence intervals if `alpha` is provided.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    x_mean = np.mean(x)\n",
    "    y_mean = np.mean(y)\n",
    "    n = len(x)\n",
    "    if len(y) != n:\n",
    "        raise ValueError(\"x and y must have the same length\")\n",
    "    \n",
    "    # Variance (autocovariance at lag 0)\n",
    "    var_x = np.sum((x - x_mean)**2)\n",
    "    var_y = np.sum((y - y_mean)**2)\n",
    "\n",
    "    # Autocovariance but make sure make adjusted, meaning applying 1/(n-k) or 1/n\n",
    "    cc = np.empty(nlags)\n",
    "    for k in range(nlags):\n",
    "        num = np.sum((y[:n-k] - y_mean) * (x[k:] - x_mean))\n",
    "        r = num / np.sqrt(var_x * var_y)\n",
    "        if adjusted and k > 0:\n",
    "            r *= n / (n - k)\n",
    "        cc[k] = r\n",
    "\n",
    "    # Confidence intervals (optional)\n",
    "    if alpha is not None:\n",
    "        z = NormalDist().inv_cdf(1 - alpha/2)\n",
    "        se = np.zeros_like(cc)\n",
    "        \n",
    "        if bartlett_confint:\n",
    "            # Bartlett approximation: SE_k = sqrt((1 + 2 * sum_{j=1}^{k-1} cc[j]^2) / n)\n",
    "            if nlags >= 1:\n",
    "                r_sq_cum = np.cumsum(cc[1:]**2)\n",
    "                prev_sum = np.concatenate(([0.0], r_sq_cum[:-1]))\n",
    "                se[1:] = np.sqrt((1.0 + 2.0 * prev_sum) / n)\n",
    "            se[0] = 0.0\n",
    "        else:\n",
    "            se = np.array([1.0 / np.sqrt(n) for _ in range(nlags)])\n",
    "\n",
    "        confint = np.column_stack((cc - z*se, cc + z*se))\n",
    "        return cc, confint\n",
    "    else:\n",
    "        return cc, None \n",
    "    \n",
    "\n",
    "\n",
    "def pacf_strength(series, alpha=0.05, n_lags=5, adjusted=True):\n",
    "    \"\"\"\n",
    "    Calculate the pacf scores for the partial autocorrelation function (PACF) of a time series to identify powerful significant lags.\n",
    "    It is calculated as (PACF value - bound) / bound for positive exceedances and (PACF value + bound) / bound for negative exceedances.\n",
    "\n",
    "    Parameters:\n",
    "    - series: The input time series data.\n",
    "    - alpha: Significance level for the confidence intervals.\n",
    "    - n_lags: Number of lags to consider for the PACF.\n",
    "    - adjusted: Whether to use an adjusted bound for the PACF.\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame containing the exceedance scores for each lag. Also includes the absolute scores\n",
    "    \"\"\"\n",
    "    \n",
    "    pacf_vals = pacf(series, nlags=n_lags)\n",
    "    n= len(series)   \n",
    "    z = NormalDist().inv_cdf(1 - alpha/2)\n",
    "    bound = z / np.sqrt(n - n_lags) if adjusted else z / np.sqrt(n)\n",
    "    exceed_score = []\n",
    "    for i, j in enumerate(pacf_vals):\n",
    "        # print(i)\n",
    "        if j > bound:\n",
    "            exceed_score.append([i, (j-bound)/bound, j, bound])\n",
    "\n",
    "        elif j < -bound:\n",
    "            exceed_score.append([i, (j+bound)/bound, j, -bound])\n",
    "        else:\n",
    "            exceed_score.append([i, 0, j, bound])\n",
    "    exceed_score = pd.DataFrame(exceed_score)\n",
    "    exceed_score.columns = [\"lags\", \"pacf_score\", \"pacf_value\", \"z_bound\"]\n",
    "    exceed_score[\"abs_pacf_score\"] = exceed_score[\"pacf_score\"].abs()\n",
    "    exceed_score = exceed_score.sort_values(by=\"abs_pacf_score\", ascending=False)\n",
    "    exceed_score = exceed_score[exceed_score[\"abs_pacf_score\"] > 0]\n",
    "    exceed_score.index.name = \"lag\"\n",
    "    exceed_score = exceed_score[1:]  # remove lag 0\n",
    "\n",
    "    return exceed_score\n",
    "\n",
    "\n",
    "def ccf_strength(x, y, alpha=0.05, n_lags=5, adjusted=True):\n",
    "    \"\"\"\n",
    "    Calculate exceedance scores for the cross-correlation function (CCF).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y : array-like\n",
    "        Input time series.\n",
    "    alpha : float\n",
    "        Significance level for CI.\n",
    "    n_lags : int\n",
    "        Number of lags to consider.\n",
    "    adjusted : bool\n",
    "        If True, use lag-specific CI (sqrt(n-k)); \n",
    "        if False, use fixed CI (sqrt(n)).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        Exceedance scores for each lag (excluding lag 0).\n",
    "    \"\"\"\n",
    "    n = len(y)\n",
    "    z = NormalDist().inv_cdf(1 - alpha/2)\n",
    "    ccr_values = ccf(x, y)[: n_lags + 1]\n",
    "\n",
    "    exceed_score = []\n",
    "    for k, j in enumerate(ccr_values):\n",
    "        if adjusted:\n",
    "            bound = z / np.sqrt(n - k)\n",
    "        else:\n",
    "            bound = z / np.sqrt(n)\n",
    "\n",
    "        if j > bound:\n",
    "            exceed_score.append([k, (j - bound) / bound, j, bound])\n",
    "        elif j < -bound:\n",
    "            exceed_score.append([k, (j + bound) / bound, j, -bound])\n",
    "        else:\n",
    "            exceed_score.append([k, 0, j, bound])\n",
    "\n",
    "    exceed_score = pd.DataFrame(exceed_score, columns=[\"lags\", \"corr_score\", \"ccf_value\", \"z_bound\"])\n",
    "    exceed_score[\"abs_corr_score\"] = exceed_score[\"corr_score\"].abs()\n",
    "\n",
    "    # drop lag 0 (auto-correlation with itself)\n",
    "    exceed_score = exceed_score.loc[exceed_score[\"lags\"] != 0]\n",
    "    exceed_score = exceed_score[exceed_score[\"abs_corr_score\"] > 0]\n",
    "    exceed_score = exceed_score.sort_values(by=\"abs_corr_score\", ascending=False)\n",
    "\n",
    "    return exceed_score\n",
    "\n",
    "def lr_trend_model(series, breakpoints=None, type='linear', degree=1):\n",
    "    \"\"\"\n",
    "    Compute the piecewise trend of a time series using linear regression.\n",
    "    Args:\n",
    "        series (pd.Series): The input time series.\n",
    "        breakpoints (list): A list of breakpoints for the piecewise segments when type is \"piecewise\". It could be a list of indices.\n",
    "        type (str): The type of model (\"linear\" or \"piecewise\")\n",
    "        degree (int): The degree of the polynomial trend when type is \"linear\". Default is 1 (linear trend).\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Fitted trend values, LinearRegression model, and design time index matrix.\n",
    "    \"\"\" \n",
    "\n",
    "    T = np.arange(len(series), dtype=int)\n",
    "\n",
    "    if type == 'piecewise' and breakpoints is not None:\n",
    "        # Base regressor (time itself)\n",
    "        X_trend = T.reshape(-1, 1)\n",
    "\n",
    "        # Add hinge terms for each breakpoint\n",
    "        for bp in breakpoints:\n",
    "            hinge = np.maximum(0, T - bp).reshape(-1, 1)\n",
    "            X_trend = np.hstack([X_trend, hinge])\n",
    "\n",
    "        model_lr = LinearRegression().fit(X_trend, np.array(series))\n",
    "        trend = model_lr.predict(X_trend)\n",
    "    else:\n",
    "        if degree == 1:\n",
    "            X_trend = T.reshape(-1, 1)\n",
    "        elif degree != 1 and isinstance(degree, int):\n",
    "            X_trend = np.column_stack([T**i for i in range(1, degree+1)])\n",
    "        elif isinstance(degree, list):\n",
    "            X_trend = np.column_stack([T**i for i in degree])\n",
    "        else:\n",
    "            raise ValueError(\"Degree must be a positive integer.\")\n",
    "        model_lr = LinearRegression().fit(X_trend, np.array(series))\n",
    "        trend = model_lr.predict(X_trend)\n",
    "    return trend, model_lr, X_trend\n",
    "\n",
    "\n",
    "def forecast_trend(model, H, start, degree = 1, breakpoints=None):\n",
    "    \"\"\"\n",
    "    Forecast future trend values using the fitted model.\n",
    "    Args:\n",
    "        model (LinearRegression): The fitted linear regression model.\n",
    "        breakpoints (list): A list of breakpoints for the piecewise segments.\n",
    "        H (int): The forecast horizon.\n",
    "        start (int): The starting point for the forecast.\n",
    "        degree (int): The degree of the polynomial trend when type is \"linear\". Default is 1 (linear trend).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The forecasted trend values.\n",
    "    \"\"\"\n",
    "    TH = np.arange(start, start + H, dtype=int)\n",
    "    if degree == 1:\n",
    "        T_future = TH.reshape(-1, 1)\n",
    "    elif degree != 1 and isinstance(degree, int):\n",
    "        T_future = np.column_stack([TH**i for i in range(1, degree+1)])\n",
    "    elif isinstance(degree, list):\n",
    "        T_future = np.column_stack([TH**i for i in degree])\n",
    "    else:\n",
    "        raise ValueError(\"Degree must be a positive integer.\")\n",
    "\n",
    "    # Build future design matrix\n",
    "    X_future = T_future\n",
    "    if breakpoints is not None:\n",
    "        for bp in breakpoints:\n",
    "            hinge = np.maximum(0, T_future - bp).reshape(-1, 1) #\n",
    "            X_future = np.hstack([X_future, hinge])\n",
    "\n",
    "    return model.predict(X_future), X_future # Forecasted trend values and future design matrix\n",
    "\n",
    "\n",
    "def trend_strength(series, **kwargs):\n",
    "    \"\"\"\n",
    "    Compute the strength of the trend component in a time series using Hyndman formula.\n",
    "    Args:\n",
    "        series (pd.Series): The time series data.\n",
    "        **kwargs: Additional arguments passed to the STL decomposition. For example, you can specify the period, seasonal and/or trend components.\n",
    "\n",
    "    \"\"\"\n",
    "    res = STL(series, **kwargs).fit()\n",
    "    return np.max(1-np.var(res.resid)/(np.var(res.resid+res.trend)), 0)\n",
    "\n",
    "def seasonality_strength(series, **kwargs):\n",
    "    \"\"\"\n",
    "    Compute the strength of the seasonal component in a time series using Hyndman formula.\n",
    "    Args:\n",
    "        series (pd.Series): The time series data.\n",
    "        **kwargs: Additional arguments passed to the STL decomposition. For example, you can specify the period, seasonal and/or trend components.\n",
    "    \"\"\"\n",
    "    res = STL(series, **kwargs).fit()\n",
    "    return np.max(1-np.var(res.resid)/(np.var(res.resid+res.seasonal)), 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
