{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fe95ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045fe2ec",
   "metadata": {},
   "source": [
    "# Model selection and tuning methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115e5531",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from pyexpat import model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from numba import jit\n",
    "from scipy.stats import boxcox\n",
    "from scipy.special import inv_boxcox\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from numba import jit\n",
    "##Stationarity Check\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK, space_eval\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statistics import NormalDist\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from tqdm import tqdm_notebook\n",
    "from itertools import product\n",
    "from typing import List, Dict, Optional, Callable, Tuple, Any, Union\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Feature Selection Algorithms\n",
    "# ------------------------------------------------------------------------------\n",
    "            \n",
    "\n",
    "def forward_feature_selection(df, n_folds = None, H = None, model = None, metrics = None,\n",
    "                                  lags_to_consider = None, candidate_features = None, transformations = None, \n",
    "                                    step_size = None, starting_lags = None, starting_transforms = None, verbose = False):\n",
    "    \"\"\"\n",
    "    Performs forward lag/feature/transform selection for Regression models.\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing the time series data.\n",
    "        n_folds (int, optional): Number of cross-validation folds.\n",
    "        H (int, optional): Forecast horizon.\n",
    "        model: Model to be used for training and evaluation.\n",
    "        metrics (list, optional): List of metrics to evaluate the model.\n",
    "        lags_to_consider (list, optional): List of lags to consider for feature selection.\n",
    "        candidate_features (list, optional): List of candidate exogenous features.\n",
    "        transformations (list, optional): List of transformations to apply.\n",
    "        starting_lags (list, optional): List of starting lags.\n",
    "        starting_transforms (list, optional): List of starting transformations.\n",
    "        step_size (int, optional): Step size for rolling window.\n",
    "        verbose (bool, optional): Whether to print progress messages.\n",
    "    Returns:\n",
    "        dict: Dictionary of best features\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if lags_to_consider is not None:\n",
    "        remaining_lags = list(range(1, lags_to_consider + 1))\n",
    "        model.n_lag = None # Start with no lags\n",
    "\n",
    "        if starting_lags is not None:\n",
    "            if not isinstance(starting_lags, list):\n",
    "                raise ValueError(\"starting_lags should be a list of integers.\")\n",
    "            model.n_lag = starting_lags\n",
    "            remaining_lags = [x for x in remaining_lags if x not in starting_lags]\n",
    "\n",
    "    if candidate_features is not None:\n",
    "        df_orig = df.copy() # Keep original for feature add-back\n",
    "        df = df.drop(columns=candidate_features)\n",
    "\n",
    "    if transformations is not None:\n",
    "        if starting_transforms is not None:\n",
    "            if not isinstance(starting_transforms, list):\n",
    "                raise ValueError(\"starting_transforms should be a list of transformation instances.\")\n",
    "            model.lag_transform = starting_transforms\n",
    "            transformations = [x for x in transformations if x not in starting_transforms]\n",
    "        else:\n",
    "            model.lag_transform = None\n",
    "            \n",
    "\n",
    "    best_features = {\n",
    "    \"best_lags\": list(starting_lags) if starting_lags is not None else [],\n",
    "    \"best_exogs\": [],\n",
    "    \"best_transforms\": list(starting_transforms) if starting_transforms is not None else []}\n",
    "    best_score = [float('inf')] * len(metrics)\n",
    "\n",
    "    if isinstance(metrics, list):\n",
    "        best_score = [float('inf')] * len(metrics)\n",
    "    else:\n",
    "        best_score = float('inf')\n",
    "\n",
    "    if isinstance(best_score, list):\n",
    "        def is_elementwise_improvement(score, best_s):\n",
    "            return all(s < b for s, b in zip(score, best_s))\n",
    "    else:\n",
    "        def is_elementwise_improvement(score, best_s):\n",
    "            return score < best_s\n",
    "        \n",
    "    def validation(model_test, df_test):    \n",
    "        cv_result = cross_validate(model=model_test, df=df_test, cv_split=n_folds,\n",
    "                            test_size=H, metrics=metrics, step_size=step_size)\n",
    "\n",
    "        if isinstance(metrics, list):\n",
    "            return cv_result[\"score\"].tolist()\n",
    "        else:\n",
    "            return cv_result[\"score\"].values[0]\n",
    "    # Initial score with starting features\n",
    "    if starting_lags is not None or starting_transforms is not None:\n",
    "        model_start = model.copy()\n",
    "        best_score = validation(model_start, df)\n",
    "\n",
    "    while True:\n",
    "        improvement = False\n",
    "        candidate = {'type': None, 'name': None}\n",
    "        scores = best_score\n",
    "\n",
    "        # Test Lags\n",
    "        if lags_to_consider is not None:\n",
    "            for lag in remaining_lags:\n",
    "                current_lags = sorted(best_features[\"best_lags\"] + [lag])\n",
    "                model_test = model.copy()\n",
    "                model_test.n_lag = current_lags\n",
    "                score = validation(model_test, df)\n",
    "                # print(f'testing lag: {lag} with score: {score}')\n",
    "                if  is_elementwise_improvement(score, scores):\n",
    "                    scores = score\n",
    "                    candidate = {'type': 'lag', 'name': lag}\n",
    "                    # print(candidate[\"type\"], candidate[\"name\"], score)\n",
    "                    improvement = True\n",
    "\n",
    "        # Test Exogenous Features\n",
    "        if candidate_features is not None:\n",
    "            for feat in candidate_features:\n",
    "                df_test = df.copy()\n",
    "                df_test[feat] = df_orig[feat]\n",
    "                model_test = model.copy()\n",
    "                score = validation(model_test, df_test)\n",
    "                if is_elementwise_improvement(score, scores):\n",
    "                    scores = score\n",
    "                    candidate = {'type': 'exog', 'name': feat}\n",
    "                    improvement = True\n",
    "\n",
    "        # Test Transformations\n",
    "        if transformations is not None:\n",
    "            for trans in transformations:\n",
    "                model_test = model.copy()\n",
    "                lag_transform = (model_test.lag_transform or []) + [trans]\n",
    "                model_test.lag_transform = lag_transform\n",
    "                score = validation(model_test, df)\n",
    "                # print(f'testing transformation: {trans.get_name()} with score: {score}')\n",
    "                if is_elementwise_improvement(score, scores):\n",
    "                    scores = score\n",
    "                    candidate = {'type': 'transform', 'name': trans}\n",
    "                    # print(candidate[\"type\"], candidate[\"name\"].get_name(), score)\n",
    "                    improvement = True\n",
    "\n",
    "        # Update best features\n",
    "        if improvement:\n",
    "            best_score = scores\n",
    "            if candidate['type'] == 'lag':\n",
    "                best_features[\"best_lags\"].append(candidate['name'])\n",
    "                remaining_lags.remove(candidate['name'])\n",
    "            elif candidate['type'] == 'exog':\n",
    "                best_features[\"best_exogs\"].append(candidate['name'])\n",
    "                candidate_features.remove(candidate['name'])\n",
    "                df[candidate['name']] = df_orig[candidate['name']]\n",
    "            elif candidate['type'] == 'transform':\n",
    "                best_features[\"best_transforms\"].append(candidate['name'])\n",
    "                transformations.remove(candidate['name'])\n",
    "                if model.lag_transform is None:\n",
    "                    model.lag_transform = [candidate['name']]\n",
    "                else:\n",
    "                    model.lag_transform.append(candidate['name'])\n",
    "\n",
    "            if verbose:\n",
    "                if candidate['type'] == 'transform':\n",
    "                    print(f\"Added {candidate['type']}: {candidate['name'].get_name()} with score: {best_score}\")\n",
    "                else:\n",
    "                    print(f\"Added {candidate['type']}: {candidate['name']} with score: {best_score}\")\n",
    "        else:\n",
    "            break  # No improvement\n",
    "\n",
    "    if transformations is not None and best_features[\"best_transforms\"]:\n",
    "        best_features[\"best_transforms\"] = [trans.get_name() for trans in best_features[\"best_transforms\"]]\n",
    "        \n",
    "    if lags_to_consider is not None and best_features[\"best_lags\"]:\n",
    "        best_features[\"best_lags\"].sort()\n",
    "\n",
    "    return best_features\n",
    "\n",
    "\n",
    "\n",
    "def backward_feature_selection(df, n_folds = None, H = None, model = None, metrics = None,\n",
    "                                  lags_to_consider = None, candidate_features = None, transformations = None, \n",
    "                                    step_size = None, verbose = False):\n",
    "    \"\"\"\n",
    "    Performs backward lag selection for Regression models.\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing the time series data.\n",
    "        n_folds (int, optional): Number of cross-validation folds.\n",
    "        H (int, optional): Forecast horizon.\n",
    "        model: Model to be used for training and evaluation.\n",
    "        metrics (list, optional): List of metrics to evaluate the model.\n",
    "        lags_to_consider (list, optional): List of lags to consider for feature selection.\n",
    "        candidate_features (list, optional): List of candidate exogenous features.\n",
    "        transformations (list, optional): List of transformations to apply.\n",
    "        step_size (int, optional): Step size for rolling window.\n",
    "        verbose (bool, optional): Whether to print progress messages.\n",
    "    Returns:\n",
    "        dict: Dictionary of best features\n",
    "    \"\"\"\n",
    "    remaining_lags = list(range(1, lags_to_consider + 1)) if lags_to_consider is not None else []\n",
    "    candidate_features = candidate_features.copy() if candidate_features is not None else []\n",
    "    transformations = transformations.copy() if transformations is not None else None\n",
    "    best_features = {\"best_lags\": remaining_lags, \"best_exogs\": candidate_features, \"best_transforms\": transformations}\n",
    "\n",
    "    ## setting the full model\n",
    "    # model_full = model.copy()\n",
    "    if lags_to_consider is not None:\n",
    "        model.n_lag = remaining_lags # Start with all lags to consider\n",
    "    if transformations is not None:\n",
    "        model.lag_transform = transformations # Start with all transformations\n",
    "\n",
    "    best_score = list(np.repeat(float('inf'), len(metrics)))\n",
    "\n",
    "    # best_lags = None\n",
    "    while True:\n",
    "        improvement = False\n",
    "        candidate = {'type': None, 'name': None}\n",
    "        scores = best_score\n",
    "        if best_features[\"best_lags\"]:\n",
    "            for lg in best_features[\"best_lags\"]:\n",
    "                lags_to_test = [x for x in best_features[\"best_lags\"] if x != lg]\n",
    "                lags_to_test.sort()\n",
    "                model_test = model.copy()\n",
    "                model_test.n_lag = lags_to_test\n",
    "                my_cv = cross_validate(model=model_test, df=df, cv_split=n_folds,\n",
    "                                    test_size=H, metrics=metrics, step_size=step_size)\n",
    "                score = my_cv[\"score\"].tolist()\n",
    "                # print(f\"len of lags_to_test: {len(lags_to_test)} and score: {score}\")\n",
    "                if score < scores:\n",
    "                    scores = score\n",
    "                    candidate = {'type': 'lag', 'name': lg}\n",
    "                    improvement = True\n",
    "        if best_features[\"best_transforms\"]:\n",
    "            for trans in best_features[\"best_transforms\"]:\n",
    "                trans_to_test = [x for x in best_features[\"best_transforms\"] if x != trans]\n",
    "                model_test = model.copy()\n",
    "                model_test.lag_transform = trans_to_test\n",
    "                my_cv = cross_validate(model=model_test, df=df, cv_split=n_folds,\n",
    "                                    test_size=H, metrics=metrics, step_size=step_size)\n",
    "                score = my_cv[\"score\"].tolist()\n",
    "                if score < scores:\n",
    "                    scores = score\n",
    "                    candidate = {'type': 'transform', 'name': trans}\n",
    "                    improvement = True\n",
    "        if best_features[\"best_exogs\"]:\n",
    "            for feat in best_features[\"best_exogs\"]:\n",
    "                # feat_to_test = [x for x in candidate_features if x != feat]\n",
    "                df_test = df.drop(columns=feat)\n",
    "                model_test = model.copy()\n",
    "                my_cv = cross_validate(model=model_test, df=df_test, cv_split=n_folds,\n",
    "                                    test_size=H, metrics=metrics, step_size=step_size)\n",
    "                score = my_cv[\"score\"].tolist()\n",
    "                if score < scores:\n",
    "                    scores = score\n",
    "                    candidate = {'type': 'exog', 'name': feat}\n",
    "                    improvement = True\n",
    "\n",
    "        # Update best features\n",
    "        if improvement and candidate['type']:\n",
    "            best_score = scores\n",
    "            if candidate['type'] == 'lag':\n",
    "                best_features[\"best_lags\"].remove(candidate['name'])\n",
    "            elif candidate['type'] == 'exog':\n",
    "                best_features[\"best_exogs\"].remove(candidate['name'])\n",
    "                df = df.drop(columns=candidate['name'])\n",
    "            elif candidate['type'] == 'transform':\n",
    "                best_features[\"best_transforms\"].remove(candidate['name'])\n",
    "                if not best_features[\"best_transforms\"]:\n",
    "                    model.lag_transform = best_features[\"best_transforms\"]\n",
    "                else:\n",
    "                    model.lag_transform = None\n",
    "\n",
    "            if verbose:\n",
    "                if candidate['type'] == 'transform':\n",
    "                    print(f\"Removed {candidate['type']}: {candidate['name'].get_name()} with score: {best_score}\")\n",
    "                else:\n",
    "                    print(f\"Removed {candidate['type']}: {candidate['name']} with score: {best_score}\")\n",
    "        else:\n",
    "            break  # No improvement\n",
    "\n",
    "    if transformations is not None and best_features[\"best_transforms\"]:\n",
    "        best_features[\"best_transforms\"] = [trans.get_name() for trans in best_features[\"best_transforms\"]]\n",
    "    if lags_to_consider is not None and best_features[\"best_lags\"]:\n",
    "        best_features[\"best_lags\"].sort()\n",
    "    return best_features\n",
    "\n",
    "\n",
    "def mv_forward_feature_selection(df, target_col, n_folds = None, H = None, model = None, metrics = None,\n",
    "                                  lags_to_consider = None, candidate_features = None, transformations = None, \n",
    "                                    step_size = None, starting_lags = None, starting_transforms = None, verbose = False):\n",
    "    \"\"\"\n",
    "    Performs forward lag selection for multivariate models \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing the time series data.\n",
    "        target_col (str): The target column for accuracy evaluation.\n",
    "        n_folds (int): Number of folds for cross-validation.\n",
    "        H (int): Forecast horizon.\n",
    "        model: The forecasting model to be used.\n",
    "        metrics (list): List of metrics to evaluate the model.\n",
    "        lags_to_consider (dict): Dictionary of maximum lags for each variable.\n",
    "        candidate_features (list): List of candidate exogenous features.\n",
    "        transformations (list): List of transformations to consider.\n",
    "        step_size (int, optional): Step size for lag selection. Defaults to None.\n",
    "        starting_lags (dict, optional): Dictionary of starting lags for each variable. Defaults to None.\n",
    "        starting_transforms (dict, optional): Dictionary of starting transformations for each variable. Defaults to None.\n",
    "        verbose (bool, optional): Whether to print progress. Defaults to False.\n",
    "    Returns:\n",
    "        dict: Dictionary of best features for each variable.\n",
    "    \"\"\"\n",
    "\n",
    "    # max_lag = sum(x for x in max_lags.values())\n",
    "    \n",
    "    # lags = list(range(1, max_lags+1))\n",
    "\n",
    "    if lags_to_consider is None:\n",
    "        lags_to_consider = {}\n",
    "    if transformations is None:\n",
    "        transformations = {}\n",
    "\n",
    "    best_features = {\"best_lags\": {i: [] for i in lags_to_consider if lags_to_consider is not None}, \"best_transforms\": {i: [] for i in transformations if transformations is not None}, \"best_exogs\": []}\n",
    "    remaining_lags = {i:list(range(1, j+1)) for i, j in lags_to_consider.items()}\n",
    "\n",
    "    if starting_lags is not None:\n",
    "        for k, v in starting_lags.items():\n",
    "            all_lags = remaining_lags[k]\n",
    "            remaining_lags[k] = [x for x in all_lags if x not in v]\n",
    "            best_features[\"best_lags\"][k].extend(v)\n",
    "        \n",
    "    if lags_to_consider is not None:\n",
    "        model.n_lag = None # Start with no lags\n",
    "\n",
    "    # Drop candidate features initially\n",
    "    if candidate_features:\n",
    "        df = df.drop(columns=candidate_features) # Drop candidate features to start with feature selection\n",
    "        df_orig = df.copy() # Keep original for feature add-back\n",
    "    if transformations is not None:\n",
    "        if starting_transforms is not None:\n",
    "            for k, v in starting_transforms.items():\n",
    "                transformations[k] = [x for x in transformations if x not in v]\n",
    "                best_features[\"best_transforms\"][k].extend(v)\n",
    "            model.lag_transform = starting_transforms\n",
    "        else:\n",
    "            model.lag_transform = None # Start with no transformations\n",
    "\n",
    "    if isinstance(metrics, list):\n",
    "        best_score = [float('inf')] * len(metrics)\n",
    "    else:\n",
    "        best_score = float('inf')\n",
    "\n",
    "    if isinstance(best_score, list):\n",
    "        def is_elementwise_improvement(score, best_s):\n",
    "            return all(s < b for s, b in zip(score, best_s))\n",
    "    else:\n",
    "        def is_elementwise_improvement(score, best_s):\n",
    "            return score < best_s\n",
    "\n",
    "    def validation(model_test, df_test):\n",
    "        # cv_result = mv_cross_validate(model=model_test, df=df_test, cv_split=n_folds,\n",
    "        #                                     test_size=H, metrics=metrics, step_size=step_size)\n",
    "        cv_result = var_cross_validate(model=model_test, df= df_test,\n",
    "                                       target_col=target_col, step_size=step_size, cv_split=n_folds,\n",
    "                                       test_size=H, metrics=metrics)\n",
    "    \n",
    "        if isinstance(metrics, list):\n",
    "            return cv_result[\"score\"].tolist()\n",
    "        else:\n",
    "            return cv_result[\"score\"].values[0]\n",
    "\n",
    "    # while max_lag>0:\n",
    "    while True:\n",
    "        improvement = False\n",
    "        candidate = {'target': None, 'type': None, 'name': None}\n",
    "        scores = best_score\n",
    "        if lags_to_consider is not None:\n",
    "            for k, lg in remaining_lags.items():\n",
    "                for x in lg:\n",
    "                    model_test = model.copy()\n",
    "                    current_lag = {a:b for a, b in best_features['best_lags'].items()}\n",
    "                    current_lag[k] = best_features['best_lags'][k] + [x]\n",
    "                    current_lag[k].sort()\n",
    "                    model_test.n_lag = current_lag\n",
    "                    score = validation(model_test, df)\n",
    "                    if is_elementwise_improvement(score, scores):\n",
    "                        scores = score\n",
    "                        candidate = {'target': k, 'type': 'lag', 'name': x}\n",
    "                        improvement = True\n",
    "\n",
    "        # Test Exogenous Features\n",
    "        if candidate_features is not None:\n",
    "            for feat in candidate_features:\n",
    "                df_test = df.copy()\n",
    "                df_test[feat] = df_orig[feat]\n",
    "                model_test = model.copy()\n",
    "                score = validation(model_test, df_test)\n",
    "                if is_elementwise_improvement(score, scores):\n",
    "                    scores = score\n",
    "                    candidate = {'target': None, 'type': 'exog', 'name': feat}\n",
    "                    improvement = True\n",
    "\n",
    "            # Test Transformations\n",
    "        if transformations is not None:\n",
    "            for k, trans in transformations.items():\n",
    "                for t in trans:\n",
    "                    model_test = model.copy()\n",
    "                    lag_transform = (model_test.lag_transform[k] or []) + [t]\n",
    "                    model_test.lag_transform[k] = lag_transform\n",
    "                    score = validation(model_test, df)\n",
    "                    if is_elementwise_improvement(score, scores):\n",
    "                        scores = score\n",
    "                        candidate = {'target': k, 'type': 'transform', 'name': t}\n",
    "                        improvement = True\n",
    "\n",
    "        # Update best features\n",
    "        if improvement:\n",
    "            best_score = scores\n",
    "            if candidate['type'] == 'lag':\n",
    "                best_features[\"best_lags\"][candidate['target']].append(candidate['name']) # store lags by target\n",
    "                remaining_lags[candidate['target']].remove(candidate['name'])\n",
    "            elif candidate['type'] == 'exog':\n",
    "                best_features[\"best_exogs\"].append(candidate['name'])\n",
    "                candidate_features.remove(candidate['name'])\n",
    "                df[candidate['name']] = df_orig[candidate['name']]\n",
    "            elif candidate['type'] == 'transform':\n",
    "                best_features[\"best_transforms\"][candidate['target']].append(candidate['name'])\n",
    "                transformations[candidate['target']].remove(candidate['name'])\n",
    "                if model.lag_transform is None:\n",
    "                    transform_dict = {candidate['target']: [candidate['name']]}\n",
    "                    model.lag_transform = transform_dict\n",
    "                else:\n",
    "                    if candidate['target'] not in model.lag_transform:\n",
    "                        model.lag_transform[candidate['target']] = [candidate['name']]\n",
    "                    else:\n",
    "                        model.lag_transform[candidate['target']].append(candidate['name'])\n",
    "\n",
    "            if verbose:\n",
    "                if candidate['type'] == 'transform':\n",
    "                    print(f\"Added {candidate['type']} for target {candidate['target']}: {candidate['name'].get_name()} with score: {best_score}\")\n",
    "                else:\n",
    "                    print(f\"Added {candidate['type']} for target {candidate['target']}: {candidate['name']} with score: {best_score}\")\n",
    "        else:\n",
    "            break  # No improvement\n",
    "\n",
    "    if transformations is not None:\n",
    "        for key, trans in best_features[\"best_transforms\"].items():\n",
    "            if trans:  # only process non-empty lists\n",
    "                best_features[\"best_transforms\"][key] = [t.get_name() for t in trans]\n",
    "\n",
    "    if lags_to_consider is not None:\n",
    "        # sort the lags for each variable\n",
    "        for key in best_features[\"best_lags\"]:\n",
    "            best_features[\"best_lags\"][key].sort()\n",
    "\n",
    "    return best_features\n",
    "\n",
    "\n",
    "def mv_backward_feature_selection(df, target_col, n_folds = None, H = None, model = None, metrics = None,\n",
    "                                  lags_to_consider = None, candidate_features = None, transformations = None, \n",
    "                                    step_size = None, verbose = False):\n",
    "    \"\"\"\n",
    "    Performs backward lag selection for multivariate models.\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing the time series data.\n",
    "        target_col (str): The target column for accuracy evaluation.\n",
    "        n_folds (int, optional): Number of cross-validation folds.\n",
    "        H (int, optional): Forecast horizon.\n",
    "        model: The forecasting model to be used.\n",
    "        metrics (list): List of metrics to evaluate the model.\n",
    "        step_size (int, optional): Step size for cross-validation. Defaults to None.\n",
    "        verbose (bool, optional): Whether to print progress. Defaults to False.\n",
    "    Returns:\n",
    "        dict: Dictionary of best features for each variable.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # remaining_lags = {i:list(range(1, j+1)) for i, j in lags_to_consider.items()}\n",
    "    # best_lags = {i:[] for i in max_lags}\n",
    "    best_features = {\n",
    "        \"best_lags\": {i: list(range(1, j+1)) for i, j in (lags_to_consider or {}).items()},\n",
    "        \"best_exogs\": candidate_features.copy() if candidate_features is not None else [],\n",
    "        \"best_transforms\": {i: j for i, j in (transformations or {}).items()}\n",
    "}\n",
    "    \n",
    "    ## setting the full model\n",
    "    if lags_to_consider is not None:\n",
    "        model.n_lag = best_features[\"best_lags\"]  # Start with all lags to consider\n",
    "    if transformations is not None:\n",
    "        model.lag_transform = best_features[\"best_transforms\"]  # Start with all transformations to consider\n",
    "    # exogenous variables should be in df before passing df\n",
    "    best_score = list(np.repeat(float('inf'), len(metrics)))\n",
    "    \n",
    "    while True:\n",
    "        improvement = False\n",
    "        candidate = {'target': None, 'type': None, 'name': None}\n",
    "        scores = best_score\n",
    "        if lags_to_consider is not None:\n",
    "            for targ_l, lags in best_features[\"best_lags\"].items():\n",
    "                for lg in lags:\n",
    "                    lags_to_test = {a:b for a, b in lags.items()}\n",
    "                    # Remove the current lag lg from current target\n",
    "                    lags_to_test[targ_l] = [x for x in lags if x != lg]\n",
    "                    lags_to_test[targ_l].sort()\n",
    "                    model_test = model.copy()\n",
    "                    model_test.n_lag = lags_to_test\n",
    "                    my_cv = mv_cross_validate(model=model_test, df=df, cv_split=n_folds,\n",
    "                                                         test_size=H, metrics=metrics, step_size=step_size)\n",
    "                    score = my_cv[target_col].tolist()\n",
    "                    if score < scores:\n",
    "                        scores = score\n",
    "                        candidate = {'target': targ_l, 'type': 'lag', 'name': lg}\n",
    "                        improvement = True\n",
    "        if transformations is not None:\n",
    "            for targ_t, trans in best_features[\"best_transforms\"].items():\n",
    "                for tr in trans:\n",
    "                    trans_to_test = {a:b for a, b in best_features[\"best_transforms\"].items()}\n",
    "                    trans_to_test[targ_t] = [x for x in trans if x != tr]\n",
    "                    model_test = model.copy()\n",
    "                    # model_test.lags = remaining_lags\n",
    "                    model_test.lag_transform = trans_to_test\n",
    "                    my_cv = mv_cross_validate(model=model_test, df=df, cv_split=n_folds,\n",
    "                                                         test_size=H, metrics=metrics, step_size=step_size)\n",
    "                    scores = my_cv[target_col].tolist()\n",
    "                    if score < scores:\n",
    "                        scores = score\n",
    "                        candidate = {'target': targ_t, 'type': 'transform', 'name': trans}\n",
    "                        improvement = True\n",
    "        if candidate_features is not None:\n",
    "            for feat in best_features[\"best_exogs\"]:\n",
    "                # feat_to_test = [x for x in candidate_features if x != feat]\n",
    "                df_test = df.drop(columns=feat)\n",
    "                model_test = model.copy()\n",
    "                model_test.data_prep(df_test) # update data preparation because if new lags to be consistent with coefficients\n",
    "                model_test.compute_coeffs() # update model coefficients because of new lags\n",
    "                my_cv = mv_cross_validate(model=model_test, df=df_test, cv_split=n_folds,\n",
    "                                                         test_size=H, metrics=metrics, step_size=step_size)\n",
    "                score = my_cv[target_col].tolist()\n",
    "                if score < scores:\n",
    "                    scores = score\n",
    "                    candidate = {'target': None, 'type': 'exog', 'name': feat}\n",
    "                    improvement = True\n",
    "\n",
    "        # Update best features\n",
    "        if improvement and candidate['type']:\n",
    "            best_score = scores\n",
    "            if candidate['type'] == 'lag':\n",
    "                best_features[\"best_lags\"][candidate['target']].remove(candidate['name'])\n",
    "            elif candidate['type'] == 'exog':\n",
    "                best_features[\"best_exogs\"].remove(candidate['name'])\n",
    "                df = df.drop(columns=candidate['name'])\n",
    "            elif candidate['type'] == 'transform':\n",
    "                best_features[\"best_transforms\"][candidate['target']].remove(candidate['name'])\n",
    "                if any(best_features[\"best_transforms\"][key] for key in best_features[\"best_transforms\"]):\n",
    "                    best_features[\"best_transforms\"] = {k: v for k, v in best_features[\"best_transforms\"].items() if not len(v) == 0}\n",
    "                    model.lag_transform = best_features[\"best_transforms\"]\n",
    "                else:\n",
    "                    model.lag_transform = None\n",
    "\n",
    "            if verbose:\n",
    "                if candidate['type'] == 'transform':\n",
    "                    print(f\"Removed {candidate['type']} for target {candidate['target']}: {candidate['name'].get_name()} with score: {best_score}\")\n",
    "                else:\n",
    "                    print(f\"Removed {candidate['type']} for target {candidate['target']}: {candidate['name']} with score: {best_score}\")\n",
    "        else:\n",
    "            break  # No improvement\n",
    "\n",
    "    # if transformations is not None and at least one key is not empty get their names\n",
    "    if transformations is not None:\n",
    "        for key, trans in best_features[\"best_transforms\"].items():\n",
    "            if trans:  # only process non-empty lists\n",
    "                best_features[\"best_transforms\"][key] = [t.get_name() for t in trans]\n",
    "    if lags_to_consider is not None:\n",
    "        # sort the lags for each variable\n",
    "        for key in best_features[\"best_lags\"]:\n",
    "            best_features[\"best_lags\"][key].sort()\n",
    "\n",
    "\n",
    "    return best_features\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Forward Feature Selection for HMM\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def hmm_forward_feature_selection(df, n_folds = None, H = None, model = None, metrics = None,\n",
    "                                  lags_to_consider = None, candidate_features = None, transformations = None, \n",
    "                                    step_size = None, starting_lags = None, starting_transforms = None,\n",
    "                                    validation_type = \"cv\", iterations = 10, verbose = False):\n",
    "    \"\"\"\n",
    "    Performs forward lag/feature/transform selection for Regression models.\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing the time series data.\n",
    "        n_folds (int, optional): Number of cross-validation folds.\n",
    "        H (int, optional): Forecast horizon.\n",
    "        model: Model to be used for training and evaluation.\n",
    "        metrics (list, optional): List of metrics to evaluate the model. Even one metric, should be provided in a list.\n",
    "        lags_to_consider (list, optional): List of lags to consider for feature selection.\n",
    "        candidate_features (list, optional): List of candidate exogenous features.\n",
    "        transformations (list, optional): List of transformations to apply.\n",
    "        step_size (int, optional): Step size for rolling window.\n",
    "        starting_lags (list, optional): List of starting lags.\n",
    "        starting_transforms (list, optional): List of starting transformations.\n",
    "        validation_type (str, optional): Type of validation to use (\"cv\", \"BIC\", \"AIC\" or both \"AIC_BIC\"). if \"AIC_BIC\" are both selected, the model will be evaluated using both criteria.\n",
    "        iterations (int, optional): Number of iterations for model fitting to update parameters.\n",
    "        verbose (bool, optional): Whether to print progress messages.\n",
    "    Returns:\n",
    "        dict: Dictionary of best features\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if lags_to_consider is not None:\n",
    "        if isinstance(lags_to_consider, int):\n",
    "            remaining_lags = list(range(1, lags_to_consider + 1))\n",
    "        elif isinstance(lags_to_consider, list):\n",
    "            remaining_lags = lags_to_consider\n",
    "        model.lags = None\n",
    "        if starting_lags is not None:\n",
    "            if not isinstance(starting_lags, list):\n",
    "                raise ValueError(\"starting_lags should be a list of integers.\")\n",
    "            model.lags = starting_lags\n",
    "            remaining_lags = [x for x in remaining_lags if x not in starting_lags]\n",
    "\n",
    "    if candidate_features is not None:\n",
    "        df_orig = df.copy() # Keep original for feature add-ba\n",
    "        df = df.drop(columns=candidate_features)\n",
    "    if transformations is not None:\n",
    "        if starting_transforms is not None:\n",
    "            if not isinstance(starting_transforms, list):\n",
    "                raise ValueError(\"starting_transforms should be a list of transformation instances.\")\n",
    "            model.lag_transform = starting_transforms\n",
    "            transformations = [x for x in transformations if x not in starting_transforms]\n",
    "        else:\n",
    "            model.lag_transform = None\n",
    "            \n",
    "    best_features = {\n",
    "    \"best_lags\": list(starting_lags) if starting_lags is not None else [],\n",
    "    \"best_exogs\": [],\n",
    "    \"best_transforms\": list(starting_transforms) if starting_transforms is not None else []}\n",
    "\n",
    "    if validation_type == \"cv\":\n",
    "        if isinstance(metrics, list):\n",
    "            best_score = [float('inf')] * len(metrics)\n",
    "        else:\n",
    "            best_score = float('inf')\n",
    "    elif validation_type in (\"BIC\", \"AIC\"):\n",
    "        best_score = float('inf')\n",
    "    elif validation_type == \"AIC_BIC\":\n",
    "        best_score = [float('inf')] * 2\n",
    "    else:\n",
    "        raise ValueError(\"Invalid validation_type. Choose from 'cv', 'BIC', 'AIC', or 'AIC_BIC'.\")\n",
    "\n",
    "    if isinstance(best_score, list):\n",
    "        def is_elementwise_improvement(score, best_s):\n",
    "            return all(s < b for s, b in zip(score, best_s))\n",
    "    else:\n",
    "        def is_elementwise_improvement(score, best_s):\n",
    "            return score < best_s\n",
    "\n",
    "# After each feature selection step iterate model to make sure parameters are updated like transition probabilities and stds\n",
    "    def model_update(model_test, df_test, iterations=iterations):\n",
    "        model_test.data_prep(df_test) # update data preparation because if new lags to be consistent with coefficients\n",
    "        model_test.compute_coeffs() # update model coefficients because of new lags\n",
    "        model_test.fit(df_test, n_iter=iterations) # update model parameters like transition probs and stds\n",
    "        return model_test\n",
    "    \n",
    "\n",
    "    def validation(model_test, df_test):\n",
    "        if validation_type == \"cv\":\n",
    "            cv_result = hmm_cross_validate(model=model_test, df=df_test, cv_split=n_folds, test_size=H,\n",
    "                                metrics=metrics, step_size=step_size, n_iter=iterations)\n",
    "    \n",
    "            if isinstance(metrics, list):\n",
    "                score = cv_result[\"score\"].tolist()\n",
    "            else:\n",
    "                score = cv_result[\"score\"].values[0]\n",
    "        elif validation_type == \"BIC\":\n",
    "            score = model_test.BIC\n",
    "        elif validation_type == \"AIC\":\n",
    "            score = model_test.AIC\n",
    "        elif validation_type == \"AIC_BIC\":\n",
    "            score = [model_test.AIC, model_test.BIC]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid validation_type. Choose from 'cv', 'BIC', 'AIC', or 'AIC_BIC'.\")\n",
    "\n",
    "        return score\n",
    "\n",
    "    # Initial score with starting features\n",
    "    if starting_lags is not None or starting_transforms is not None:\n",
    "        model_start = model.copy()\n",
    "        model_start = model_update(model_start, df)\n",
    "        best_score = validation(model_start, df)\n",
    "\n",
    "    while True:\n",
    "        improvement = False\n",
    "        candidate = {'type': None, 'name': None}\n",
    "        scores = best_score\n",
    "\n",
    "        # Test Lags\n",
    "        if lags_to_consider is not None:\n",
    "            for lag in remaining_lags:\n",
    "                current_lags = sorted(best_features[\"best_lags\"] + [lag])\n",
    "                model_test = model.copy()\n",
    "                model_test.lags = current_lags\n",
    "                model_test = model_update(model_test, df)\n",
    "                score = validation(model_test, df)\n",
    "                if is_elementwise_improvement(score, scores):\n",
    "                    scores = score\n",
    "                    candidate = {'type': 'lag', 'name': lag, 'model': model_test}\n",
    "                    improvement = True\n",
    "\n",
    "        # Test Exogenous Features\n",
    "        if candidate_features is not None:\n",
    "            for feat in candidate_features:\n",
    "                df_test = df.copy()\n",
    "                df_test[feat] = df_orig[feat]\n",
    "                model_test = model.copy()\n",
    "                model_test = model_update(model_test, df_test)\n",
    "                score = validation(model_test, df_test)\n",
    "                if is_elementwise_improvement(score, scores):\n",
    "                    scores = score\n",
    "                    candidate = {'type': 'exog', 'name': feat, 'model': model_test}\n",
    "                    improvement = True\n",
    "\n",
    "        # Test Transformations\n",
    "        if transformations is not None:\n",
    "            for trans in transformations:\n",
    "                model_test = model.copy()\n",
    "                lag_transform = (model_test.lag_transform or []) + [trans]\n",
    "                model_test.lag_transform = lag_transform\n",
    "                model_test = model_update(model_test, df)\n",
    "                score = validation(model_test, df)\n",
    "                if is_elementwise_improvement(score, scores):\n",
    "                    scores = score\n",
    "                    candidate = {'type': 'transform', 'name': trans, 'model': model_test}\n",
    "                    improvement = True\n",
    "\n",
    "        # Update best features\n",
    "        if improvement:\n",
    "            best_score = scores\n",
    "            if candidate['type'] == 'lag':\n",
    "                best_features[\"best_lags\"].append(candidate['name'])\n",
    "                remaining_lags.remove(candidate['name'])\n",
    "            elif candidate['type'] == 'exog':\n",
    "                best_features[\"best_exogs\"].append(candidate['name'])\n",
    "                candidate_features.remove(candidate['name'])\n",
    "                df[candidate['name']] = df_orig[candidate['name']]\n",
    "            elif candidate['type'] == 'transform':\n",
    "                best_features[\"best_transforms\"].append(candidate['name'])\n",
    "                transformations.remove(candidate['name'])\n",
    "                if model.lag_transform is None:\n",
    "                    model.lag_transform = [candidate['name']]\n",
    "                else:\n",
    "                    model.lag_transform.append(candidate['name'])\n",
    "            # update transition probs and stds of states\n",
    "\n",
    "            model.A = candidate['model'].A\n",
    "            model.stds = candidate['model'].stds\n",
    "            model.LL = candidate['model'].LL\n",
    "            model.pi = candidate['model'].pi\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Added {candidate['type']}: {candidate['name']} with score: {best_score} and loglik and BIC: {model.LL}, {model.BIC}\")\n",
    "        else:\n",
    "            break  # No improvement\n",
    "\n",
    "    # Finalize model with best features\n",
    "    model_ = model.copy()\n",
    "    if lags_to_consider is not None and best_features[\"best_lags\"]:\n",
    "        model_.lags = best_features[\"best_lags\"]\n",
    "    if transformations is not None and best_features[\"best_transforms\"]:\n",
    "        model_.lag_transform = best_features[\"best_transforms\"]\n",
    "    model_ = model_update(model_, df)\n",
    "\n",
    "\n",
    "    if transformations is not None and best_features[\"best_transforms\"]:\n",
    "        best_features[\"best_transforms\"] = [trans.get_name() for trans in best_features[\"best_transforms\"]]\n",
    "    \n",
    "    return best_features, model_\n",
    "\n",
    "\n",
    "\n",
    "def hmm_backward_feature_selection(df, n_folds = None, H = None, model = None, metrics = None,\n",
    "                                  lags_to_consider = None, candidate_features = None, transformations = None, \n",
    "                                    step_size = None, validation_type = \"cv\", iterations = 10, verbose = False):\n",
    "    \"\"\"\n",
    "    Performs backward lag selection for Regression models.\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing the time series data.\n",
    "        n_folds (int, optional): Number of cross-validation folds.\n",
    "        H (int, optional): Forecast horizon.\n",
    "        model: Model to be used for training and evaluation.\n",
    "        metrics (list, optional): List of metrics to evaluate the model.\n",
    "        lags_to_consider (list, optional): List of lags to consider for feature selection.\n",
    "        candidate_features (list, optional): List of candidate exogenous features.\n",
    "        transformations (list, optional): List of transformations to apply.\n",
    "        step_size (int, optional): Step size for rolling window.\n",
    "        validation_type (str, optional): Type of validation to use (\"cv\", \"BIC\", \"AIC\" or both \"AIC_BIC\"). if \"AIC_BIC\" are both selected, the model will be evaluated using both criteria.\n",
    "        iterations (int, optional): Number of iterations for model fitting to update parameters.\n",
    "        verbose (bool, optional): Whether to print progress messages.\n",
    "    Returns:\n",
    "        dict: Dictionary of best features\n",
    "    \"\"\"\n",
    "    remaining_lags = list(range(1, lags_to_consider + 1)) if lags_to_consider is not None else []\n",
    "    candidate_features = candidate_features.copy() if candidate_features is not None else []\n",
    "    transformations = transformations.copy() if transformations is not None else []\n",
    "    best_features = {\"best_lags\": remaining_lags, \"best_exogs\": candidate_features, \"best_transforms\": transformations}\n",
    "\n",
    "    ## setting the full model\n",
    "    # model_full = model.copy()\n",
    "    if lags_to_consider is not None:\n",
    "        model.lags = remaining_lags\n",
    "    if transformations is not None:\n",
    "        model.lag_transform = transformations\n",
    "\n",
    "    # set validation\n",
    "    if validation_type == \"cv\":\n",
    "        if isinstance(metrics, list):\n",
    "            best_score = [float('inf')] * len(metrics)\n",
    "        else:\n",
    "            best_score = float('inf')\n",
    "    elif validation_type in (\"BIC\", \"AIC\"):\n",
    "        best_score = float('inf')\n",
    "    elif validation_type == \"AIC_BIC\":\n",
    "        best_score = [float('inf')] * 2\n",
    "    else:\n",
    "        raise ValueError(\"Invalid validation_type. Choose from 'cv', 'BIC', 'AIC', or 'AIC_BIC'.\")\n",
    "\n",
    "    if isinstance(best_score, list):\n",
    "        def is_elementwise_improvement(score, best_s):\n",
    "            return all(s < b for s, b in zip(score, best_s))\n",
    "    else:\n",
    "        def is_elementwise_improvement(score, best_s):\n",
    "            return score < best_s\n",
    "        \n",
    "# After each feature selection step iterate model to make sure parameters are updated like transition probabilities and stds\n",
    "    def model_update(model_test, df_test, iterations=iterations):\n",
    "        model_test.data_prep(df_test) # update data preparation because if new lags to be consistent with coefficients\n",
    "        model_test.compute_coeffs() # update model coefficients because of new lags\n",
    "        model_test.fit(df_test, n_iter=iterations) # update model parameters like transition probs and stds\n",
    "        return model_test\n",
    "    \n",
    "\n",
    "    def validation(model_test, df_test):\n",
    "        if validation_type == \"cv\":\n",
    "            cv_result = hmm_cross_validate(model=model_test, df=df_test, cv_split=n_folds, test_size=H,\n",
    "                                metrics=metrics, step_size=step_size, n_iter=iterations)\n",
    "\n",
    "            if isinstance(metrics, list):\n",
    "                score = cv_result[\"score\"].tolist()\n",
    "            else:\n",
    "                score = cv_result[\"score\"].values[0]\n",
    "        elif validation_type == \"BIC\":\n",
    "            score = model_test.BIC\n",
    "        elif validation_type == \"AIC\":\n",
    "            score = model_test.AIC\n",
    "        elif validation_type == \"AIC_BIC\":\n",
    "            score = [model_test.AIC, model_test.BIC]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid validation_type. Choose from 'cv', 'BIC', 'AIC', or 'AIC_BIC'.\")\n",
    "\n",
    "        return score\n",
    "\n",
    "    # best_lags = None\n",
    "    while True:\n",
    "        improvement = False\n",
    "        candidate = {'type': None, 'name': None}\n",
    "        scores = best_score\n",
    "        if best_features[\"best_lags\"]:\n",
    "            for lg in best_features[\"best_lags\"]:\n",
    "                lags_to_test = [x for x in best_features[\"best_lags\"] if x != lg]\n",
    "                lags_to_test.sort()\n",
    "                model_test = model.copy()\n",
    "                model_test.lags = lags_to_test\n",
    "                model_test = model_update(model_test, df)\n",
    "                score = validation(model_test, df)\n",
    "                if is_elementwise_improvement(score, scores):\n",
    "                    scores = score\n",
    "                    candidate = {'type': 'lag', 'name': lg, 'model': model_test}\n",
    "                    improvement = True\n",
    "        if best_features[\"best_transforms\"]:\n",
    "            for trans in best_features[\"best_transforms\"]:\n",
    "                trans_to_test = [x for x in best_features[\"best_transforms\"] if x != trans]\n",
    "                model_test = model.copy()\n",
    "                model_test.lag_transform = trans_to_test\n",
    "                model_test = model_update(model_test, df)\n",
    "                score = validation(model_test, df)\n",
    "                if is_elementwise_improvement(score, scores):\n",
    "                    scores = score\n",
    "                    candidate = {'type': 'transform', 'name': trans, 'model': model_test}\n",
    "                    improvement = True\n",
    "        if best_features[\"best_exogs\"]:\n",
    "            for feat in best_features[\"best_exogs\"]:\n",
    "                # feat_to_test = [x for x in candidate_features if x != feat]\n",
    "                df_test = df.drop(columns=feat)\n",
    "                model_test = model.copy()\n",
    "                model_test = model_update(model_test, df_test)\n",
    "                score = validation(model_test, df_test)\n",
    "                if is_elementwise_improvement(score, scores):\n",
    "                    scores = score\n",
    "                    candidate = {'type': 'exog', 'name': feat, 'model': model_test}\n",
    "                    improvement = True\n",
    "\n",
    "        # Update best features\n",
    "        if improvement and candidate['type']:\n",
    "            best_score = scores\n",
    "            if candidate['type'] == 'lag':\n",
    "                best_features[\"best_lags\"].remove(candidate['name'])\n",
    "            elif candidate['type'] == 'exog':\n",
    "                best_features[\"best_exogs\"].remove(candidate['name'])\n",
    "                df = df.drop(columns=candidate['name'])\n",
    "            elif candidate['type'] == 'transform':\n",
    "                best_features[\"best_transforms\"].remove(candidate['name'])\n",
    "                if not best_features[\"best_transforms\"]:\n",
    "                    model.lag_transform = best_features[\"best_transforms\"]\n",
    "                else:\n",
    "                    model.lag_transform = None\n",
    "            model.A = candidate['model'].A\n",
    "            model.stds = candidate['model'].stds\n",
    "            model.LL = candidate['model'].LL\n",
    "            model.pi = candidate['model'].pi\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Removed {candidate['type']}: {candidate['name']} with score: {best_score} and loglik and BIC: {model.LL}, {model.BIC}\")\n",
    "        else:\n",
    "            break  # No improvement\n",
    "\n",
    "        # Finalize model with best features\n",
    "        model_ = model.copy()\n",
    "        if lags_to_consider is not None and best_features[\"best_lags\"]:\n",
    "            model_.lags = best_features[\"best_lags\"]\n",
    "        if transformations is not None and best_features[\"best_transforms\"]:\n",
    "            model_.lag_transform = best_features[\"best_transforms\"]\n",
    "        model_ = model_update(model_, df)\n",
    "\n",
    "\n",
    "    if transformations is not None and best_features[\"best_transforms\"]:\n",
    "        best_features[\"best_transforms\"] = [trans.get_name() for trans in best_features[\"best_transforms\"]]\n",
    "\n",
    "    return best_features, model_\n",
    "\n",
    "\n",
    "def hmm_mv_forward_feature_selection(df, target_col, n_folds = None, H = None, model = None, metrics = None,\n",
    "                                  lags_to_consider = None, candidate_features = None, transformations = None, \n",
    "                                    step_size = None, starting_lags = None, starting_transforms = None,\n",
    "                                    validation_type = \"cv\", iterations = 10, verbose = False):\n",
    "    \"\"\"\n",
    "    Performs forward lag selection for Vektor Autoregressive models and bidirectional ml models\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing the time series data.\n",
    "        target_col (str): The target column for accuracy evaluation.\n",
    "        n_folds (int): Number of folds for cross-validation.\n",
    "        H (int): Forecast horizon.\n",
    "        model: The forecasting model to be used.\n",
    "        metrics (list): List of metrics to evaluate the model.\n",
    "        lags_to_consider (dict): Dictionary of maximum lags for each variable.\n",
    "        candidate_features (list): List of candidate exogenous features.\n",
    "        transformations (list): List of transformations to consider.\n",
    "        step_size (int, optional): Step size for lag selection. Defaults to None.\n",
    "        starting_lags (dict, optional): Dictionary of starting lags for each variable. Defaults to None.\n",
    "        starting_transforms (dict, optional): Dictionary of starting transformations for each variable. Defaults to None.\n",
    "        verbose (bool, optional): Whether to print progress. Defaults to False.\n",
    "    Returns:\n",
    "        dict: Dictionary of best features for each variable.\n",
    "    \"\"\"\n",
    "\n",
    "    # max_lag = sum(x for x in max_lags.values())\n",
    "    \n",
    "    # lags = list(range(1, max_lags+1))\n",
    "    if lags_to_consider is None:\n",
    "        lags_to_consider = {}\n",
    "    if transformations is None:\n",
    "        transformations = {}\n",
    "\n",
    "    best_features = {\"best_lags\": {i: [] for i in lags_to_consider if lags_to_consider is not None}, \"best_transforms\": {i: [] for i in transformations if transformations is not None}, \"best_exogs\": []}\n",
    "    remaining_lags = {i:list(range(1, j+1)) for i, j in lags_to_consider.items()}\n",
    "    if starting_lags is not None:\n",
    "        for k, v in starting_lags.items():\n",
    "            all_lags = remaining_lags[k]\n",
    "            remaining_lags[k] = [x for x in all_lags if x not in v]\n",
    "            best_features[\"best_lags\"][k].extend(v)\n",
    "\n",
    "    if lags_to_consider is not None:\n",
    "        model.lags = None # Start with no lags\n",
    "\n",
    "    # Keep original for feature add-back\n",
    "    df_orig = df.copy()\n",
    "\n",
    "    # Drop candidate features initially\n",
    "    if candidate_features:\n",
    "        df = df.drop(columns=candidate_features) # Drop candidate features to start with feature selection\n",
    "    if transformations is not None:\n",
    "        if starting_transforms is not None:\n",
    "            for k, v in starting_transforms.items():\n",
    "                transformations[k] = [x for x in transformations if x not in v]\n",
    "                best_features[\"best_transforms\"][k].extend(v)\n",
    "            model.lag_transform = starting_transforms\n",
    "        else:\n",
    "            model.lag_transform = None # Start with no transformations\n",
    "\n",
    "\n",
    "    if validation_type == \"cv\":\n",
    "        if isinstance(metrics, list):\n",
    "            best_score = [float('inf')] * len(metrics)\n",
    "        else:\n",
    "            best_score = float('inf')\n",
    "    elif validation_type in (\"BIC\", \"AIC\"):\n",
    "        best_score = float('inf')\n",
    "    elif validation_type == \"AIC_BIC\":\n",
    "        best_score = [float('inf')] * 2\n",
    "    else:\n",
    "        raise ValueError(\"Invalid validation_type. Choose from 'cv', 'BIC', 'AIC', or 'AIC_BIC'.\")\n",
    "\n",
    "    if isinstance(best_score, list):\n",
    "        def is_elementwise_improvement(score, best_s):\n",
    "            return all(s < b for s, b in zip(score, best_s))\n",
    "    else:\n",
    "        def is_elementwise_improvement(score, best_s):\n",
    "            return score < best_s\n",
    "\n",
    "# After each feature selection step iterate model to make sure parameters are updated like transition probabilities and stds\n",
    "    def model_update(model_test, df_test, iterations=iterations):\n",
    "        model_test.data_prep(df_test) # update data preparation because if new lags to be consistent with coefficients\n",
    "        model_test.compute_coeffs() # update model coefficients because of new lags\n",
    "        model_test.fit(df_test, n_iter=iterations) # update model parameters like transition probs and stds\n",
    "        return model_test\n",
    "    \n",
    "\n",
    "    def validation(model_test, df_test):\n",
    "        if validation_type == \"cv\":\n",
    "            cv_result = hmm_mv_cross_validate(model = model_test, df=df_test, cv_split=n_folds, test_size=H,\n",
    "                                        metrics = metrics, step_size= step_size, n_iter=iterations)\n",
    "\n",
    "            if isinstance(metrics, list):\n",
    "                score = cv_result[target_col].tolist()\n",
    "            else:\n",
    "                score = cv_result[target_col].values[0]\n",
    "        elif validation_type == \"BIC\":\n",
    "            score = model_test.BIC\n",
    "        elif validation_type == \"AIC\":\n",
    "            score = model_test.AIC\n",
    "        elif validation_type == \"AIC_BIC\":\n",
    "            score = [model_test.AIC, model_test.BIC]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid validation_type. Choose from 'cv', 'BIC', 'AIC', or 'AIC_BIC'.\")\n",
    "\n",
    "        return score\n",
    "\n",
    "    \n",
    "    # while max_lag>0:\n",
    "    while True:\n",
    "        improvement = False\n",
    "        candidate = {'target': None, 'type': None, 'name': None}\n",
    "        scores = best_score\n",
    "        if lags_to_consider is not None:\n",
    "            for k, lg in remaining_lags.items():\n",
    "                for x in lg:\n",
    "                    model_test = model.copy()\n",
    "                    current_lag = {a:b for a, b in best_features['best_lags'].items()}\n",
    "                    current_lag[k] = best_features['best_lags'][k] + [x]\n",
    "                    current_lag[k].sort()\n",
    "                    model_test.lags = current_lag\n",
    "\n",
    "                    model_test = model_update(model_test, df)\n",
    "                    score = validation(model_test, df)\n",
    "\n",
    "                    if is_elementwise_improvement(score, scores):\n",
    "                        scores = score\n",
    "                        candidate = {'target': k, 'type': 'lag', 'name': x, 'model': model_test}\n",
    "                        improvement = True\n",
    "\n",
    "        # Test Exogenous Features\n",
    "        if candidate_features is not None:\n",
    "            for feat in candidate_features:\n",
    "                df_test = df.copy()\n",
    "                df_test[feat] = df_orig[feat]\n",
    "                model_test = model.copy()\n",
    "                model_test = model_update(model_test, df_test)\n",
    "                score = validation(model_test, df_test)\n",
    "\n",
    "                if is_elementwise_improvement(score, scores):\n",
    "                    scores = score\n",
    "                    candidate = {'target': None, 'type': 'exog', 'name': feat, 'model': model_test}\n",
    "                    improvement = True\n",
    "\n",
    "            # Test Transformations\n",
    "        if transformations is not None:\n",
    "            for k, trans in transformations.items():\n",
    "                for t in trans:\n",
    "                    model_test = model.copy()\n",
    "                    lag_transform = (model_test.lag_transform[k] or []) + [t]\n",
    "                    model_test.lag_transform[k] = lag_transform\n",
    "                    model_test = model_update(model_test, df)\n",
    "                    score = validation(model_test, df)\n",
    "                    if is_elementwise_improvement(score, scores):\n",
    "                        scores = score\n",
    "                        candidate = {'target': k, 'type': 'transform', 'name': t, 'model': model_test}\n",
    "                        improvement = True\n",
    "\n",
    "        # Update best features\n",
    "        if improvement:\n",
    "            best_score = scores\n",
    "            if candidate['type'] == 'lag':\n",
    "                best_features[\"best_lags\"][candidate['target']].append(candidate['name']) # store lags by target\n",
    "                remaining_lags[candidate['target']].remove(candidate['name'])\n",
    "            elif candidate['type'] == 'exog':\n",
    "                best_features[\"best_exogs\"].append(candidate['name'])\n",
    "                candidate_features.remove(candidate['name'])\n",
    "                df[candidate['name']] = df_orig[candidate['name']]\n",
    "            elif candidate['type'] == 'transform':\n",
    "                best_features[\"best_transforms\"][candidate['target']].append(candidate['name'])\n",
    "                transformations[candidate['target']].remove(candidate['name'])\n",
    "                if model.lag_transform is None:\n",
    "                    transform_dict = {candidate['target']: [candidate['name']]}\n",
    "                    model.lag_transform = transform_dict\n",
    "                else:\n",
    "                    if candidate['target'] not in model.lag_transform:\n",
    "                        model.lag_transform[candidate['target']] = [candidate['name']]\n",
    "                    else:\n",
    "                        model.lag_transform[candidate['target']].append(candidate['name'])\n",
    "\n",
    "            model.A = candidate['model'].A\n",
    "            model.covs = candidate['model'].covs\n",
    "            model.LL = candidate['model'].LL\n",
    "            model.pi = candidate['model'].pi\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Added {candidate['type']}: {candidate['name']} with score: {best_score} and loglik and BIC: {model.LL}, {model.BIC}\")\n",
    "        else:\n",
    "            break  # No improvement\n",
    "\n",
    "    # Finalize model with best features\n",
    "    model_ = model.copy()\n",
    "    if lags_to_consider is not None and best_features[\"best_lags\"]:\n",
    "        model_.lags = best_features[\"best_lags\"]\n",
    "    if transformations is not None and best_features[\"best_transforms\"]:\n",
    "        model_.lag_transform = best_features[\"best_transforms\"]\n",
    "\n",
    "    model_ = model_update(model_, df)\n",
    "\n",
    "\n",
    "    if transformations is not None:\n",
    "        for key, trans in best_features[\"best_transforms\"].items():\n",
    "            if trans:  # only process non-empty lists\n",
    "                best_features[\"best_transforms\"][key] = [t.get_name() for t in trans]\n",
    "\n",
    "    return best_features, model_\n",
    "\n",
    "\n",
    "def hmm_mv_backward_feature_selection(df, target_col, n_folds = None, H = None, model = None, metrics = None,\n",
    "                                  lags_to_consider = None, candidate_features = None, transformations = None, \n",
    "                                    step_size = None, validation_type = \"cv\", iterations = 10, \n",
    "                                    verbose = False):\n",
    "    \"\"\"\n",
    "    Performs backward lag selection for Regression models.\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing the time series data.\n",
    "        target_col (str): The target column for accuracy evaluation.\n",
    "        n_folds (int, optional): Number of cross-validation folds.\n",
    "        H (int, optional): Forecast horizon.\n",
    "        model: The forecasting model to be used.\n",
    "        metrics (list): List of metrics to evaluate the model.\n",
    "        step_size (int, optional): Step size for cross-validation. Defaults to None.\n",
    "        verbose (bool, optional): Whether to print progress. Defaults to False.\n",
    "    Returns:\n",
    "        dict: Dictionary of best features for each variable.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # remaining_lags = {i:list(range(1, j+1)) for i, j in lags_to_consider.items()}\n",
    "    # best_lags = {i:[] for i in max_lags}\n",
    "    best_features = {\n",
    "        \"best_lags\": {i: list(range(1, j+1)) for i, j in (lags_to_consider or {}).items()},\n",
    "        \"best_exogs\": candidate_features.copy() if candidate_features is not None else [],\n",
    "        \"best_transforms\": {i: j for i, j in (transformations or {}).items()}\n",
    "}\n",
    "    \n",
    "    ## setting the full model\n",
    "    if lags_to_consider is not None:\n",
    "        model.lags = best_features[\"best_lags\"]\n",
    "    if transformations is not None:\n",
    "        model.lag_transform = best_features[\"best_transforms\"]\n",
    "    # exogenous variables should be in df before passing df\n",
    "\n",
    "    if validation_type == \"cv\":\n",
    "        if isinstance(metrics, list):\n",
    "            best_score = [float('inf')] * len(metrics)\n",
    "        else:\n",
    "            best_score = float('inf')\n",
    "    elif validation_type in (\"BIC\", \"AIC\"):\n",
    "        best_score = float('inf')\n",
    "    elif validation_type == \"AIC_BIC\":\n",
    "        best_score = [float('inf')] * 2\n",
    "    else:\n",
    "        raise ValueError(\"Invalid validation_type. Choose from 'cv', 'BIC', 'AIC', or 'AIC_BIC'.\")\n",
    "\n",
    "    if isinstance(best_score, list):\n",
    "        def is_elementwise_improvement(score, best_s):\n",
    "            return all(s < b for s, b in zip(score, best_s))\n",
    "    else:\n",
    "        def is_elementwise_improvement(score, best_s):\n",
    "            return score < best_s\n",
    "\n",
    "# After each feature selection step iterate model to make sure parameters are updated like transition probabilities and stds\n",
    "    def model_update(model_test, df_test, iterations=iterations):\n",
    "        model_test.data_prep(df_test) # update data preparation because if new lags to be consistent with coefficients\n",
    "        model_test.compute_coeffs() # update model coefficients because of new lags\n",
    "        model_test.fit(df_test, n_iter=iterations) # update model parameters like transition probs and stds\n",
    "        return model_test\n",
    "    \n",
    "\n",
    "    def validation(model_test, df_test):\n",
    "        if validation_type == \"cv\":\n",
    "            cv_result = hmm_mv_cross_validate(model = model_test, df=df_test, cv_split=n_folds, test_size=H,\n",
    "                                        metrics = metrics, step_size= step_size, n_iter=iterations)\n",
    "\n",
    "            if isinstance(metrics, list):\n",
    "                score = cv_result[target_col].tolist()\n",
    "            else:\n",
    "                score = cv_result[target_col].values[0]\n",
    "        elif validation_type == \"BIC\":\n",
    "            score = model_test.BIC\n",
    "        elif validation_type == \"AIC\":\n",
    "            score = model_test.AIC\n",
    "        elif validation_type == \"AIC_BIC\":\n",
    "            score = [model_test.AIC, model_test.BIC]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid validation_type. Choose from 'cv', 'BIC', 'AIC', or 'AIC_BIC'.\")\n",
    "\n",
    "        return score\n",
    "\n",
    "    \n",
    "    while True:\n",
    "        improvement = False\n",
    "        candidate = {'target': None, 'type': None, 'name': None}\n",
    "        scores = best_score\n",
    "        if lags_to_consider is not None:\n",
    "            for targ_l, lags in best_features[\"best_lags\"].items():\n",
    "                for lg in lags:\n",
    "                    lags_to_test = {a:b for a, b in lags.items()}\n",
    "                    # Remove the current lag lg from current target\n",
    "                    lags_to_test[targ_l] = [x for x in lags if x != lg]\n",
    "                    lags_to_test[targ_l].sort()\n",
    "                    model_test = model.copy()\n",
    "                    model_test.lags = lags_to_test\n",
    "\n",
    "                    model_test = model_update(model_test, df)\n",
    "                    score = validation(model_test, df)\n",
    "                    if is_elementwise_improvement(score, scores):\n",
    "                        scores = score\n",
    "                        candidate = {'target': targ_l, 'type': 'lag', 'name': lg, 'model': model_test}\n",
    "                        improvement = True\n",
    "        if transformations is not None:\n",
    "            for targ_t, trans in best_features[\"best_transforms\"].items():\n",
    "                for tr in trans:\n",
    "                    trans_to_test = {a:b for a, b in best_features[\"best_transforms\"].items()}\n",
    "                    trans_to_test[targ_t] = [x for x in trans if x != tr]\n",
    "                    model_test = model.copy()\n",
    "                    # model_test.lags = remaining_lags\n",
    "                    model_test.lag_transform = trans_to_test\n",
    "                    model_test = model_update(model_test, df)\n",
    "                    score = validation(model_test, df)\n",
    "                    if is_elementwise_improvement(score, scores):\n",
    "                        scores = score\n",
    "                        candidate = {'target': targ_t, 'type': 'transform', 'name': trans, 'model': model_test}\n",
    "                        improvement = True\n",
    "        if candidate_features is not None:\n",
    "            for feat in best_features[\"best_exogs\"]:\n",
    "                # feat_to_test = [x for x in candidate_features if x != feat]\n",
    "                df_test = df.drop(columns=feat)\n",
    "                model_test = model.copy()\n",
    "                model_test = model_update(model_test, df_test)\n",
    "                score = validation(model_test, df_test)\n",
    "                if is_elementwise_improvement(score, scores):\n",
    "                    scores = score\n",
    "                    candidate = {'target': None, 'type': 'exog', 'name': feat, 'model': model_test}\n",
    "                    improvement = True\n",
    "\n",
    "        # Update best features\n",
    "        if improvement and candidate['type']:\n",
    "            best_score = scores\n",
    "            if candidate['type'] == 'lag':\n",
    "                best_features[\"best_lags\"][candidate['target']].remove(candidate['name'])\n",
    "            elif candidate['type'] == 'exog':\n",
    "                best_features[\"best_exogs\"].remove(candidate['name'])\n",
    "                df = df.drop(columns=candidate['name'])\n",
    "            elif candidate['type'] == 'transform':\n",
    "                best_features[\"best_transforms\"][candidate['target']].remove(candidate['name'])\n",
    "                if any(best_features[\"best_transforms\"][key] for key in best_features[\"best_transforms\"]):\n",
    "                    best_features[\"best_transforms\"] = {k: v for k, v in best_features[\"best_transforms\"].items() if not len(v) == 0}\n",
    "                    model.lag_transform = best_features[\"best_transforms\"]\n",
    "                else:\n",
    "                    model.lag_transform = None\n",
    "\n",
    "            model.A = candidate['model'].A\n",
    "            model.covs = candidate['model'].covs\n",
    "            model.LL = candidate['model'].LL\n",
    "            model.pi = candidate['model'].pi\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Removed {candidate['type']}: {candidate['name']} with score: {best_score} and loglik and BIC: {model.LL}, {model.BIC}\")\n",
    "        else:\n",
    "            break  # No improvement\n",
    "\n",
    "    # Finalize model with best features\n",
    "    model_ = model.copy()\n",
    "    if lags_to_consider is not None and best_features[\"best_lags\"]:\n",
    "        model_.lags = best_features[\"best_lags\"]\n",
    "    if transformations is not None and best_features[\"best_transforms\"]:\n",
    "        model_.lag_transform = best_features[\"best_transforms\"]\n",
    "\n",
    "    model_ = model_update(model_, df)\n",
    "\n",
    "    # if transformations is not None and at least one key is not empty get their names\n",
    "    if transformations is not None:\n",
    "        for key, trans in best_features[\"best_transforms\"].items():\n",
    "            if trans:  # only process non-empty lists\n",
    "                best_features[\"best_transforms\"][key] = [t.get_name() for t in trans]\n",
    "\n",
    "    return best_features, model_\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Holt-Winters Exponential Smoothing Model Tuning\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def tune_ets(data, param_space, cv_splits, horizon, eval_metric, eval_num, step_size = None, verbose = False):\n",
    "    \"\"\"\n",
    "    Tune ETS model hyperparameters using Hyperopt.\n",
    "\n",
    "    Args:\n",
    "        data (array-like): Time series data.\n",
    "        param_space (dict): Hyperparameter search space.\n",
    "        cv_splits (int): Number of cross-validation splits.\n",
    "        horizon (int): Forecast horizon.\n",
    "        step_size (int): Step size for time series cross-validation.\n",
    "        eval_metric (function): Evaluation metric function.\n",
    "        eval_num (int): Number of evaluations for hyperparameter tuning.\n",
    "        verbose (bool): Whether to print progress.\n",
    "    Returns:\n",
    "        tuple: Best model parameters and fit parameters.\n",
    "    \"\"\"\n",
    "    from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "    from hyperopt import fmin, tpe, hp, Trials, STATUS_OK, space_eval\n",
    "    from hyperopt.pyll import scope\n",
    "    tscv = ParametricTimeSeriesSplit(n_splits=cv_splits, test_size=horizon, step_size=step_size)\n",
    "    # Define the objective function for hyperparameter tuning\n",
    "    def objective(params):\n",
    "        if (params.get(\"trend\") != None) & (params.get(\"seasonal\") != None): # Both trend and seasonal are specified\n",
    "            alpha = params.get('smoothing_level') # Smoothing level for the level component\n",
    "            beta = params.get('smoothing_trend') # Smoothing level for the trend component\n",
    "            gamma = params.get('smoothing_seasonal') # Smoothing level for the seasonal component\n",
    "            trend_type = params.get('trend') # Trend type\n",
    "            season_type = params.get('seasonal') # Seasonal type\n",
    "            S = params.get('seasonal_periods') # Seasonal periods\n",
    "            if params.get(\"damped_trend\"): # Damped trend\n",
    "                damped_bool = params.get(\"damped_trend\")\n",
    "                if damped_bool:\n",
    "                    damp_trend = params.get('damping_trend')\n",
    "                else:\n",
    "                    damp_trend = None\n",
    "            else:\n",
    "                damped_bool = params.get(\"damped_trend\")\n",
    "                damp_trend = None\n",
    "\n",
    "        elif (params.get(\"trend\") != None) & (params.get(\"seasonal\") == None): # Trend is specified, seasonal is not\n",
    "            alpha = params.get('smoothing_level')\n",
    "            beta = params.get('smoothing_trend')\n",
    "            gamma = None\n",
    "            trend_type = params.get('trend')\n",
    "            season_type = params.get('seasonal')\n",
    "            S=None\n",
    "            if params.get(\"damped_trend\"):\n",
    "                damped_bool = params.get(\"damped_trend\")\n",
    "                damp_trend = params.get('damping_trend')\n",
    "            else:\n",
    "                damped_bool = params.get(\"damped_trend\")\n",
    "                damp_trend = None\n",
    "\n",
    "        elif (params.get(\"trend\") == None) & (params.get(\"seasonal\") != None): # Seasonal is specified, trend is not\n",
    "            alpha = params.get('smoothing_level')\n",
    "            beta = None\n",
    "            gamma = params.get('smoothing_seasonal')\n",
    "            trend_type = params.get('trend')\n",
    "            season_type = params.get('seasonal')\n",
    "            S = params.get('seasonal_periods')\n",
    "            if params.get(\"damped_trend\"):\n",
    "                damped_bool = False\n",
    "                damp_trend = None\n",
    "            else:\n",
    "                damped_bool = params.get(\"damped_trend\")\n",
    "                damp_trend = None\n",
    "                \n",
    "        else: # Neither trend nor seasonal is specified\n",
    "            alpha = params.get('smoothing_level')\n",
    "            beta = None\n",
    "            gamma = None\n",
    "            trend_type = params.get('trend')\n",
    "            season_type = params.get('seasonal')\n",
    "            S = None\n",
    "            if params.get(\"damped_trend\"):\n",
    "                damped_bool = False\n",
    "                damp_trend = None\n",
    "            else:\n",
    "                damped_bool = params.get(\"damped_trend\")\n",
    "                damp_trend = None\n",
    "            \n",
    "\n",
    "        metric = [] # List to store evaluation metrics\n",
    "        # Perform time series cross-validation\n",
    "        for train_index, test_index in tscv.split(data):\n",
    "            train, test = data[train_index], data[test_index]\n",
    "            # Fit the Holt-Winters model with the specified parameters\n",
    "            hw_fit = ExponentialSmoothing(train ,seasonal_periods=S , seasonal=season_type, trend=trend_type, damped_trend = damped_bool).fit(smoothing_level = alpha, \n",
    "                                                                                                                      smoothing_trend = beta,\n",
    "                                                                                                                      smoothing_seasonal = gamma,\n",
    "                                                                                                damping_trend=damp_trend)\n",
    "            \n",
    "            hw_forecast = hw_fit.forecast(len(test))\n",
    "            forecast_filled = np.nan_to_num(hw_forecast, nan=0)\n",
    "\n",
    "            #Evaluate using the specified metric\n",
    "            if eval_metric.__name__ in [\"MASE\", \"SMAE\", \"SRMSE\", \"RMSSE\"]:\n",
    "                accuracy = eval_metric(test,\n",
    "                                    forecast_filled,\n",
    "                                    train)\n",
    "            else:\n",
    "                accuracy = eval_metric(\n",
    "                        test,\n",
    "                        forecast_filled,\n",
    "                    )\n",
    "\n",
    "            # accuracy = eval_metric(test, forecast_filled)\n",
    "            metric.append(accuracy)\n",
    "            \n",
    "        score = np.mean(metric)\n",
    "        if verbose ==True:\n",
    "            print (\"SCORE:\", score)\n",
    "\n",
    "        return {'loss':score, 'status':STATUS_OK}\n",
    "    \n",
    "    # Perform hyperparameter optimization using Hyperopt\n",
    "    trials = Trials()\n",
    "    \n",
    "    best_hyperparams = fmin(fn = objective,\n",
    "                    space = param_space,\n",
    "                    algo = tpe.suggest,\n",
    "                    max_evals = eval_num,\n",
    "                    trials = trials)\n",
    "    best_params = space_eval(param_space, best_hyperparams)\n",
    "    model_params = {\n",
    "        \"trend\": best_params.get(\"trend\"),\n",
    "        \"seasonal_periods\": best_params.get(\"seasonal_periods\"),\n",
    "        \"seasonal\": best_params.get(\"seasonal\"),\n",
    "        \"damped_trend\": best_params.get(\"damped_trend\")\n",
    "    }\n",
    "    fit_params = {\n",
    "        \"smoothing_level\": best_params.get(\"smoothing_level\"),\n",
    "        \"smoothing_trend\": best_params.get(\"smoothing_trend\"),\n",
    "        \"smoothing_seasonal\": best_params.get(\"smoothing_seasonal\"),\n",
    "        \"damping_trend\": best_params.get(\"damping_trend\")\n",
    "    }\n",
    "\n",
    "    \n",
    "    if set(model_params.keys()) == {\"damped_trend\"}: # if only damped_trend is in model_params, ensure it's False\n",
    "        model_params[\"damped_trend\"] = False\n",
    "\n",
    "    # Remove all keys with value None in a single step\n",
    "    model_params = {k: v for k, v in model_params.items() if v is not None}\n",
    "    fit_params = {k: v for k, v in fit_params.items() if v is not None}\n",
    "    fit_params = {k: v for k, v in fit_params.items()\n",
    "                  if not (k == \"damping_trend\" and model_params.get(\"damped_trend\") is False)}\n",
    "    return model_params, fit_params\n",
    "\n",
    "def cv_hmm_lag_tune(\n",
    "    model,\n",
    "    df,\n",
    "    cv_split,\n",
    "    test_size,\n",
    "    eval_metric,\n",
    "    lag_space=None,\n",
    "    step_size=None,\n",
    "    opt_horizon=None,\n",
    "    eval_num=100,\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Tune forecasting model hyperparameters using cross-validation and Bayesian optimization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : object\n",
    "        Forecasting model object with .fit and .forecast methods and relevant attributes.\n",
    "    df : pd.DataFrame\n",
    "        Time series dataframe.\n",
    "    cv_split : int\n",
    "        Number of time series splits.\n",
    "    test_size : int\n",
    "        Size of test window for each split.\n",
    "    lag_space : dict\n",
    "        Hyperopt lag search space.\n",
    "    eval_metric : callable\n",
    "        Evaluation metric function.\n",
    "    step_size : int, optional\n",
    "        Step size for moving the window. Defaults to None (equal to test_size).\n",
    "    opt_horizon : int, optional\n",
    "        Evaluate only on last N points of each split. Defaults to None (all points).\n",
    "    eval_num : int, optional\n",
    "        Number of hyperopt evaluations. Defaults to 100.\n",
    "    verbose : bool, optional\n",
    "        Print progress. Defaults to False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Best hyperparameter values found.\n",
    "    \"\"\"\n",
    "    tscv = ParametricTimeSeriesSplit(n_splits=cv_split, test_size=test_size, step_size=step_size)\n",
    "    \n",
    "    max_lag = lag_space \n",
    "    space = {f\"lag_{i}\": hp.choice(f\"lag_{i}\", [0, 1]) for i in range(1, max_lag + 1)}\n",
    "    def objective(params):\n",
    "\n",
    "        selected_lags = [i for i in range(1, max_lag + 1) if params[f\"lag_{i}\"] == 1]\n",
    "        model_ = model.copy()\n",
    "        model_.lags = selected_lags\n",
    "\n",
    "                # Optional: penalize too few lags\n",
    "        if len(selected_lags) < 1:\n",
    "            return {\"loss\": 1e6, \"status\": STATUS_OK}\n",
    "\n",
    "        metrics = []\n",
    "        for idx, (train_index, test_index) in enumerate(tscv.split(df)):\n",
    "            train, test = df.iloc[train_index], df.iloc[test_index]\n",
    "            x_test = test.drop(columns=[model.target_col])\n",
    "            y_test = np.array(test[model.target_col])\n",
    "            if idx == 0:\n",
    "                model_.fit_em(train)\n",
    "            else:\n",
    "                model_.fit(train)\n",
    "            \n",
    "            y_pred = model_.forecast(len(y_test), x_test)\n",
    "\n",
    "            #Evaluate using the specified metric\n",
    "            if eval_metric.__name__ in [\"MASE\", \"SMAE\", \"SRMSE\", \"RMSSE\"]:\n",
    "                score = eval_metric(y_test[-opt_horizon:] if opt_horizon else y_test,\n",
    "                                    y_pred[-opt_horizon:] if opt_horizon else y_pred,\n",
    "                                    train[model.target_col])\n",
    "            else:\n",
    "                score = eval_metric(\n",
    "                        y_test[-opt_horizon:] if opt_horizon else y_test,\n",
    "                        y_pred[-opt_horizon:] if opt_horizon else y_pred,\n",
    "                    )\n",
    "            metrics.append(score)\n",
    "\n",
    "        mean_score = np.mean(metrics)\n",
    "        if verbose:\n",
    "            print(\"Score:\", mean_score)\n",
    "        return {\"loss\": mean_score, \"status\": STATUS_OK}\n",
    "\n",
    "    trials = Trials()\n",
    "    best_hyperparams = fmin(\n",
    "        fn=objective,\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=eval_num,\n",
    "        trials=trials,\n",
    "    )\n",
    "\n",
    "    # Extract and sort lag values\n",
    "    best_lag_indexes = [value for key, value in sorted(((k, v) for k, v in best_hyperparams.items() if k.startswith(\"lag_\")),\n",
    "                                                          key=lambda x: int(x[0].split(\"_\")[1]))]\n",
    "    best_lag_values = [i for i in range(1, lag_space + 1) if best_lag_indexes[i-1]==1]\n",
    "    return best_lag_values\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# SARIMA Model Tuning\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def tune_sarima(y, d, D, season,p, q, P, Q, X=None):\n",
    "    \"\"\"\n",
    "    Finds the best SARIMA parameters using AIC as the evaluation metric.\n",
    "    Args:\n",
    "        y (array-like): The time series data.\n",
    "        d (int): The non-seasonal differencing order.\n",
    "        d (int): The non-seasonal differencing order.\n",
    "        D (int): The seasonal differencing order.\n",
    "        season (int): The seasonal period.\n",
    "        p (int): Max range of non-seasonal AR orders to test.\n",
    "        q (int): Max range of non-seasonal MA orders to test.\n",
    "        P (int): Max range of seasonal AR orders to test.\n",
    "        Q (int): Max range of seasonal MA orders to test.\n",
    "        X (array-like, optional): Exogenous variables. Defaults to None.\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the combinations of parameters and their corresponding AIC values.\n",
    "    \"\"\"\n",
    "    if X is not None:\n",
    "        X = np.array(X, dtype = np.float64)\n",
    "    p_orders = range(0, p+1)\n",
    "    q_orders = range(0, q+1) # MA(q)\n",
    "    P_orders = range(0, P+1) # Seasonal autoregressive order.\n",
    "    Q_orders = range(0, Q+1) #Seasonal moving average order.\n",
    "    parameters = product(p_orders, q_orders, P_orders, Q_orders)\n",
    "    result = []\n",
    "    for param in tqdm_notebook(parameters):\n",
    "        try:\n",
    "            model = SARIMAX(endog=y, exog = X, order = (param[0], d, param[1]), seasonal_order= (param[2], D, param[3], season)).fit(disp = -1)\n",
    "        except:\n",
    "            continue\n",
    "                            \n",
    "        aic = model.aic\n",
    "        result.append([param, aic])\n",
    "    result_df = pd.DataFrame(result)\n",
    "    result_df.columns = [\"(p, q)x(P, Q)\", \"AIC\"] \n",
    "    result_df = result_df.sort_values(\"AIC\", ascending = True) #Sort in ascending order, lower AIC is better\n",
    "    return result_df\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# ML tuning utility function\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def cross_validate(model, df, cv_split, test_size, metrics, step_size=1,\n",
    "                   h_split_point=None):\n",
    "    \"\"\"\n",
    "    Run cross-validation using time series splits.\n",
    "\n",
    "    Args:\n",
    "        model (class): Machine learning model class (e.g., CatBoostRegressor, LGBMRegressor).\n",
    "        df (pd.DataFrame): Input data.\n",
    "        cv_split (int): Number of splits in TimeSeriesSplit.\n",
    "        test_size (int): Size of test window.\n",
    "        metrics (list): List of metric functions.\n",
    "        step_size (int): Step size for time series cross-validation.\n",
    "        h_split_point (int, optional): If provided, split the horizon into two parts for separate evaluation.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Performance metrics for CV.\n",
    "    \"\"\"\n",
    "    tscv = ParametricTimeSeriesSplit(n_splits=cv_split, test_size=test_size, step_size=step_size)\n",
    "    metrics_dict = {m.__name__: [] for m in metrics}\n",
    "    if h_split_point is not None:\n",
    "        metrics_dict1 = {m.__name__: [] for m in metrics}\n",
    "        metrics_dict2 = {m.__name__: [] for m in metrics}\n",
    "    for train_index, test_index in tscv.split(df):\n",
    "        train, test = df.iloc[train_index], df.iloc[test_index]\n",
    "        x_test = test.drop(columns=[model.target_col])\n",
    "        y_test = np.array(test[model.target_col])\n",
    "        model.fit(train)\n",
    "        bb_forecast = model.forecast(test_size, x_test)\n",
    "        # Evaluate each metric\n",
    "        for m in metrics:\n",
    "            if m.__name__ in [\"MASE\", \"SMAE\", \"SRMSE\", \"RMSSE\"]:\n",
    "                eval_val = m(y_test, bb_forecast, train[model.target_col])\n",
    "            else:\n",
    "                eval_val = m(y_test, bb_forecast)\n",
    "            metrics_dict[m.__name__].append(eval_val)\n",
    "        if h_split_point is not None and isinstance(h_split_point, int):\n",
    "            y_test_1, y_test_2 = y_test[:h_split_point], y_test[h_split_point:]\n",
    "            bb_forecast_1, bb_forecast_2 = bb_forecast[:h_split_point], bb_forecast[h_split_point:]\n",
    "            for m in metrics:\n",
    "                if m.__name__ in ['MASE', 'SMAE', 'SRMSE', 'RMSSE']:\n",
    "                    eval_val1 = m(y_test_1, bb_forecast_1, np.array(train[model.target_col]))\n",
    "                    eval_val2 = m(y_test_2, bb_forecast_2, np.array(train[model.target_col]))\n",
    "                else:\n",
    "                    eval_val1 = m(y_test_1, bb_forecast_1)\n",
    "                    eval_val2 = m(y_test_2, bb_forecast_2)\n",
    "                metrics_dict1[m.__name__].append(eval_val1)\n",
    "                metrics_dict2[m.__name__].append(eval_val2)\n",
    "\n",
    "    overall_performance = [[m.__name__, np.mean(metrics_dict[m.__name__])] for m in metrics]\n",
    "    overall_performance = pd.DataFrame(overall_performance).rename(columns={0: \"eval_metric\", 1: \"score\"})\n",
    "    if h_split_point is not None and isinstance(h_split_point, int):\n",
    "        performance_1 = [[m.__name__, np.mean(metrics_dict1[m.__name__])] for m in metrics]\n",
    "        performance_2 = [[m.__name__, np.mean(metrics_dict2[m.__name__])] for m in metrics]\n",
    "        overall_performance = pd.DataFrame(overall_performance).rename(columns={0: \"eval_metric\", 1: \"score\"})\n",
    "        perf_1_df = pd.DataFrame(performance_1).rename(columns={0: \"eval_metric\", 1: f\"score_before_{h_split_point}\"})\n",
    "        perf_2_df = pd.DataFrame(performance_2).rename(columns={0: \"eval_metric\", 1: f\"score_after_{h_split_point}\"})\n",
    "        # merge all three dataframes\n",
    "        overall_performance = overall_performance.merge(perf_1_df, on=\"eval_metric\").merge(perf_2_df, on=\"eval_metric\")\n",
    "    return overall_performance\n",
    "\n",
    "def var_cross_validate(\n",
    "    model,\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str,\n",
    "    cv_split: int,\n",
    "    test_size: int,\n",
    "    metrics: List[Callable],\n",
    "    step_size: int = 1,\n",
    "    h_split_point: Optional[int] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform cross-validation for VAR model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : object\n",
    "        VAR model instance with defined target_cols attribute, lag order, and etc.\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe.\n",
    "    target_col : str\n",
    "        Target variable for evaluation.\n",
    "    cv_split : int\n",
    "        Number of cross-validation folds.\n",
    "    test_size : int\n",
    "        Test size per fold.\n",
    "    metrics : List[Callable]\n",
    "        List of metric functions.\n",
    "    step_size : int, optional\n",
    "        Step size for rolling window. Default is 1.\n",
    "    h_split_point : int, optional\n",
    "        Point to split the test set for separate evaluation. Default is None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with averaged cross-validation metric scores.\n",
    "    \"\"\"\n",
    "\n",
    "    tscv = ParametricTimeSeriesSplit(n_splits=cv_split, test_size=test_size, step_size=step_size)\n",
    "    metrics_dict = {m.__name__: [] for m in metrics}\n",
    "    if h_split_point is not None:\n",
    "        metrics_dict1 = {m.__name__: [] for m in metrics}\n",
    "        metrics_dict2 = {m.__name__: [] for m in metrics}\n",
    "\n",
    "    for train_index, test_index in tscv.split(df):\n",
    "        train, test = df.iloc[train_index], df.iloc[test_index]\n",
    "        x_test, y_test = test.drop(columns=model.target_cols), np.array(test[target_col])\n",
    "        model.fit(train)\n",
    "        forecasts = model.forecast(test_size, x_test)[target_col]\n",
    "\n",
    "        forecast_df = test[target_col].to_frame()\n",
    "        forecast_df[\"forecasts\"] = forecasts\n",
    "        # cv_forecasts_df = pd.concat([cv_forecasts_df, forecast_df], axis=0)\n",
    "\n",
    "        for m in metrics:\n",
    "            if m.__name__ in [\"MASE\", \"SMAE\", \"SRMSE\", \"RMSSE\"]:\n",
    "                eval_val = m(y_test, forecasts, train[target_col])\n",
    "            else:\n",
    "                eval_val = m(y_test, forecasts)\n",
    "            metrics_dict[m.__name__].append(eval_val)\n",
    "        if h_split_point is not None and isinstance(h_split_point, int):\n",
    "            y_test_1, y_test_2 = y_test[:h_split_point], y_test[h_split_point:]\n",
    "            bb_forecast_1, bb_forecast_2 = forecasts[:h_split_point], forecasts[h_split_point:]\n",
    "            for m in metrics:\n",
    "                if m.__name__ in ['MASE', 'SMAE', 'SRMSE', 'RMSSE']:\n",
    "                    eval_val1 = m(y_test_1, bb_forecast_1, np.array(train[model.target_col]))\n",
    "                    eval_val2 = m(y_test_2, bb_forecast_2, np.array(train[model.target_col]))\n",
    "                else:\n",
    "                    eval_val1 = m(y_test_1, bb_forecast_1)\n",
    "                    eval_val2 = m(y_test_2, bb_forecast_2)\n",
    "                metrics_dict1[m.__name__].append(eval_val1)\n",
    "                metrics_dict2[m.__name__].append(eval_val2)\n",
    "\n",
    "    overall_performance = [[m.__name__, np.mean(metrics_dict[m.__name__])] for m in metrics]\n",
    "    overall_performance = pd.DataFrame(overall_performance).rename(columns={0: \"eval_metric\", 1: \"score\"})\n",
    "    if h_split_point is not None and isinstance(h_split_point, int):\n",
    "        performance_1 = [[m.__name__, np.mean(metrics_dict1[m.__name__])] for m in metrics]\n",
    "        performance_2 = [[m.__name__, np.mean(metrics_dict2[m.__name__])] for m in metrics]\n",
    "        overall_performance = pd.DataFrame(overall_performance).rename(columns={0: \"eval_metric\", 1: \"score\"})\n",
    "        perf_1_df = pd.DataFrame(performance_1).rename(columns={0: \"eval_metric\", 1: f\"score_before_{h_split_point}\"})\n",
    "        perf_2_df = pd.DataFrame(performance_2).rename(columns={0: \"eval_metric\", 1: f\"score_after_{h_split_point}\"})\n",
    "        # merge all three dataframes\n",
    "        overall_performance = overall_performance.merge(perf_1_df, on=\"eval_metric\").merge(perf_2_df, on=\"eval_metric\")\n",
    "    return overall_performance\n",
    "\n",
    "def arima_cross_validate(model, df, target_col, cv_split, test_size, metrics,\n",
    "                         step_size=1, h_split_point=None):\n",
    "    \"\"\"\n",
    "    Run cross-validation using time series splits.\n",
    "\n",
    "    Args:\n",
    "        model (class): ARIMA model instance. Supports ARIMA from statsforecast (Nixtla).\n",
    "        df (pd.DataFrame): Input data.\n",
    "        target_col (str): Name of the target column.\n",
    "        cv_split (int): Number of splits in TimeSeriesSplit.\n",
    "        test_size (int): Size of test window.\n",
    "        metrics (list): List of metric functions.\n",
    "        step_size (int): Step size for time series cross-validation.\n",
    "        h_split_point (int, optional): If provided, split the horizon into two parts for separate evaluation.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Performance metrics for CV.\n",
    "    \"\"\"\n",
    "    tscv = ParametricTimeSeriesSplit(n_splits=cv_split, test_size=test_size, step_size=step_size)\n",
    "    metrics_dict = {m.__name__: [] for m in metrics}\n",
    "    if h_split_point is not None:\n",
    "        metrics_dict1 = {m.__name__: [] for m in metrics}\n",
    "        metrics_dict2 = {m.__name__: [] for m in metrics}\n",
    "    for train_index, test_index in tscv.split(df):\n",
    "        train, test = df.iloc[train_index], df.iloc[test_index]\n",
    "        x_test = test.drop(columns=[target_col]).to_numpy()\n",
    "        y_test = np.array(test[target_col])\n",
    "        y_train = train[target_col].to_numpy()\n",
    "        x_train = train.drop(columns=[target_col]).to_numpy()\n",
    "        forecasts = model.forecast(y=y_train, h=test_size, X=x_train, X_future=x_test)[\"mean\"]\n",
    "        for m in metrics:\n",
    "            if m.__name__ in [\"MASE\", \"SMAE\", \"SRMSE\", \"RMSSE\"]:\n",
    "                eval_val = m(y_test, forecasts, train[target_col])\n",
    "            else:\n",
    "                eval_val = m(y_test, forecasts)\n",
    "            metrics_dict[m.__name__].append(eval_val)\n",
    "        if h_split_point is not None and isinstance(h_split_point, int):\n",
    "            y_test_1, y_test_2 = y_test[:h_split_point], y_test[h_split_point:]\n",
    "            bb_forecast_1, bb_forecast_2 = forecasts[:h_split_point], forecasts[h_split_point:]\n",
    "            for m in metrics:\n",
    "                if m.__name__ in ['MASE', 'SMAE', 'SRMSE', 'RMSSE']:\n",
    "                    eval_val1 = m(y_test_1, bb_forecast_1, np.array(train[model.target_col]))\n",
    "                    eval_val2 = m(y_test_2, bb_forecast_2, np.array(train[model.target_col]))\n",
    "                else:\n",
    "                    eval_val1 = m(y_test_1, bb_forecast_1)\n",
    "                    eval_val2 = m(y_test_2, bb_forecast_2)\n",
    "                metrics_dict1[m.__name__].append(eval_val1)\n",
    "                metrics_dict2[m.__name__].append(eval_val2)\n",
    "\n",
    "    overall_performance = [[m.__name__, np.mean(metrics_dict[m.__name__])] for m in metrics]\n",
    "    overall_performance = pd.DataFrame(overall_performance).rename(columns={0: \"eval_metric\", 1: \"score\"})\n",
    "    if h_split_point is not None and isinstance(h_split_point, int):\n",
    "        performance_1 = [[m.__name__, np.mean(metrics_dict1[m.__name__])] for m in metrics]\n",
    "        performance_2 = [[m.__name__, np.mean(metrics_dict2[m.__name__])] for m in metrics]\n",
    "        overall_performance = pd.DataFrame(overall_performance).rename(columns={0: \"eval_metric\", 1: \"score\"})\n",
    "        perf_1_df = pd.DataFrame(performance_1).rename(columns={0: \"eval_metric\", 1: f\"score_before_{h_split_point}\"})\n",
    "        perf_2_df = pd.DataFrame(performance_2).rename(columns={0: \"eval_metric\", 1: f\"score_after_{h_split_point}\"})\n",
    "        # merge all three dataframes\n",
    "        overall_performance = overall_performance.merge(perf_1_df, on=\"eval_metric\").merge(perf_2_df, on=\"eval_metric\")\n",
    "    return overall_performance\n",
    "\n",
    "def mv_cross_validate(model, df, cv_split, test_size, metrics, step_size=None):\n",
    "    \"\"\"\n",
    "    Cross-validate the bidirectional CatBoost model with time series split.\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe.\n",
    "        cv_split (int): Number of folds.\n",
    "        test_size (int): Forecast window for each split.\n",
    "        metrics (list): List of evaluation metric functions.\n",
    "    Returns:\n",
    "        pd.DataFrame: CV performance metrics for each target variable.\n",
    "    \"\"\"\n",
    "    tscv = ParametricTimeSeriesSplit(n_splits=cv_split, test_size=test_size, step_size=step_size)\n",
    "    metrics_dict = {m.__name__: [] for m in metrics}\n",
    "    cv_fi = pd.DataFrame()\n",
    "    cv_forecasts_df = pd.DataFrame()\n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(df)):\n",
    "        train, test = df.iloc[train_index], df.iloc[test_index]\n",
    "        x_test = test.drop(columns=model.target_cols)\n",
    "        y_test1 = np.array(test[model.target_cols[0]])\n",
    "        y_test2 = np.array(test[model.target_cols[1]])\n",
    "        \n",
    "        model.fit(train)\n",
    "\n",
    "        forecast_vals1, forecast_vals2 = model.forecast(test_size, x_test)\n",
    "        forecat_df = test[model.target_cols]\n",
    "        forecat_df[\"forecasts1\"] = forecast_vals1\n",
    "        forecat_df[\"forecasts2\"] = forecast_vals2\n",
    "        cv_forecasts_df = pd.concat([cv_forecasts_df, forecat_df], axis=0)\n",
    "        for m in metrics:\n",
    "            if m.__name__ in [\"MASE\", \"SMAE\", \"SRMSE\", \"RMSSE\"]:\n",
    "                val1 = m(y_test1, forecast_vals1, train[model.target_cols[0]])\n",
    "                val2 = m(y_test2, forecast_vals2, train[model.target_cols[1]])\n",
    "            else:\n",
    "                val1 = m(y_test1, forecast_vals1)\n",
    "                val2 = m(y_test2, forecast_vals2)\n",
    "\n",
    "            metrics_dict[m.__name__].append([val1, val2])\n",
    "\n",
    "        cv_tr_df1 = pd.DataFrame({\"feat_name\": model.model1_fit.feature_names_in_,\n",
    "                                \"importance\": model.model1_fit.feature_importances_}).sort_values(by=\"importance\", ascending=False)\n",
    "        cv_tr_df1[\"target\"] = model.target_cols[0]\n",
    "        cv_tr_df1[\"fold\"] = i\n",
    "        cv_tr_df2 = pd.DataFrame({\"feat_name\": model.model2_fit.feature_names_in_,\n",
    "                                \"importance\": model.model2_fit.feature_importances_}).sort_values(by=\"importance\", ascending=False)\n",
    "        cv_tr_df2[\"target\"] = model.target_cols[1]\n",
    "        cv_tr_df2[\"fold\"] = i\n",
    "        cv_fi = pd.concat([cv_fi, cv_tr_df1, cv_tr_df2], axis=0)\n",
    "    overall = [[m.__name__, np.mean([v[0] for v in metrics_dict[m.__name__]])] for m in metrics]\n",
    "    # pd.DataFrame(overall).rename(columns={0: \"eval_metric\", 1: \"score1\", 2: \"score2\"})\n",
    "    return pd.DataFrame(overall).rename(columns={0: \"eval_metric\", 1: model.target_cols[0], 2: model.target_cols[1]})\n",
    "\n",
    "def cv_tune(\n",
    "    model,\n",
    "    df,\n",
    "    cv_split,\n",
    "    test_size,\n",
    "    param_space,\n",
    "    eval_metric,\n",
    "    lag_space=None,\n",
    "    starting_lags=None,\n",
    "    transform_space=None,\n",
    "    step_size=None,\n",
    "    opt_horizon=None,\n",
    "    eval_num=100,\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Tune forecasting model hyperparameters using cross-validation and Bayesian optimization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : object\n",
    "        Forecasting model object with .fit and .forecast methods and relevant attributes.\n",
    "    df : pd.DataFrame\n",
    "        Time series dataframe.\n",
    "    cv_split : int\n",
    "        Number of time series splits.\n",
    "    test_size : int\n",
    "        Size of test window for each split.\n",
    "    param_space : dict\n",
    "        Hyperopt parameter search space.\n",
    "    lag_space : int, optional\n",
    "        Maximum number of lags to consider for each variable. Defaults to None (no lag).\n",
    "    starting_lags : list, optional\n",
    "        List of starting lags for each variable. Defaults to None (no starting lags).\n",
    "    transform_space : int, optional\n",
    "        Possible transformations to consider for each variable. Defaults to None (no transformation).\n",
    "    eval_metric : callable\n",
    "        Evaluation metric function.\n",
    "    step_size : int, optional\n",
    "        Step size for moving the window. Defaults to None (equal to test_size).\n",
    "    opt_horizon : int, optional\n",
    "        Evaluate only on last N points of each split. Defaults to None (all points).\n",
    "    eval_num : int, optional\n",
    "        Number of hyperopt evaluations. Defaults to 100.\n",
    "    verbose : bool, optional\n",
    "        Print progress. Defaults to False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Best hyperparameter values found.\n",
    "    \"\"\"\n",
    "    tscv = ParametricTimeSeriesSplit(n_splits=cv_split, test_size=test_size, step_size=step_size)\n",
    "\n",
    "    def _set_model_params(params):\n",
    "        # Handle special model parameters that are not passed to model constructor\n",
    "        # and must be set directly on the forecasting model object\n",
    "        if \"lags\" in params:\n",
    "            if isinstance(params[\"lags\"], int):\n",
    "                model.n_lag = list(range(1, params[\"lags\"] + 1))\n",
    "            elif isinstance(params[\"lags\"], list):\n",
    "                model.n_lag = params[\"lags\"]\n",
    "        if \"box_cox\" in params:\n",
    "            model.box_cox = params[\"box_cox\"]\n",
    "        if \"box_cox_lmda\" in params:\n",
    "            model.lamda = params[\"box_cox_lmda\"]\n",
    "        if \"box_cox_biasadj\" in params:\n",
    "            model.biasadj = params[\"box_cox_biasadj\"]\n",
    "\n",
    "    def _get_model_params_for_fit(params):\n",
    "        # Exclude special parameters that should not be passed to the model constructor\n",
    "        skip_keys = {\"box_cox\", \"lags\", \"box_cox_lmda\", \"box_cox_biasadj\"}\n",
    "        if lag_space is not None:\n",
    "            skip_keys.update([f\"lag_{i}\" for i in range(1, lag_space+1)])\n",
    "        if transform_space is not None:\n",
    "            skip_keys.update([t.get_name() for t in transform_space])\n",
    "\n",
    "        return {k: v for k, v in params.items() if k not in skip_keys}\n",
    "    \n",
    "    if lag_space is not None:\n",
    "        lags_to_consider = list(range(1, lag_space + 1))\n",
    "        if starting_lags is not None:\n",
    "            # if list of starting_lags is provided, ensure they are not in lags_to_consider\n",
    "            lags_to_consider = [lag for lag in lags_to_consider if lag not in starting_lags]\n",
    "\n",
    "        lag_postions = {f\"lag_{i}\": hp.choice(f\"lag_{i}\", [0, 1]) for i in lags_to_consider}\n",
    "        search_space = {**lag_postions, **param_space}\n",
    "    else:\n",
    "        search_space = {**param_space}\n",
    "\n",
    "    if transform_space is not None:\n",
    "        transform_positions = {t.get_name(): hp.choice(t.get_name(), [0, 1]) for t in transform_space}\n",
    "        search_space = {**transform_positions, **search_space}\n",
    "\n",
    "    def objective(params):\n",
    "        _set_model_params(params)\n",
    "        if isinstance(model.model, LinearRegression):\n",
    "            # For LinearRegression, we don't need to set model_params\n",
    "            model_params = None\n",
    "        else:\n",
    "            # For other models, get the parameters to set\n",
    "            model_params = _get_model_params_for_fit(params)\n",
    "\n",
    "        if lag_space is not None:\n",
    "            if starting_lags is not None:\n",
    "                selected_lags = starting_lags + [i for i in lags_to_consider if params[f\"lag_{i}\"] == 1]\n",
    "            else:\n",
    "                selected_lags = [i for i in lags_to_consider if params[f\"lag_{i}\"] == 1]\n",
    "            model.n_lag = selected_lags\n",
    "\n",
    "        if transform_space is not None:\n",
    "            selected_transforms = [t for t in transform_space if params[t.get_name()] == 1]\n",
    "            model.lag_transform = selected_transforms\n",
    "\n",
    "                # Optional: penalize too few lags\n",
    "        if lag_space is not None:\n",
    "            if len(selected_lags) < 1:\n",
    "                return {\"loss\": 1e6, \"status\": STATUS_OK}\n",
    "\n",
    "        metrics = []\n",
    "        for train_index, test_index in tscv.split(df):\n",
    "            train, test = df.iloc[train_index], df.iloc[test_index]\n",
    "            x_test = test.drop(columns=[model.target_col])\n",
    "            y_test = np.array(test[model.target_col])\n",
    "\n",
    "            if model_params is not None:\n",
    "                model.model.set_params(**model_params)\n",
    "            model.fit(train)\n",
    "            \n",
    "            y_pred = model.forecast(len(y_test), x_test)\n",
    "\n",
    "            #Evaluate using the specified metric\n",
    "            if eval_metric.__name__ in [\"MASE\", \"SMAE\", \"SRMSE\", \"RMSSE\"]:\n",
    "                score = eval_metric(y_test[-opt_horizon:] if opt_horizon else y_test,\n",
    "                                    y_pred[-opt_horizon:] if opt_horizon else y_pred,\n",
    "                                    train[model.target_col])\n",
    "            else:\n",
    "                score = eval_metric(\n",
    "                        y_test[-opt_horizon:] if opt_horizon else y_test,\n",
    "                        y_pred[-opt_horizon:] if opt_horizon else y_pred,\n",
    "                    )\n",
    "            metrics.append(score)\n",
    "\n",
    "        mean_score = np.mean(metrics)\n",
    "        if verbose:\n",
    "            print(\"Score:\", mean_score)\n",
    "        return {\"loss\": mean_score, \"status\": STATUS_OK}\n",
    "\n",
    "    trials = Trials()\n",
    "    best_hyperparams = fmin(\n",
    "        fn=objective,\n",
    "        space=search_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=eval_num,\n",
    "        trials=trials,\n",
    "    )\n",
    "\n",
    "    model.tuned_params = [\n",
    "        space_eval(param_space, {k: v[0] for k, v in t[\"misc\"][\"vals\"].items()})\n",
    "        for t in trials.trials\n",
    "    ]\n",
    "\n",
    "    # Extract and sort lag values\n",
    "    best_lag_values = []\n",
    "    best_transforms = []\n",
    "    if lag_space is not None:\n",
    "        best_lag_values = sorted(int(k.split(\"_\")[1]) for k, v in best_hyperparams.items() if k.startswith(\"lag_\") and v == 1)\n",
    "        if starting_lags is not None:\n",
    "            best_lag_values = sorted(list(set(best_lag_values).union(set(starting_lags))))\n",
    "            \n",
    "    if transform_space is not None:\n",
    "        best_transform_orig = [v for v in best_hyperparams.values() if v in transform_space]\n",
    "        # get the name of transforms\n",
    "        best_transforms = [t.get_name() for t in best_transform_orig]\n",
    "    return space_eval(param_space, best_hyperparams), best_lag_values, best_transforms\n",
    "\n",
    "def mv_cv_tune(\n",
    "    model,\n",
    "    df,\n",
    "    forecast_col,\n",
    "    cv_split,\n",
    "    test_size,\n",
    "    eval_metric,\n",
    "    param_space,\n",
    "    lag_space=None,\n",
    "    transform_space=None,\n",
    "    step_size=None,\n",
    "    opt_horizon=None,\n",
    "    eval_num=100,\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Tune forecasting model hyperparameters using cross-validation and Bayesian optimization.\n",
    "\n",
    "    Args:\n",
    "        model: Forecasting model object with .fit and .forecast methods and relevant attributes.\n",
    "        df (pd.DataFrame): Time series dataframe.\n",
    "        forecast_col (str): Name of the target variable to test.\n",
    "        cv_split (int): Number of time series splits.\n",
    "        test_size (int): Size of test window for each split.\n",
    "        step_size (int): Step size for moving the window. Defaults to None (equal to test_size).\n",
    "        param_space (dict): Hyperopt parameter search space. params for lags, differencing, etc. can be {'n_lag': (hp.choice('lag_y1', [1,2,3]), hp.choice('lag_y2', [1,2]))}\n",
    "        lag_space (dict, optional): Lag hyperparameter search space.\n",
    "        transform_space (dict, optional): Transformation hyperparameter search space.\n",
    "        eval_metric (callable): Evaluation metric function.\n",
    "        opt_horizon (int, optional): Evaluate only on last N points of each split. Defaults to None (all points).\n",
    "        eval_num (int, optional): Number of hyperopt evaluations. Defaults to 100.\n",
    "        verbose (bool, optional): Print progress. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        dict: Best hyperparameter values found.\n",
    "    \"\"\"\n",
    "\n",
    "    # target_cols = model.target_cols\n",
    "    tscv = ParametricTimeSeriesSplit(n_splits=cv_split, test_size=test_size, step_size=step_size)\n",
    "\n",
    "\n",
    "    def _set_model_params(params):\n",
    "        # Handle special model parameters that are not passed to model constructor\n",
    "        # and must be set directly on the forecasting model object\n",
    "        if \"n_lag\" in params:\n",
    "            if isinstance(params[\"n_lag\"], dict):\n",
    "                if model.n_lag is None:\n",
    "                    model.n_lag = {}\n",
    "                # If n_lag is a dict, set it for each target column\n",
    "                for target_col, lags in params[\"n_lag\"].items():\n",
    "                    if isinstance(lags, int):\n",
    "                        model.n_lag[target_col] = list(range(1, lags + 1))\n",
    "                    elif isinstance(lags, list):\n",
    "                        model.n_lag[target_col] = lags\n",
    "\n",
    "        if \"box_cox\" in params:\n",
    "            if isinstance(params[\"box_cox\"], dict):\n",
    "                for target_col, box_cox in params[\"box_cox\"].items():\n",
    "                    model.box_cox[target_col] = box_cox\n",
    "        if \"box_cox_lmda\" in params:\n",
    "            if isinstance(params[\"box_cox_lmda\"], dict):\n",
    "                for target_col, lamda in params[\"box_cox_lmda\"].items():\n",
    "                    model.lamda[target_col] = lamda\n",
    "        if \"box_cox_biasadj\" in params:\n",
    "            if isinstance(params[\"box_cox_biasadj\"], dict):\n",
    "                for target_col, biasadj in params[\"box_cox_biasadj\"].items():\n",
    "                    model.biasadj[target_col] = biasadj\n",
    "\n",
    "    def _get_model_params_for_fit(params):\n",
    "        # Exclude special parameters that should not be passed to the model constructor\n",
    "        skip_keys = {\n",
    "            \"box_cox\", \"n_lag\", \"box_cox_lmda\", \"box_cox_biasadj\"}\n",
    "        if lag_space is not None:\n",
    "            for var, max_lag in lag_space.items():\n",
    "                skip_keys.update([f\"{var}_lag_{i}\" for i in range(1, max_lag + 1)])\n",
    "        if transform_space is not None:\n",
    "            for var, trans in transform_space.items():\n",
    "                skip_keys.update([f\"{var}_{t.get_name()}\" for t in trans])\n",
    "\n",
    "        return {k: v for k, v in params.items() if k not in skip_keys}\n",
    "\n",
    "    if lag_space is not None:\n",
    "        lag_positions = {}\n",
    "        for var, max_lag in lag_space.items():\n",
    "            for i in range(1, max_lag + 1):\n",
    "                lag_positions[f\"{var}_lag_{i}\"] = hp.choice(f\"{var}_lag_{i}\", [0, 1])\n",
    "        search_space = {**lag_positions, **param_space}\n",
    "    else:\n",
    "        search_space = {**param_space}\n",
    "\n",
    "    if transform_space is not None:\n",
    "        transform_positions = {}\n",
    "        for var, t in transform_space.items():\n",
    "            transform_positions[f\"{var}_{t.get_name()}\"] = hp.choice(t.get_name(), [0, 1])\n",
    "        search_space = {**transform_positions, **search_space}\n",
    "\n",
    "    def objective(params):\n",
    "        _set_model_params(params)\n",
    "        if isinstance(model.model, LinearRegression):\n",
    "            # For LinearRegression, we don't need to set model_params\n",
    "            model_params = None\n",
    "        else:\n",
    "            # For other models, get the parameters to set\n",
    "            model_params = _get_model_params_for_fit(params)\n",
    "\n",
    "        # Set lagged features\n",
    "        if lag_space is not None:\n",
    "            n_lag_dict = {}\n",
    "            for var, max_lag in lag_space.items():\n",
    "                selected = [i for i in range(1, max_lag + 1) if params[f\"{var}_lag_{i}\"] == 1]\n",
    "                if selected:   # keep only if some lags chosen\n",
    "                    n_lag_dict[var] = selected\n",
    "            model.n_lag = n_lag_dict\n",
    "            # penalize if no lag is selected for any target\n",
    "            if all(len(lags) == 0 for lags in n_lag_dict.values()):\n",
    "                model.n_lag = None\n",
    "\n",
    "        # Set transformations\n",
    "        if transform_space is not None:\n",
    "            transform_dict = {}\n",
    "            for var, trans in transform_space.items():\n",
    "                selected_trans = [t for t in trans if params[f\"{var}_{t.get_name()}\"] == 1]\n",
    "                if selected_trans:  # keep only if some transformations chosen\n",
    "                    transform_dict[var] = selected_trans\n",
    "            model.transform = transform_dict\n",
    "            if all(len(t) == 0 for t in transform_dict.values()):\n",
    "                model.transform = None\n",
    "\n",
    "        metrics = []\n",
    "        for train_index, test_index in tscv.split(df):\n",
    "            train, test = df.iloc[train_index], df.iloc[test_index]\n",
    "            x_test = test.drop(columns=model.target_cols)\n",
    "            y_test = np.array(test[forecast_col])\n",
    "\n",
    "            if model_params is not None:\n",
    "                model.model.set_params(**model_params)\n",
    "            model.fit(train)\n",
    "\n",
    "            y_pred = model.forecast(len(y_test), x_test)[forecast_col]\n",
    "\n",
    "            #Evaluate using the specified metric\n",
    "            if eval_metric.__name__ in [\"MASE\", \"SMAE\", \"SRMSE\", \"RMSSE\"]:\n",
    "                score = eval_metric(y_test[-opt_horizon:] if opt_horizon else y_test,\n",
    "                                    y_pred[-opt_horizon:] if opt_horizon else y_pred,\n",
    "                                    train[model.target_col])\n",
    "            else:\n",
    "                score = eval_metric(\n",
    "                        y_test[-opt_horizon:] if opt_horizon else y_test,\n",
    "                        y_pred[-opt_horizon:] if opt_horizon else y_pred,\n",
    "                    )\n",
    "            metrics.append(score)\n",
    "\n",
    "        mean_score = np.mean(metrics)\n",
    "        if verbose:\n",
    "            print(\"Score:\", mean_score)\n",
    "        return {\"loss\": mean_score, \"status\": STATUS_OK}\n",
    "\n",
    "    trials = Trials()\n",
    "    best_hyperparams = fmin(\n",
    "        fn=objective,\n",
    "        space=search_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=eval_num,\n",
    "        trials=trials,\n",
    "    )\n",
    "\n",
    "    model.tuned_params = [\n",
    "        space_eval(param_space, {k: v[0] for k, v in t[\"misc\"][\"vals\"].items()})\n",
    "        for t in trials.trials\n",
    "    ]\n",
    "\n",
    "    best_lags = {}\n",
    "    if lag_space is not None:\n",
    "        for var, max_lag in lag_space.items():\n",
    "            chosen = [i for i in range(1, max_lag + 1) if best_hyperparams[f\"{var}_lag_{i}\"] == 1]\n",
    "            best_lags[var] = chosen\n",
    "    best_transforms = {}\n",
    "    if transform_space is not None:\n",
    "        for var, trans in transform_space.items():\n",
    "            chosen = [t.get_name() for t in trans if best_hyperparams[f\"{var}_{t.get_name()}\"] == 1]\n",
    "            best_transforms[var] = chosen\n",
    "\n",
    "    return space_eval(param_space, best_hyperparams), best_lags, best_transforms\n",
    "\n",
    "def cv_lag_tune(\n",
    "    model,\n",
    "    df,\n",
    "    cv_split,\n",
    "    test_size,\n",
    "    eval_metric,\n",
    "    lag_space=None,\n",
    "    step_size=None,\n",
    "    opt_horizon=None,\n",
    "    eval_num=100,\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Tune forecasting model hyperparameters using cross-validation and Bayesian optimization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : object\n",
    "        Forecasting model object with .fit and .forecast methods and relevant attributes.\n",
    "    df : pd.DataFrame\n",
    "        Time series dataframe.\n",
    "    cv_split : int\n",
    "        Number of time series splits.\n",
    "    test_size : int\n",
    "        Size of test window for each split.\n",
    "    lag_space : dict\n",
    "        Hyperopt lag search space.\n",
    "    eval_metric : callable\n",
    "        Evaluation metric function.\n",
    "    step_size : int, optional\n",
    "        Step size for moving the window. Defaults to None (equal to test_size).\n",
    "    opt_horizon : int, optional\n",
    "        Evaluate only on last N points of each split. Defaults to None (all points).\n",
    "    eval_num : int, optional\n",
    "        Number of hyperopt evaluations. Defaults to 100.\n",
    "    verbose : bool, optional\n",
    "        Print progress. Defaults to False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Best hyperparameter values found.\n",
    "    \"\"\"\n",
    "    tscv = ParametricTimeSeriesSplit(n_splits=cv_split, test_size=test_size, step_size=step_size)\n",
    "    max_lag = lag_space\n",
    "    space = {f\"lag_{i}\": hp.choice(f\"lag_{i}\", [0, 1]) for i in range(1, max_lag + 1)}\n",
    "    def objective(params):\n",
    "\n",
    "        selected_lags = [i for i in range(1, max_lag + 1) if params[f\"lag_{i}\"] == 1]\n",
    "        model_ = model.copy()\n",
    "        model_.n_lag = selected_lags\n",
    "\n",
    "                # Optional: penalize too few lags\n",
    "        if len(selected_lags) < 1:\n",
    "            return {\"loss\": 1e6, \"status\": STATUS_OK}\n",
    "\n",
    "        metrics = []\n",
    "        for train_index, test_index in tscv.split(df):\n",
    "            train, test = df.iloc[train_index], df.iloc[test_index]\n",
    "            x_test = test.drop(columns=[model.target_col])\n",
    "            y_test = np.array(test[model.target_col])\n",
    "            model_.fit(train)\n",
    "\n",
    "            y_pred = model_.forecast(len(y_test), x_test)\n",
    "\n",
    "            #Evaluate using the specified metric\n",
    "            if eval_metric.__name__ in [\"MASE\", \"SMAE\", \"SRMSE\", \"RMSSE\"]:\n",
    "                score = eval_metric(y_test[-opt_horizon:] if opt_horizon else y_test,\n",
    "                                    y_pred[-opt_horizon:] if opt_horizon else y_pred,\n",
    "                                    train[model.target_col])\n",
    "            else:\n",
    "                score = eval_metric(\n",
    "                        y_test[-opt_horizon:] if opt_horizon else y_test,\n",
    "                        y_pred[-opt_horizon:] if opt_horizon else y_pred,\n",
    "                    )\n",
    "            metrics.append(score)\n",
    "\n",
    "        mean_score = np.mean(metrics)\n",
    "        if verbose:\n",
    "            print(\"Score:\", mean_score)\n",
    "        return {\"loss\": mean_score, \"status\": STATUS_OK}\n",
    "\n",
    "    trials = Trials()\n",
    "    best_hyperparams = fmin(\n",
    "        fn=objective,\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=eval_num,\n",
    "        trials=trials,\n",
    "    )\n",
    "\n",
    "    # Extract and sort lag values\n",
    "    best_lag_indexes = [value for key, value in sorted(((k, v) for k, v in best_hyperparams.items() if k.startswith(\"lag_\")),\n",
    "                                                          key=lambda x: int(x[0].split(\"_\")[1]))]\n",
    "    best_lag_values = [i for i in range(1, lag_space + 1) if best_lag_indexes[i-1]==1]\n",
    "    return best_lag_values\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# HMM CV utility function\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def hmm_cross_validate(model, df, cv_split,\n",
    "                       test_size, metrics,\n",
    "                       n_iter=1, step_size=1, h_split_point=None):\n",
    "    \"\"\"\n",
    "    Run cross-validation using time series splits.\n",
    "\n",
    "    Args:\n",
    "        model (class): Machine learning model class (e.g., CatBoostRegressor, LGBMRegressor).\n",
    "        df (pd.DataFrame): Input data.\n",
    "        cv_split (int): Number of splits in TimeSeriesSplit.\n",
    "        test_size (int): Size of test window.\n",
    "        metrics (list): List of metric functions.\n",
    "        n_iter (int): Number of iterations for HMM fitting.\n",
    "        step_size (int): Step size for time series cross-validation.\n",
    "        h_split_point (int, optional): If provided, split the horizon into two parts for separate evaluation.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Performance metrics for CV.\n",
    "    \"\"\"\n",
    "    tscv = ParametricTimeSeriesSplit(n_splits=cv_split, test_size=test_size, step_size=step_size)\n",
    "    metrics_dict = {m.__name__: [] for m in metrics}\n",
    "    if h_split_point is not None:\n",
    "        metrics_dict1 = {m.__name__: [] for m in metrics}\n",
    "        metrics_dict2 = {m.__name__: [] for m in metrics}\n",
    "    for idx, (train_index, test_index) in enumerate(tscv.split(df)):\n",
    "        train, test = df.iloc[train_index], df.iloc[test_index]\n",
    "        x_test = test.drop(columns=[model.target_col])\n",
    "        y_test = np.array(test[model.target_col])\n",
    "\n",
    "        # If it is first fold, fit the model\n",
    "        # model_ = model.copy()\n",
    "        # if (idx == 0) and (learn_per_fold in [\"first\", \"all\"]):\n",
    "        #     model_.fit_em(train)\n",
    "        # # If learning per fold, learn the model on each fold\n",
    "        # elif (learn_per_fold == \"all\") and (idx != 0):\n",
    "        #     model_.fit_em(train)\n",
    "        # # If not learning per fold, fit the model on the first fold\n",
    "        # else: # learn_per_fold == \"None\" or learn_per_fold == \"first\" for remaining folds\n",
    "        model.fit(train, n_iter=n_iter)\n",
    "\n",
    "        # Forecast using the model\n",
    "        bb_forecast = model.forecast(test_size, x_test)\n",
    "        # Evaluate each metric\n",
    "        for m in metrics:\n",
    "            if m.__name__ in ['MASE', 'SMAE', 'SRMSE', 'RMSSE']:\n",
    "                eval_val = m(y_test, bb_forecast, np.array(train[model.target_col]))\n",
    "            else:\n",
    "                eval_val = m(y_test, bb_forecast)\n",
    "            metrics_dict[m.__name__].append(eval_val)\n",
    "        if h_split_point is not None and isinstance(h_split_point, int):\n",
    "            y_test_1, y_test_2 = y_test[:h_split_point], y_test[h_split_point:]\n",
    "            bb_forecast_1, bb_forecast_2 = bb_forecast[:h_split_point], bb_forecast[h_split_point:]\n",
    "            for m in metrics:\n",
    "                if m.__name__ in ['MASE', 'SMAE', 'SRMSE', 'RMSSE']:\n",
    "                    eval_val1 = m(y_test_1, bb_forecast_1, np.array(train[model.target_col]))\n",
    "                    eval_val2 = m(y_test_2, bb_forecast_2, np.array(train[model.target_col]))\n",
    "                else:\n",
    "                    eval_val1 = m(y_test_1, bb_forecast_1)\n",
    "                    eval_val2 = m(y_test_2, bb_forecast_2)\n",
    "                metrics_dict1[m.__name__].append(eval_val1)\n",
    "                metrics_dict2[m.__name__].append(eval_val2)\n",
    "\n",
    "    overall_performance = [[m.__name__, np.mean(metrics_dict[m.__name__])] for m in metrics]\n",
    "    overall_performance = pd.DataFrame(overall_performance).rename(columns={0: \"eval_metric\", 1: \"score\"})\n",
    "    if h_split_point is not None and isinstance(h_split_point, int):\n",
    "        performance_1 = [[m.__name__, np.mean(metrics_dict1[m.__name__])] for m in metrics]\n",
    "        performance_2 = [[m.__name__, np.mean(metrics_dict2[m.__name__])] for m in metrics]\n",
    "        overall_performance = pd.DataFrame(overall_performance).rename(columns={0: \"eval_metric\", 1: \"score\"})\n",
    "        perf_1_df = pd.DataFrame(performance_1).rename(columns={0: \"eval_metric\", 1: f\"score_before_{h_split_point}\"})\n",
    "        perf_2_df = pd.DataFrame(performance_2).rename(columns={0: \"eval_metric\", 1: f\"score_after_{h_split_point}\"})\n",
    "        # merge all three dataframes\n",
    "        overall_performance = overall_performance.merge(perf_1_df, on=\"eval_metric\").merge(perf_2_df, on=\"eval_metric\")\n",
    "    return overall_performance\n",
    "\n",
    "def hmm_mv_cross_validate(\n",
    "    model,\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str,\n",
    "    cv_split: int,\n",
    "    test_size: int,\n",
    "    metrics: List[Callable],\n",
    "    n_iter: int = 1,\n",
    "    step_size: int = 1,\n",
    "    h_split_point: Optional[int] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform cross-validation for VAR model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : object\n",
    "        VAR model instance with defined target_cols attribute, lag order, and etc.\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe.\n",
    "    target_col : str\n",
    "        Target variable for evaluation.\n",
    "    cv_split : int\n",
    "        Number of cross-validation folds.\n",
    "    test_size : int\n",
    "        Test size per fold.\n",
    "    metrics : List[Callable]\n",
    "        List of metric functions.\n",
    "    step_size : int, optional\n",
    "        Step size for rolling window. Default is 1.\n",
    "    h_split_point : int, optional\n",
    "        Point to split the test set for separate evaluation. Default is None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with averaged cross-validation metric scores.\n",
    "    \"\"\"\n",
    "\n",
    "    tscv = ParametricTimeSeriesSplit(n_splits=cv_split, test_size=test_size, step_size=step_size)\n",
    "    metrics_dict = {m.__name__: [] for m in metrics}\n",
    "    if h_split_point is not None:\n",
    "        metrics_dict1 = {m.__name__: [] for m in metrics}\n",
    "        metrics_dict2 = {m.__name__: [] for m in metrics}\n",
    "\n",
    "    for train_index, test_index in tscv.split(df):\n",
    "        train, test = df.iloc[train_index], df.iloc[test_index]\n",
    "        x_test, y_test = test.drop(columns=model.target_cols), np.array(test[target_col])\n",
    "        model.fit(train, n_iter=n_iter)\n",
    "\n",
    "        forecasts = model.forecast(test_size, x_test)[target_col]\n",
    "\n",
    "        for m in metrics:\n",
    "            if m.__name__ in [\"MASE\", \"SMAE\", \"SRMSE\", \"RMSSE\"]:\n",
    "                eval_val = m(y_test, forecasts, train[target_col])\n",
    "            else:\n",
    "                eval_val = m(y_test, forecasts)\n",
    "            metrics_dict[m.__name__].append(eval_val)\n",
    "        if h_split_point is not None and isinstance(h_split_point, int):\n",
    "            y_test_1, y_test_2 = y_test[:h_split_point], y_test[h_split_point:]\n",
    "            bb_forecast_1, bb_forecast_2 = forecasts[:h_split_point], forecasts[h_split_point:]\n",
    "            for m in metrics:\n",
    "                if m.__name__ in ['MASE', 'SMAE', 'SRMSE', 'RMSSE']:\n",
    "                    eval_val1 = m(y_test_1, bb_forecast_1, np.array(train[model.target_col]))\n",
    "                    eval_val2 = m(y_test_2, bb_forecast_2, np.array(train[model.target_col]))\n",
    "                else:\n",
    "                    eval_val1 = m(y_test_1, bb_forecast_1)\n",
    "                    eval_val2 = m(y_test_2, bb_forecast_2)\n",
    "                metrics_dict1[m.__name__].append(eval_val1)\n",
    "                metrics_dict2[m.__name__].append(eval_val2)\n",
    "\n",
    "    overall_performance = [[m.__name__, np.mean(metrics_dict[m.__name__])] for m in metrics]\n",
    "    overall_performance = pd.DataFrame(overall_performance).rename(columns={0: \"eval_metric\", 1: \"score\"})\n",
    "    if h_split_point is not None and isinstance(h_split_point, int):\n",
    "        performance_1 = [[m.__name__, np.mean(metrics_dict1[m.__name__])] for m in metrics]\n",
    "        performance_2 = [[m.__name__, np.mean(metrics_dict2[m.__name__])] for m in metrics]\n",
    "        overall_performance = pd.DataFrame(overall_performance).rename(columns={0: \"eval_metric\", 1: \"score\"})\n",
    "        perf_1_df = pd.DataFrame(performance_1).rename(columns={0: \"eval_metric\", 1: f\"score_before_{h_split_point}\"})\n",
    "        perf_2_df = pd.DataFrame(performance_2).rename(columns={0: \"eval_metric\", 1: f\"score_after_{h_split_point}\"})\n",
    "        # merge all three dataframes\n",
    "        overall_performance = overall_performance.merge(perf_1_df, on=\"eval_metric\").merge(perf_2_df, on=\"eval_metric\")\n",
    "    return overall_performance\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Parametric Time Series Split\n",
    "#------------------------------------------------------------------------------\n",
    "class ParametricTimeSeriesSplit:\n",
    "    \"\"\"\n",
    "    Rolling window time series cross-validator with fixed step size\n",
    "\n",
    "    Parameters:\n",
    "        test_size (int): Number of test samples in each split.\n",
    "        step_size (int): Number of steps to move backward for each split.\n",
    "        n_splits (int, optional): Number of splits to generate.\n",
    "\n",
    "    Yields:\n",
    "        train_index, test_index: Indices for training and test sets.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_splits, test_size, step_size=None):\n",
    "        self.test_size = test_size\n",
    "        self.step_size = test_size if step_size is None else step_size\n",
    "        self.n_splits = n_splits\n",
    "\n",
    "    def split(self, X):\n",
    "        n_samples = len(X)\n",
    "        split_starts = []\n",
    "        # Start the last test set at the last possible position\n",
    "        last_test_start = n_samples - self.test_size\n",
    "        # Build test starts, moving backward with fixed step_size\n",
    "        current = last_test_start\n",
    "        while current >= 0:\n",
    "            split_starts.append(current)\n",
    "            current -= self.step_size\n",
    "        split_starts = split_starts[::-1]  # Reverse to start from earliest\n",
    "\n",
    "        # Use only the last n_splits\n",
    "        split_starts = split_starts[-self.n_splits:]\n",
    "\n",
    "        for test_start in split_starts:\n",
    "            test_end = test_start + self.test_size\n",
    "            train_index = np.arange(0, test_start)\n",
    "            test_index = np.arange(test_start, test_end)\n",
    "            yield train_index, test_index\n",
    "\n",
    "\n",
    "def prob_param_forecasts(model, H, train_df, test_df=None):\n",
    "    prob_forecasts = []\n",
    "    for params in model.tuned_params:\n",
    "        if ('n_lag' in params) |('differencing_number' in params)|('box_cox' in params)|('box_cox_lmda' in params)|('box_cox_biasadj' in params):\n",
    "            if ('n_lag' in params):\n",
    "                if type(params[\"n_lag\"]) is tuple:\n",
    "                    model.n_lag = list(params[\"n_lag\"])\n",
    "                else:\n",
    "                    model.n_lag = range(1, params[\"n_lag\"]+1)\n",
    "\n",
    "            if ('differencing_number' in params):\n",
    "                model.difference = params[\"differencing_number\"]\n",
    "            if ('box_cox' in params):\n",
    "                model.box_cox = params[\"box_cox\"]\n",
    "            if ('box_cox_lmda' in params):\n",
    "                model.lmda = params[\"box_cox_lmda\"]\n",
    "\n",
    "            if ('box_cox_biasadj' in params):\n",
    "                model.biasadj = params[\"box_cox_biasadj\"]\n",
    "\n",
    "        \n",
    "        if model.model.__name__ != 'LinearRegression':\n",
    "            model_params = {k: v for k, v in params.items() if (k not in [\"box_cox\", \"n_lag\", \"box_cox_lmda\", \"box_cox_biasadj\"])}\n",
    "            model.fit(train_df, model_params)\n",
    "        else:\n",
    "            model.fit(train_df)\n",
    "        if test_df is not None:\n",
    "            forecasts = model.forecast(H, test_df)\n",
    "        else:\n",
    "            forecasts = model.forecast(H)\n",
    "\n",
    "        prob_forecasts.append(forecasts)\n",
    "    prob_forecasts = np.row_stack(prob_forecasts)\n",
    "    prob_forecasts=pd.DataFrame(prob_forecasts)\n",
    "    prob_forecasts.columns = [\"horizon_\"+str(i+1) for i in prob_forecasts.columns]\n",
    "    return prob_forecasts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
