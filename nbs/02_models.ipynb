{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab84d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4834a69d",
   "metadata": {},
   "source": [
    "# ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9662bccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from typing import List, Dict, Optional, Callable, Tuple, Any, Union\n",
    "from sklearn.base import clone\n",
    "from tabnanny import verbose\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.model_selection import TimeSeriesSplit, KFold\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK, space_eval\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, HistGradientBoostingRegressor\n",
    "from peshbeen.transformations import (box_cox_transform, back_box_cox_transform, undiff_ts, seasonal_diff,\n",
    "                        invert_seasonal_diff, kfold_target_encoder, target_encoder_for_test,\n",
    "                        rolling_quantile, rolling_mean, rolling_std,\n",
    "                        expanding_mean, expanding_std, expanding_quantile)\n",
    "from peshbeen.model_selection import ParametricTimeSeriesSplit\n",
    "from peshbeen.statstools import lr_trend_model, forecast_trend\n",
    "from peshbeen.formatting import make_main_gt, gt_mini, inject_header_table_groups, cov_table, make_var_gt_regimes\n",
    "from catboost import CatBoostRegressor\n",
    "from cubist import Cubist\n",
    "# dot not show warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import copy\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm, multivariate_normal\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from scipy.special import logsumexp\n",
    "from scipy.stats import t\n",
    "\n",
    "\n",
    "class ml_forecaster:\n",
    "    \"\"\"\n",
    "    ml Forecaster for time series forecasting.\n",
    "\n",
    "    Args:\n",
    "        model (class): Machine learning model class (e.g., CatBoostRegressor, LGBMRegressor).\n",
    "        target_col (str): Name of the target variable.\n",
    "        cat_variables (list, optional): List of categorical features.\n",
    "        target_encode (bool, optional): Whether to use target encoding for categorical features. Default is False.\n",
    "        lags (list or int, optional): Lag(s) to include as features.\n",
    "        difference (int, optional): Order of difference (e.g. 1 for first difference).\n",
    "        seasonal_length (int, optional): Seasonal period for seasonal differencing.\n",
    "        trend (bool, optional): Whether to remove trend.\n",
    "        ets_params (tuple, optional): A tuple (model_params, fit_params) for exponential smoothing. Ex.g. ({'trend': 'add', 'seasonal': 'add'}, {'damped_trend': True}).\n",
    "        change_points (list, optional): List of change points for piecewise regression if trend is passed as : \"linear\".\n",
    "        pol_degree (int, optional): Degree of polynomial trend if trend is \"linear\". Integer or list of integers. Default is 1.\n",
    "        box_cox (bool, optional): Whether to perform a Box–Cox transformation.\n",
    "        box_cox_lmda (float, optional): The lambda value for Box–Cox.\n",
    "        box_cox_biasadj (bool, optional): If True, adjust bias after Box–Cox inversion. Default is False.\n",
    "        lag_transform (list, optional): List specifying additional lag transformations.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, target_col, cat_variables=None, target_encode=False, lags=None, difference=None, seasonal_diff=None,\n",
    "                 trend=None, ets_params=None, change_points=None, pol_degree=1, box_cox=False, box_cox_lmda=None,\n",
    "                 box_cox_biasadj=False, lag_transform=None):\n",
    "\n",
    "        self.target_col = target_col\n",
    "        self.cat_variables = cat_variables\n",
    "        self.target_encode = target_encode  # whether to use target encoding for categorical variables\n",
    "        self.n_lag = lags\n",
    "        if self.n_lag is not None:\n",
    "            if isinstance(self.n_lag, int):\n",
    "                self.n_lag = list(range(1, self.n_lag + 1))  # convert to list if single integer\n",
    "            elif isinstance(self.n_lag, list):\n",
    "                if not all(isinstance(l, int) for l in self.n_lag):\n",
    "                    raise TypeError(\"n_lag list must contain only integers\")\n",
    "                # No assignment needed here, already a list\n",
    "            else:\n",
    "                raise TypeError(\"n_lag must be an integer or a list of integers\")\n",
    "        self.difference = difference\n",
    "        self.season_diff = seasonal_diff\n",
    "        self.trend = trend\n",
    "        if ets_params is not None:\n",
    "            self.ets_model = ets_params[0]\n",
    "            self.ets_fit = ets_params[1]\n",
    "        else:\n",
    "            self.ets_model = None\n",
    "            self.ets_fit = None\n",
    "        self.cps = change_points\n",
    "        self.pol = pol_degree\n",
    "        self.box_cox = box_cox\n",
    "        self.lamda = box_cox_lmda\n",
    "        self.biasadj = box_cox_biasadj\n",
    "        self.lag_transform = lag_transform\n",
    "        \n",
    "        # Set default tuned parameters and placeholders for fitted attributes\n",
    "        self.tuned_params = None\n",
    "        self.actuals = None\n",
    "        self.prob_forecasts = None\n",
    "        self.model = model  # the chosen ML model\n",
    "    \n",
    "    def data_prep(self, df):\n",
    "        \"\"\"\n",
    "        Prepare the data with lag features, differencing, trend-removal, and Box–Cox transformation.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): Input dataframe.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Dataframe ready for model fitting.\n",
    "        \"\"\"\n",
    "        dfc = df.copy()\n",
    "\n",
    "        # Process categorical variables if provided\n",
    "\n",
    "        if self.cat_variables is not None:\n",
    "            if self.target_encode ==True:\n",
    "                for col in self.cat_variables:\n",
    "                    encode_col = col+\"_target_encoded\"\n",
    "                    dfc[encode_col] = kfold_target_encoder(dfc, col, self.target_col, 36)\n",
    "                self.df_encode = dfc.copy()\n",
    "                dfc = dfc.drop(columns = self.cat_variables)\n",
    "                # If target encoding is not used, convert categories to dummies    \n",
    "            else:\n",
    "                if isinstance(self.model, (CatBoostRegressor, LGBMRegressor)):\n",
    "                    for col in self.cat_variables:\n",
    "                        dfc[col] = dfc[col].astype('category')\n",
    "\n",
    "                else:\n",
    "                    for col, cat in self.cat_var.items():\n",
    "                        dfc[col] = dfc[col].astype('category')\n",
    "                        # Set categories for categorical columns\n",
    "                        dfc[col] = dfc[col].cat.set_categories(cat)\n",
    "                    dfc = pd.get_dummies(dfc)\n",
    "                    if isinstance(self.model, (LinearRegression, Ridge, Lasso, ElasticNet)):\n",
    "                        for i in self.drop_categ:\n",
    "                            dfc.drop(list(dfc.filter(regex=i)), axis=1, inplace=True)\n",
    "\n",
    "        \n",
    "        if self.target_col in dfc.columns:\n",
    "            # Apply Box–Cox transformation if specified\n",
    "            if self.box_cox:\n",
    "                self.is_zero = np.any(np.array(dfc[self.target_col]) < 1) # check for zero or negative values\n",
    "                trans_data, self.lamda = box_cox_transform(x=dfc[self.target_col],\n",
    "                                                        shift=self.is_zero,\n",
    "                                                        box_cox_lmda=self.lamda)\n",
    "                dfc[self.target_col] = trans_data\n",
    "            # Detrend the series if specified\n",
    "            if self.trend is not None:\n",
    "                self.len = len(df)\n",
    "                self.target_orig = dfc[self.target_col] # Store original values for later use during forecasting\n",
    "                if self.trend in [\"linear\", \"feature_lr\"]:\n",
    "                    # self.lr_model = LinearRegression().fit(np.arange(self.len).reshape(-1, 1), dfc[self.target_col])\n",
    "                    if self.cps is not None:\n",
    "                        trend, self.lr_model, X_trend = lr_trend_model(dfc[self.target_col], degree=self.pol, breakpoints=self.cps, type='piecewise')\n",
    "                    else:\n",
    "                        trend, self.lr_model, X_trend = lr_trend_model(dfc[self.target_col], degree=self.pol)\n",
    "                    if self.trend == \"linear\":\n",
    "                        dfc[self.target_col] = dfc[self.target_col] - trend\n",
    "                if self.trend in [\"ets\", \"feature_ets\"]:\n",
    "                    self.ets_model_fit = ExponentialSmoothing(dfc[self.target_col], **self.ets_model).fit(**self.ets_fit)\n",
    "                    if self.trend == \"ets\":\n",
    "                        dfc[self.target_col] = dfc[self.target_col] - self.ets_model_fit.fittedvalues.values\n",
    "\n",
    "            # Apply differencing if specified\n",
    "            if self.difference is not None or self.season_diff is not None:\n",
    "                self.orig = dfc[self.target_col].tolist()\n",
    "                if self.difference is not None:\n",
    "                    dfc[self.target_col] = np.diff(dfc[self.target_col], n=self.difference,\n",
    "                                                prepend=np.repeat(np.nan, self.difference))\n",
    "                if self.season_diff is not None:\n",
    "                    self.orig_d = dfc[self.target_col].tolist()\n",
    "                    dfc[self.target_col] = seasonal_diff(dfc[self.target_col], self.season_diff)\n",
    "\n",
    "            # Create lag features based on n_lag parameter\n",
    "            if self.n_lag is not None:\n",
    "                for lag in self.n_lag:\n",
    "                    dfc[f\"{self.target_col}_lag_{lag}\"] = dfc[self.target_col].shift(lag)\n",
    "            # Create additional lag transformations if specified\n",
    "            if self.lag_transform is not None:\n",
    "                for func in self.lag_transform:\n",
    "                    if isinstance(func, (expanding_std, expanding_mean)):\n",
    "                        dfc[f\"{func.__class__.__name__}_shift_{func.shift}\"] = func(dfc[self.target_col])\n",
    "                    elif isinstance(func, expanding_quantile):\n",
    "                        dfc[f\"{func.__class__.__name__}_shift_{func.shift}_q{func.quantile}\"] = func(dfc[self.target_col])\n",
    "                    elif isinstance(func, rolling_quantile):\n",
    "                        dfc[f\"{func.__class__.__name__}_{func.window_size}_shift_{func.shift}_q{func.quantile}\"] = func(dfc[self.target_col])\n",
    "                    else:\n",
    "                        dfc[f\"{func.__class__.__name__}_{func.window_size}_shift_{func.shift}\"] = func(dfc[self.target_col])\n",
    "            if self.trend is not None:\n",
    "                if self.trend == \"feature_lr\":\n",
    "                    # add X_trend matrix columns as features\n",
    "                    for i in range(X_trend.shape[1]):\n",
    "                        dfc[f\"trend_{i}\"] = X_trend[:, i]\n",
    "                if self.trend == \"feature_ets\":\n",
    "                    dfc[\"trend\"] = self.ets_model.fittedvalues.values\n",
    "        return dfc.dropna()\n",
    "        \n",
    "\n",
    "    def fit(self, df):\n",
    "        \"\"\"\n",
    "        Fit the ml model.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): Input dataframe.\n",
    "        \"\"\"\n",
    "        model_ = self.model\n",
    "\n",
    "        if isinstance(self.model, (XGBRegressor, RandomForestRegressor, Cubist, HistGradientBoostingRegressor, AdaBoostRegressor, LinearRegression, Ridge, Lasso, ElasticNet)):\n",
    "            if (self.cat_variables is not None) and (self.target_encode == False):\n",
    "                # If categorical variables are provided, create a dictionary of categories\n",
    "                self.cat_var = {c: sorted(df[c].drop_duplicates().tolist()) for c in self.cat_variables}\n",
    "                # Create a list of the first category for each categorical variable\n",
    "                if isinstance(self.model, (LinearRegression, Ridge, Lasso, ElasticNet)):\n",
    "                    self.drop_categ= [sorted(df[i].drop_duplicates().tolist(), key=lambda x: x[0])[0] for i in self.cat_variables]\n",
    "\n",
    "        model_df = self.data_prep(df)\n",
    "        self.X = model_df.drop(columns=[self.target_col])\n",
    "        self.y = model_df[self.target_col]\n",
    "        # Fit the model (passing the categorical features if provided)\n",
    "        if isinstance(self.model, LGBMRegressor):\n",
    "            self.model_fit = model_.fit(self.X, self.y, categorical_feature=self.cat_variables)\n",
    "        elif isinstance(self.model, CatBoostRegressor):\n",
    "            self.model_fit = model_.fit(self.X, self.y, cat_features=self.cat_variables, verbose=True)\n",
    "        else:\n",
    "            self.model_fit = model_.fit(self.X, self.y)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def aic(self):\n",
    "        if isinstance(self.model, (LinearRegression, Ridge, Lasso, ElasticNet)):\n",
    "            k = self.X.shape[1] + 1 + 1  # number of parameters: regression coeffs + intercept + variance\n",
    "        else:\n",
    "            k = self.X.shape[1] + 1  # number of parameters: regression coeffs + variance\n",
    "        n = len(self.y)  # effective number of observations\n",
    "        rss = np.sum((self.y.to_numpy() - self.model_fit.predict(self.X))**2)\n",
    "        aic = n * np.log(rss / n) + 2 * k # AIC formula: https://otexts.com/fpp3/selecting-predictors.html\n",
    "        return aic\n",
    "    \n",
    "    @property\n",
    "    def aicc(self):\n",
    "        if isinstance(self.model, (LinearRegression, Ridge, Lasso, ElasticNet)):\n",
    "            k = self.X.shape[1] + 1 + 1  # number of parameters: regression coeffs + intercept + variance\n",
    "        else:\n",
    "            k = self.X.shape[1] + 1  # number of parameters: regression coeffs + variance\n",
    "        n = len(self.y)  # effective number of observations\n",
    "        rss = np.sum((self.y.to_numpy() - self.model_fit.predict(self.X))**2)\n",
    "        aic = n * np.log(rss / n) + 2 * k # AIC formula: https://otexts.com/fpp3/selecting-predictors.html\n",
    "        aicc = aic + (2 * k * (k + 1)) / (n - k - 1) # AICc formula\n",
    "        return aicc\n",
    "\n",
    "    @property\n",
    "    def bic(self):\n",
    "        if isinstance(self.model, (LinearRegression, Ridge, Lasso, ElasticNet)):\n",
    "            k = self.X.shape[1] + 1 + 1  # number of parameters: regression coeffs + intercept + variance\n",
    "        else:\n",
    "            k = self.X.shape[1] + 1  # number of parameters: regression coeffs + variance\n",
    "        n = len(self.y)  # effective number of observations\n",
    "        rss = np.sum((self.y.to_numpy() - self.model_fit.predict(self.X))**2)\n",
    "        bic = n * np.log(rss / n) + k * np.log(n) # BIC formula: https://otexts.com/fpp3/selecting-predictors.html\n",
    "        return bic\n",
    "\n",
    "    @property\n",
    "    def hqc(self):\n",
    "        if isinstance(self.model, (LinearRegression, Ridge, Lasso, ElasticNet)):\n",
    "            k = self.X.shape[1] + 1 + 1  # number of parameters: regression coeffs + intercept + variance\n",
    "        else:\n",
    "            k = self.X.shape[1] + 1  # number of parameters: regression coeffs + variance\n",
    "        n = len(self.y)  # effective number of observations\n",
    "        rss = np.sum((self.y.to_numpy() - self.model_fit.predict(self.X))**2)\n",
    "        hqic = n * np.log(rss / n) + 2 * k * np.log(np.log(n)) # HQIC formula\n",
    "        return hqic\n",
    "\n",
    "    def copy(self):\n",
    "        return copy.deepcopy(self)\n",
    "\n",
    "    def forecast(self, H, exog=None):\n",
    "        \"\"\"\n",
    "        Forecast H time steps.\n",
    "\n",
    "        Args:\n",
    "            H (int): Number of forecast steps.\n",
    "            exog (pd.DataFrame, optional): Exogenous variables for forecasting.\n",
    "\n",
    "        Returns:\n",
    "            np.array: Forecasted values.\n",
    "        \"\"\"\n",
    "        if exog is not None:  # if external regressors are provided\n",
    "            if self.cat_variables is not None:\n",
    "                if self.target_encode:\n",
    "                    for col in self.cat_variables:\n",
    "                        encode_col = col + \"_target_encoded\"\n",
    "                        exog[encode_col] = target_encoder_for_test(self.df_encode, exog, col)\n",
    "                    exog = exog.drop(columns=self.cat_variables)\n",
    "                else:\n",
    "                    if isinstance(self.model, (XGBRegressor, RandomForestRegressor, Cubist, HistGradientBoostingRegressor, AdaBoostRegressor, LinearRegression, Ridge, Lasso, ElasticNet)):\n",
    "                        exog = self.data_prep(exog)\n",
    "\n",
    "        lags = self.y.tolist() # to keep the latest values for lag features\n",
    "        predictions = []\n",
    "        \n",
    "        # Compute trend forecasts if needed\n",
    "        if self.trend:\n",
    "            # orig_lags = self.target_orig.tolist()\n",
    "            if self.trend in [\"feature_lr\", \"linear\"]:\n",
    "                # future_time = np.arange(self.len, self.len + H).reshape(-1, 1)\n",
    "                # trend_forecast = np.array(self.lr_model.predict(future_time)) # Predicting trend\n",
    "                trend_forecast, X_trend_forecast = forecast_trend(model = self.lr_model, H=H, start=self.len, degree=self.pol, breakpoints=self.cps)\n",
    "            else:  # ets or feature_ets\n",
    "                trend_forecast = np.array(self.ets_model_fit.forecast(H))\n",
    "\n",
    "        for i in range(H):\n",
    "            # If external regressors are provided, extract the i-th row\n",
    "            if exog is not None:\n",
    "                x_var = exog.iloc[i, :].tolist()\n",
    "            else:\n",
    "                x_var = []\n",
    "\n",
    "            # Build lag-based features from the latest forecast–history\n",
    "            inp_lag = []\n",
    "            if self.n_lag is not None:\n",
    "                inp_lag.extend([lags[-lag] for lag in self.n_lag])\n",
    "\n",
    "            # Similarly compute additional lag transforms if available\n",
    "            transform_lag = []\n",
    "            if self.lag_transform is not None:\n",
    "                transform_lag = []\n",
    "                series_array = np.array(lags)\n",
    "                for func in self.lag_transform:\n",
    "                    transform_lag.append(func(series_array, is_forecast=True).to_numpy()[-1])\n",
    "                \n",
    "                \n",
    "            # If using trend as a feature, add the forecasted trend component\n",
    "            trend_var = []\n",
    "            if self.trend is not None:\n",
    "                if self.trend in [\"feature_ets\", \"feature_lr\"]:\n",
    "                    trend_var.extend(X_trend_forecast[i, :].tolist() if self.trend == \"feature_lr\" else [trend_forecast[i]])\n",
    "\n",
    "            # Concatenate all features for the forecast step\n",
    "            inp = x_var + inp_lag + transform_lag + trend_var\n",
    "            # Ensure that the input is a DataFrame with the same columns as the training data\n",
    "            df_inp = pd.DataFrame(np.array(inp).reshape(1, -1), columns=self.X.columns)\n",
    "            if isinstance(self.model, (LGBMRegressor, CatBoostRegressor)):\n",
    "                df_inp = df_inp.astype({col: 'category' if col in self.cat_variables else 'float64' for col in df_inp.columns})\n",
    "            # Get the forecast via the model\n",
    "            pred = self.model_fit.predict(df_inp)[0]\n",
    "            lags.append(pred)  # update lag history\n",
    "            predictions.append(pred)\n",
    "\n",
    "        forecasts = np.array(predictions)\n",
    "        # If trend as ets is applied, add the trend component (ets_forecast) to the prediction\n",
    "        if self.trend is not None:\n",
    "            if self.trend in [\"ets\", \"linear\"]:\n",
    "                forecasts += trend_forecast\n",
    "        # Revert seasonal differencing if applied\n",
    "        if self.season_diff is not None:\n",
    "            forecasts = invert_seasonal_diff(self.orig_d, forecasts, self.season_diff)\n",
    "        # Revert ordinary differencing if applied\n",
    "        if self.difference is not None:\n",
    "            forecasts = undiff_ts(self.orig, forecasts, self.difference)\n",
    "        # Ensure forecasts are nonnegative\n",
    "        forecasts = np.array([max(0, x) for x in forecasts])\n",
    "        # Finally, invert Box-Cox transform if it was applied\n",
    "        if self.box_cox:\n",
    "            forecasts = back_box_cox_transform(y_pred=forecasts, lmda=self.lamda,\n",
    "                                                shift=self.is_zero,\n",
    "                                                box_cox_biasadj=self.biasadj)\n",
    "        return forecasts\n",
    "\n",
    "class VARModel:\n",
    "    \"\"\"\n",
    "    Vector Autoregressive Model class supporting data preprocessing, fitting, forecasting, and cross-validation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    target_cols : List[str]\n",
    "        Target columns for the VAR model (dependent variables).\n",
    "    lags : Dict[str, List[int]]\n",
    "        Dictionary specifying lags for each target variable. For example, {'target1': [1, 2], 'target2': [1, 2, 3]} or {'target1': 3, 'target2': 5}.\n",
    "    lag_transform : Optional[Dict[str, List[tuple]]] (default=None)\n",
    "        Dictionary specifying lag transformations per target (e.g. rolling, quantile), each as a list of tuples:\n",
    "        (lag, func, window, [quantile]). For example, { 'target1': [(1, rolling_mean, 30), (2, rolling_quantile, 30, 0.5)] }.\n",
    "    difference : Optional[Dict[str, int]] (default=None)\n",
    "        Dictionary specifying order of differencing for each variable. For example, {'target1': 1, 'target2': 2}.\n",
    "    seasonal_diff : Optional[Dict[str, int]] (default=None)\n",
    "        Dictionary specifying seasonal differencing for each variable. For example, {'target1': 12, 'target2': 7}.\n",
    "    trend : Optional[Dict[str, bool]] (default=None)\n",
    "        Dictionary specifying trend type for each variable: \"linear\", \"ses\", \"feature_lr\", or \"feature_ses\".\n",
    "    pol_degree : Optional[Dict[str, Union[int, List[int]]] (default=1)\n",
    "        Dictionary specifying the degree of the polynomial trend for each variable when type is \"linear\". Default is 1 for all variables.\n",
    "    ets_params : Optional[Dict[str, tuple]] (default=None)\n",
    "        Dictionary specifying params for ExponentialSmoothing per variable.\n",
    "        For example, {'target1': ({'trend': 'add', 'seasonal': 'add'}, {'damped_trend': True}), 'target2': ({'trend': 'add', 'seasonal': 'add'}, {'damped_trend': True})}.\n",
    "    cps: Optional[Dict[str, List[int]]] (default=None)\n",
    "        Dictionary specifying change points for each variable.\n",
    "    box_cox : Optional[Dict[str, bool]] (default=None)\n",
    "        Dictionary specifying which variables require Box-Cox transform.\n",
    "    box_cox_lmda : Optional[Dict[str, float]] (default=None)\n",
    "        Dictionary of Box-Cox lambdas for each variable.\n",
    "    box_cox_biasadj : bool or Dict[str, bool] (default=False)\n",
    "        Whether to use bias adjustment for Box-Cox for each variable.\n",
    "    add_constant : bool (default=True)\n",
    "        If True, add a constant column to exogenous variables.\n",
    "    cat_variables : Optional[List[str]] (default=None)\n",
    "        List of categorical columns to one-hot encode.\n",
    "    verbose : bool (default=False)\n",
    "        If True, print verbose messages.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    data_prep(df)\n",
    "        Prepare the data for VAR model.\n",
    "    fit(df_train)\n",
    "        Fit the VAR model to training data.\n",
    "    forecast(H, exog=None)\n",
    "        Forecast H steps ahead.\n",
    "    predict(X)\n",
    "        Predict with model coefficients.\n",
    "    cv_var(df, target_col, cv_split, test_size, metrics)\n",
    "        Cross-validate VAR model.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Assumes external utility functions exist for Box-Cox, seasonal differencing, etc.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_cols: List[str],\n",
    "        lags: Dict[str, List[int]],\n",
    "        lag_transform: Optional[Dict[str, List[tuple]]] = None,\n",
    "        difference: Optional[Dict[str, int]] = None,\n",
    "        seasonal_diff: Optional[Dict[str, int]] = None,\n",
    "        trend: Optional[Dict[str, bool]] = None,\n",
    "        pol_degree: Optional[Dict[str, int]] = 1,\n",
    "        ets_params: Optional[Dict[str, tuple]] = None,\n",
    "        change_points: Optional[Dict[str, List[int]]] = None,\n",
    "        box_cox: Optional[Dict[str, bool]] = None,\n",
    "        box_cox_lmda: Optional[Dict[str, float]] = None,\n",
    "        box_cox_biasadj: Any = False,\n",
    "        add_constant: bool = True,\n",
    "        cat_variables: Optional[List[str]] = None,\n",
    "        verbose: bool = False\n",
    "    ):\n",
    "        self.target_cols = target_cols\n",
    "        self.n_lag = lags\n",
    "        self.lag_transform = lag_transform\n",
    "        self.diffs = difference\n",
    "        self.season_diffs = seasonal_diff\n",
    "        self.ets_params = ets_params\n",
    "        self.cps = change_points\n",
    "        self.box_cox = box_cox\n",
    "        self.lamdas = box_cox_lmda\n",
    "        self.biasadj = box_cox_biasadj\n",
    "        self.cons = add_constant\n",
    "        self.cat_variables = cat_variables\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Handle box_cox bias adjustment dict default\n",
    "        if self.box_cox is not None and not isinstance(self.box_cox, dict):\n",
    "            raise TypeError(\"box_cox must be a dictionary of target values\")\n",
    "        if isinstance(self.box_cox, dict) and not isinstance(self.biasadj, dict):\n",
    "            self.biasadj = {k: False for k in self.box_cox}\n",
    "        \n",
    "        # Handle trend default types\n",
    "        self.trend = trend\n",
    "        if isinstance(pol_degree, int):\n",
    "            self.pol = {col: pol_degree for col in self.target_cols}\n",
    "        elif isinstance(pol_degree, dict):\n",
    "            self.pol = {col: pol_degree[col] if col in pol_degree else 1 for col in self.target_cols}\n",
    "        else:\n",
    "            raise ValueError(\"pol_degree must be an integer or a dictionary mapping target columns to integer degrees.\")\n",
    "        if self.trend is not None:\n",
    "            if not isinstance(self.trend, dict):\n",
    "                raise TypeError(\"trend must be a dictionary of target values\")\n",
    "\n",
    "    def data_prep(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Prepare the data according to the model configuration.\n",
    "        Applies categorical encoding, Box-Cox, detrending, differencing, seasonal differencing, lags, and lag transforms.\n",
    "        Drops rows with any NaN after transformation.\n",
    "        \"\"\"\n",
    "        dfc = df.copy()\n",
    "        # Handle categorical variables\n",
    "        if self.cat_variables is not None:\n",
    "            for col, cats in self.cat_var.items():\n",
    "                dfc[col] = pd.Categorical(dfc[col], categories=cats)\n",
    "            dfc = pd.get_dummies(dfc, dtype=float)\n",
    "            for i in self.drop_categ:\n",
    "                dfc.drop(list(dfc.filter(regex=i)), axis=1, inplace=True)\n",
    "        \n",
    "        # Check all target columns exist\n",
    "        if all(col in dfc.columns for col in self.target_cols):\n",
    "\n",
    "            # Box-Cox transformation\n",
    "            if self.box_cox is not None:\n",
    "                if self.lamdas is None:\n",
    "                    self.lamdas = {i: None for i in self.box_cox}\n",
    "                self.is_zeros = {i: None for i in self.lamdas}\n",
    "                for k, lm in self.lamdas.items():\n",
    "                    self.is_zeros[k] = (dfc[k] < 1).any()\n",
    "                    trans_data, self.lamdas[k] = box_cox_transform(\n",
    "                        x=dfc[k], shift=self.is_zeros[k], box_cox_lmda=lm\n",
    "                    )\n",
    "                    if self.box_cox.get(k, False):\n",
    "                        dfc[k] = trans_data\n",
    "\n",
    "            # Detrending\n",
    "            if self.trend is not None:\n",
    "                self.len = df.shape[0]\n",
    "                self.orig_targets = {i: dfc[i] for i in self.trend.keys()}  # Store original values for later use during forecasting\n",
    "                self.trend_models = {}\n",
    "                for k, v in self.trend.items():\n",
    "                    if v == \"linear\": # If trend removal is required for this target\n",
    "                        # trend, model_fit = LinearRegression().fit(np.arange(self.len).reshape(-1, 1), self.orig_targets[k])\n",
    "                        # dfc[k] = dfc[k] - model_fit.predict(np.arange(self.len).reshape(-1, 1))\n",
    "                        # self.trend_models[k] = model_fit\n",
    "\n",
    "                            # self.lr_model = LinearRegression().fit(np.arange(self.len).reshape(-1, 1), dfc[self.target_col])\n",
    "                        if self.cps is not None:\n",
    "                            if k in self.cps and self.cps[k]:\n",
    "                                trend, model_fit = lr_trend_model(self.orig_targets[k], degree=self.pol[k], breakpoints=self.cps[k], type='piecewise')\n",
    "                        else:\n",
    "                            trend, model_fit = lr_trend_model(self.orig_targets[k], degree=self.pol[k])\n",
    "\n",
    "                        dfc[k] = dfc[k] - trend\n",
    "                        self.trend_models[k] = model_fit\n",
    "\n",
    "                    elif v == \"ets\": # ets\n",
    "                        model_fit = ExponentialSmoothing(self.orig_targets[k], **self.ets_params[k][0]).fit(**self.ets_params[k][1])\n",
    "                        dfc[k] = dfc[k] - model_fit.fittedvalues.values\n",
    "                        self.trend_models[k] = model_fit\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unknown trend type: {v} for target {k}. Use 'linear' or 'ets'.\")\n",
    "\n",
    "            # Differencing\n",
    "            if self.diffs is not None:\n",
    "                self.origs = {i: dfc[i].tolist() for i in self.diffs}\n",
    "                for x, d in self.diffs.items():\n",
    "                    dfc[x] = np.diff(dfc[x], n=d, prepend=np.repeat(np.nan, d))\n",
    "\n",
    "            # Seasonal differencing\n",
    "            if self.season_diffs is not None:\n",
    "                self.orig_ds = {i: dfc[i].tolist() for i in self.season_diffs}\n",
    "                for w, s in self.season_diffs.items():\n",
    "                    dfc[w] = seasonal_diff(dfc[w], s)\n",
    "\n",
    "            # Lag features\n",
    "            if self.n_lag is not None:\n",
    "                for a, lags in self.n_lag.items():\n",
    "                    lag_used = lags if isinstance(lags, list) else range(1, lags + 1) # Ensure lags is a list, even if a single int\n",
    "                    for lg in lag_used:\n",
    "                        dfc[f\"{a}_lag_{lg}\"] = dfc[a].shift(lg)\n",
    "\n",
    "            # Lag transforms\n",
    "\n",
    "            if self.lag_transform is not None:\n",
    "                for idx, (target, funcs) in enumerate(self.lag_transform.items()):\n",
    "                    for func in funcs:\n",
    "                        if isinstance(func, (expanding_std, expanding_mean)):\n",
    "                            dfc[f\"trg{idx}_{func.__class__.__name__}_shift_{func.shift}\"] = func(dfc[target])\n",
    "                        elif isinstance(func, expanding_quantile):\n",
    "                            dfc[f\"trg{idx}_{func.__class__.__name__}_shift_{func.shift}_q{func.quantile}\"] = func(dfc[target])\n",
    "                        elif isinstance(func, rolling_quantile):\n",
    "                            dfc[f\"trg{idx}_{func.__class__.__name__}_{func.window_size}_shift_{func.shift}_q{func.quantile}\"] = func(dfc[target])\n",
    "                        else:\n",
    "                            dfc[f\"trg{idx}_{func.__class__.__name__}_{func.window_size}_shift_{func.shift}\"] = func(dfc[target])\n",
    "                            \n",
    "        dfc = dfc.dropna()\n",
    "        return dfc\n",
    "\n",
    "    def fit(self, df_train: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Fit the VAR model to the training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df_train : pd.DataFrame\n",
    "            Training data.\n",
    "        \"\"\"\n",
    "        if self.cat_variables is not None:\n",
    "            self.cat_var = {c: sorted(df_train[c].drop_duplicates().tolist()) for c in self.cat_variables}\n",
    "            self.drop_categ = [self.cat_var[c][0] for c in self.cat_variables]\n",
    "\n",
    "        df = self.data_prep(df_train)\n",
    "        X = df.drop(columns=self.target_cols)\n",
    "        if self.cons:\n",
    "            X = sm.add_constant(X)\n",
    "        X = X.apply(pd.to_numeric, errors='raise')\n",
    "        self.X = np.array(X)\n",
    "        self.y = np.array(df[self.target_cols])\n",
    "        self.coeffs = np.linalg.lstsq(self.X, self.y, rcond=None)[0]\n",
    "\n",
    "    def predict(self, X: List[float]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict the model output for input X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame or List[float] or np.ndarray\n",
    "            Feature DataFrame for prediction.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Model predictions for each target.\n",
    "        \"\"\"\n",
    "        arr = np.array(X)\n",
    "        return np.dot(self.coeffs.T, arr.T)\n",
    "    \n",
    "    @property\n",
    "    def aic(self):\n",
    "        res = self.y - self.predict(self.X).T\n",
    "        K = self.y.shape[1]\n",
    "\n",
    "        p_lag = list(self.n_lag.values()) # list of lags per target\n",
    "        if isinstance(p_lag[0], int): # if lags are given as integers\n",
    "            p = p_lag[0]\n",
    "        else:\n",
    "            p = len(p_lag[0])\n",
    "        q = self.X.shape[1] - p # of exogenous variables\n",
    "        total_params = (K**2) * p + K * q\n",
    "        n = self.y.shape[0]\n",
    "        det_cov = np.log(np.linalg.det(np.cov(res, rowvar=False))) # log determinant of residual covariance matrix\n",
    "        aic = det_cov + (2 * total_params) / n # AIC formula for VAR\n",
    "        return aic\n",
    "    \n",
    "    @property\n",
    "    def aicc(self):\n",
    "        res = self.y - self.predict(self.X).T\n",
    "        K = self.y.shape[1]\n",
    "\n",
    "        p_lag = list(self.n_lag.values()) # list of lags per target\n",
    "        if isinstance(p_lag[0], int): # if lags are given as integers\n",
    "            p = p_lag[0]\n",
    "        else:\n",
    "            p = len(p_lag[0])\n",
    "        q = self.X.shape[1] - p # of exogenous variables\n",
    "        total_params = (K**2) * p + K * q\n",
    "        n = self.y.shape[0]\n",
    "        det_cov = np.log(np.linalg.det(np.cov(res, rowvar=False))) # log determinant of residual covariance matrix\n",
    "        aic = det_cov + (2 * total_params) / n # AIC formula for VAR\n",
    "        aicc = aic + (2 * total_params * (total_params + 1)) / (n - total_params - 1) # AICc formula\n",
    "        return aicc\n",
    "    \n",
    "    @property\n",
    "    def bic(self):\n",
    "        res = self.y - self.predict(self.X).T\n",
    "        K = self.y.shape[1]\n",
    "\n",
    "        p_lag = list(self.n_lag.values()) # list of lags per target\n",
    "        if isinstance(p_lag[0], int): # if lags are given as integers\n",
    "            p = p_lag[0]\n",
    "        else:\n",
    "            p = len(p_lag[0])\n",
    "        q = self.X.shape[1] - p # of exogenous variables\n",
    "        total_params = (K**2) * p + K * q\n",
    "        n = self.y.shape[0]\n",
    "        det_cov = np.log(np.linalg.det(np.cov(res, rowvar=False))) # log determinant of residual covariance matrix\n",
    "        bic = det_cov + total_params * np.log(n) / n # BIC formula for VAR\n",
    "        return bic\n",
    "    \n",
    "    @property\n",
    "    def hqc(self):\n",
    "        res = self.y - self.predict(self.X).T\n",
    "        K = self.y.shape[1]\n",
    "\n",
    "        p_lag = list(self.n_lag.values()) # list of lags per target\n",
    "        if isinstance(p_lag[0], int): # if lags are given as integers\n",
    "            p = p_lag[0]\n",
    "        else:\n",
    "            p = len(p_lag[0])\n",
    "        q = self.X.shape[1] - p # of exogenous variables\n",
    "        total_params = (K**2) * p + K * q\n",
    "        n = self.y.shape[0]\n",
    "        det_cov = np.log(np.linalg.det(np.cov(res, rowvar=False))) # log determinant of residual covariance matrix\n",
    "        hqic = det_cov + 2 * total_params * np.log(np.log(n)) / n # HQIC formula for VAR\n",
    "        return hqic\n",
    "\n",
    "    def copy(self):\n",
    "        return copy.deepcopy(self)\n",
    "\n",
    "    def forecast(self, H: int, exog: Optional[pd.DataFrame] = None) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Forecast H steps ahead.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        H : int\n",
    "            Number of steps to forecast.\n",
    "        exog : Optional[pd.DataFrame]\n",
    "            Exogenous variables for forecasting.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, np.ndarray]\n",
    "            Dictionary of forecasts for each target variable.\n",
    "        \"\"\"\n",
    "        y_lists = {j: self.y[:, i].tolist() for i, j in enumerate(self.target_cols)} # Initialize lists for each target variable\n",
    "        if exog is not None:\n",
    "            if self.cons:\n",
    "                if exog.shape[0] == 1:\n",
    "                    exog.insert(0, 'const', 1)\n",
    "                else:\n",
    "                    exog = sm.add_constant(exog)\n",
    "            exog = np.array(self.data_prep(exog))\n",
    "\n",
    "        forecasts = {i: [] for i in self.target_cols}\n",
    "\n",
    "        # Store original values for later estimate trend components\n",
    "        if self.trend is not None:\n",
    "            # orig_targets = {k: self.orig_targets[k].tolist() for k in self.trend.keys()}\n",
    "            trend_forecasts = {}\n",
    "            for ff in self.trend:\n",
    "                if self.trend.get(ff) == \"linear\":\n",
    "                    if (self.cps is not None) and (ff in self.cps) and (self.cps[ff] is not None):\n",
    "                        trend_forecast= forecast_trend(model = self.trend_models[ff], H=H, start=self.len, degree=self.pol[ff], breakpoints=self.cps[ff])\n",
    "                    else:\n",
    "                        trend_forecast= forecast_trend(model = self.trend_models[ff], H=H, start=self.len, degree=self.pol[ff])  \n",
    "                elif self.trend.get(ff) == \"ets\":\n",
    "                    trend_forecast = np.array(self.trend_models[ff].forecast(H))\n",
    "\n",
    "                trend_forecasts[ff] = trend_forecast\n",
    "\n",
    "        for t in range(H):\n",
    "            # Exogenous input for step t\n",
    "            if exog is not None:\n",
    "                exo_inp = exog[t].tolist()\n",
    "            else:\n",
    "                exo_inp = [1] if self.cons else []\n",
    "\n",
    "            # Lagged features\n",
    "            lags = []\n",
    "            if self.n_lag is not None:\n",
    "                for tr, vals in y_lists.items():\n",
    "                    if tr in self.n_lag:\n",
    "                        lag_used = self.n_lag[tr] if isinstance(self.n_lag[tr], list) else range(1, self.n_lag[tr] + 1)\n",
    "                        ys = [vals[-x] for x in lag_used]\n",
    "                        lags += ys\n",
    "            # Lag transforms\n",
    "            transform_lag = []\n",
    "            if self.lag_transform is not None:\n",
    "                for target, funcs in self.lag_transform.items():\n",
    "                    series_array = np.array(y_lists[target])\n",
    "                    for func in funcs:\n",
    "                        transform_lag.append(func(series_array, is_forecast=True).to_numpy()[-1])\n",
    "\n",
    "\n",
    "            inp = exo_inp + lags + transform_lag\n",
    "            pred = self.predict(inp)\n",
    "            # Add back trend\n",
    "            \n",
    "            for id_, ff in enumerate(forecasts.keys()):\n",
    "                forecasts[ff].append(pred[id_])\n",
    "                y_lists[ff].append(pred[id_])\n",
    "        \n",
    "        # add trend if not none\n",
    "        if self.trend is not None:\n",
    "            for ff in self.trend.keys():\n",
    "                forecasts[ff] += trend_forecasts[ff]\n",
    "\n",
    "        # Invert seasonal difference\n",
    "        if self.season_diffs is not None:\n",
    "            for s in self.orig_ds:\n",
    "                forecasts[s] = invert_seasonal_diff(self.orig_ds[s], np.array(forecasts[s]), self.season_diffs[s])\n",
    "\n",
    "        # Invert difference\n",
    "        if self.diffs is not None:\n",
    "            for d in self.diffs:\n",
    "                forecasts[d] = undiff_ts(self.origs[d], np.array(forecasts[d]), self.diffs[d])\n",
    "\n",
    "\n",
    "        # Non-negativity\n",
    "        for f in forecasts:\n",
    "            forecasts[f] = np.array([max(0, x) for x in forecasts[f]])\n",
    "\n",
    "        # Invert Box-Cox\n",
    "        if self.box_cox is not None:\n",
    "            for k, lmd in self.lamdas.items():\n",
    "                if self.box_cox.get(k, False):\n",
    "                    forecasts[k] = back_box_cox_transform(\n",
    "                        y_pred=forecasts[k], lmda=lmd, shift=self.is_zeros[k], box_cox_biasadj=self.biasadj[k]\n",
    "                    )\n",
    "\n",
    "        return forecasts\n",
    "    \n",
    "\n",
    "class ml_bidirect_forecaster:\n",
    "    \"\"\"\n",
    "    Bidirectional ml Forecaster for time-series forecasting.\n",
    "\n",
    "    Args:\n",
    "         target_cols (list): Names of the target variables.\n",
    "         cat_variables (list, optional): List of categorical variable names.\n",
    "         n_lag (dict, optional): Dictionary specifying the number of lags or list of lags for each target variable. Default is None. Example: {'target1': 3, 'target2': [1, 2, 3]}.\n",
    "         difference (dict, optional): Dictionary specifying the order of ordinary differencing for each target variable. Default is None. Example: {'target1': 1, 'target2': 2}.\n",
    "         seasonal_length (dict, optional): Seasonal differencing period. Example: {'target1': 7, 'target2': 7}.\n",
    "         trend (dict, optional): Trend handling strategy; one of 'linear' or 'ets'. Default is None. Example: {'Target1': 'linear', 'Target2': 'ets'}.\n",
    "         ets_params (dict, optional): Dictionary of ETS model parameters (values are lists of dictionaries of params) and fit settings for each target variable. Example: {'Target1': [{'trend': 'add', 'seasonal': 'add'}, {'damped_trend': True}], 'Target2': [{'trend': 'mul', 'seasonal': 'mul'}, {'damped_trend': False}]}.\n",
    "         target_encode (dict, optional): Flag determining if target encoding is used for categorical features for each target variable. Default is False. Example: {'Target1': True, 'Target2': False}.\n",
    "         box_cox (dict, optional): Whether to apply a Box–Cox transformation for each target variable. Default is False. Example: {'Target1': True, 'Target2': False}.\n",
    "         box_cox_lmda (dict, optional): Lambda parameter for the Box–Cox transformation for each target variable. Example: {'Target1': 0.5, 'Target2': 0.5}.\n",
    "         box_cox_biasadj (dict, optional): Whether to adjust bias when inverting the Box–Cox transform for each target variable. Default is False. Example: {'Target1': True, 'Target2': False}.\n",
    "         lag_transform (dict, optional): Dictionary specifying additional lag transformation functions for each target variable. List of functions to apply for each target variable. Example: {'Target1': [func1, func2], 'Target2': [func3]}.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, target_cols, cat_variables=None, lags=None, difference=None, seasonal_length=None,\n",
    "                 trend=None,ets_params=None, target_encode=False,\n",
    "                 box_cox=None, box_cox_lmda=None, box_cox_biasadj=None, lag_transform=None):\n",
    "        self.model = model\n",
    "        self.target_cols = target_cols\n",
    "        self.cat_variables = cat_variables\n",
    "        self.n_lag = lags\n",
    "        if self.n_lag is not None:\n",
    "            if not isinstance(self.n_lag, dict):\n",
    "                raise TypeError(\"n_lag must be a dictionary of target values\")\n",
    "            for col, lags in self.n_lag.items():\n",
    "                if isinstance(lags, int):\n",
    "                    self.n_lag[col] = list(range(1, lags + 1))\n",
    "                elif isinstance(lags, list):\n",
    "                    self.n_lag[col] = lags\n",
    "                else:\n",
    "                    raise TypeError(\"n_lag values must be int or list of ints\")\n",
    "\n",
    "        if lag_transform is not None:\n",
    "            if not isinstance(lag_transform, dict):\n",
    "                raise TypeError(\"lag_transform must be a dictionary of target values\")\n",
    "            for col in lag_transform.keys():\n",
    "                if not isinstance(lag_transform[col], list):\n",
    "                    raise TypeError(\"lag_transform values must be a list of functions\")\n",
    "            self.lag_transform = lag_transform\n",
    "        else:\n",
    "            self.lag_transform = lag_transform\n",
    "        \n",
    "        # if difference is None, set it to None for all target columns\n",
    "        if difference is None:\n",
    "            self.difference = {col: None for col in target_cols}\n",
    "        else:\n",
    "            if not isinstance(difference, dict):\n",
    "                raise TypeError(\"difference must be a dictionary of target values\")\n",
    "            self.difference = difference\n",
    "            for col in target_cols:\n",
    "                if col not in self.difference:\n",
    "                    self.difference[col] = None\n",
    "        # if seasonal_length is None, set it to None for all target columns\n",
    "        # if seasonal_length is a dictionary, it must contain all target columns\n",
    "        if seasonal_length is None:\n",
    "            self.season_diff = {col: None for col in target_cols}\n",
    "        else:\n",
    "            if not isinstance(seasonal_length, dict):\n",
    "                raise TypeError(\"seasonal_length must be a dictionary of target values\")\n",
    "            self.season_diff = seasonal_length\n",
    "            # if any target column is not in the seasonal_length, set it to None\n",
    "            for col in target_cols:\n",
    "                if col not in self.season_diff:\n",
    "                    self.season_diff[col] = None\n",
    "        # if trend is None, set it to False for all target columns\n",
    "        # if trend is a dictionary, it must contain all target columns\n",
    "        self.trend = trend\n",
    "        if trend is not None:\n",
    "            if not isinstance(trend, dict):\n",
    "                raise TypeError(\"trend must be a dictionary of target values\")\n",
    "\n",
    "        # if ets_params is not None:\n",
    "        #     self.ets_model1 = ets_params[target_cols[0]][0]\n",
    "        #     self.ets_fit1 = ets_params[target_cols[0]][1]\n",
    "        #     self.ets_model2 = ets_params[target_cols[1]][0]\n",
    "        #     self.ets_fit2 = ets_params[target_cols[1]][1]\n",
    "        self.ets_params = ets_params\n",
    "        self.target_encode = target_encode\n",
    "        if box_cox is None:\n",
    "            self.box_cox = {col: False for col in target_cols}\n",
    "        else:\n",
    "            if not isinstance(box_cox, dict):\n",
    "                raise TypeError(\"box_cox must be a dictionary of target values\")\n",
    "            self.box_cox = box_cox\n",
    "\n",
    "        if box_cox_lmda is None:\n",
    "            self.lamda = {col: None for col in target_cols}\n",
    "        else:\n",
    "            if not isinstance(box_cox_lmda, dict):\n",
    "                raise TypeError(\"box_cox_lmda must be a dictionary of target values\")\n",
    "            self.lamda = box_cox_lmda\n",
    "            # if any target column is not in the box_cox_lmda, set it to None\n",
    "            for col in target_cols:\n",
    "                if col not in self.lamda:\n",
    "                    self.lamda[col] = None\n",
    "        \n",
    "        if box_cox_biasadj is None:\n",
    "            self.biasadj = {col: False for col in target_cols}\n",
    "        else:\n",
    "            if not isinstance(box_cox_biasadj, dict):\n",
    "                raise TypeError(\"box_cox_biasadj must be a dictionary of target values\")\n",
    "            self.biasadj = box_cox_biasadj\n",
    "            # if any target column is not in the box_cox_biasadj, set it to False\n",
    "            for col in target_cols:\n",
    "                if col not in self.biasadj:\n",
    "                    self.biasadj[col] = False\n",
    "        self.tuned_params = None\n",
    "        self.actuals = None\n",
    "        self.prob_forecasts = None\n",
    "\n",
    "\n",
    "    def data_prep(self, df):\n",
    "        \"\"\"\n",
    "        Prepare the data and handle categorical encoding, lag generation, trend removal, and differencing.\n",
    "        \"\"\"\n",
    "        dfc = df.copy()\n",
    "        if isinstance(self.model, (LGBMRegressor, CatBoostRegressor)):\n",
    "                        # Process categorical variables if provided\n",
    "            if self.cat_variables is not None:\n",
    "                for col in self.cat_variables:\n",
    "                    dfc[col] = dfc[col].astype('category')\n",
    "        else:\n",
    "            # Handle categorical variables\n",
    "            if self.cat_variables is not None:\n",
    "                for col, cats in self.cat_var.items():\n",
    "                    dfc[col] = pd.Categorical(dfc[col], categories=cats)\n",
    "                dfc = pd.get_dummies(dfc, dtype=float)\n",
    "                if isinstance(self.model, (LinearRegression, Ridge, Lasso, ElasticNet)):\n",
    "                    for i in self.drop_categ:\n",
    "                        dfc.drop(list(dfc.filter(regex=i)), axis=1, inplace=True)\n",
    "\n",
    "        if all(col in dfc.columns for col in self.target_cols):\n",
    "        # Box-Cox transformation if flag is set\n",
    "            if self.box_cox[self.target_cols[0]]:\n",
    "                self.is_zero1 = np.any(np.array(dfc[self.target_cols[0]]) < 1) # Check if any values are less than 1 for Box-Cox\n",
    "                trans_data1, self.lamda1 = box_cox_transform(x=dfc[self.target_cols[0]],\n",
    "                                                            shift=self.is_zero1,\n",
    "                                                            box_cox_lmda=self.lamda[self.target_cols[0]])\n",
    "                dfc[self.target_cols[0]] = trans_data1\n",
    "            if self.box_cox[self.target_cols[1]]:\n",
    "                self.is_zero2 = np.any(np.array(dfc[self.target_cols[1]]) < 1)\n",
    "                trans_data2, self.lamda2 = box_cox_transform(x=dfc[self.target_cols[1]],\n",
    "                                                            shift=self.is_zero2,\n",
    "                                                            box_cox_lmda=self.lamda[self.target_cols[1]])\n",
    "                dfc[self.target_cols[1]] = trans_data2\n",
    "\n",
    "            # Handle trend removal if specified\n",
    "            # if atleast one target column has a trend, we need to apply the trend removal\n",
    "            if self.trend is not None:\n",
    "                self.len = len(df) # Store the length of the dataframe for later use\n",
    "                if self.trend.get(self.target_cols[0]) in [\"linear\", \"feature_lr\"]:\n",
    "                    self.orig_target1 = dfc[self.target_cols[0]] # Store original values for later use during forecasting\n",
    "                    self.lr_model1 = LinearRegression().fit(np.arange(self.len).reshape(-1, 1), self.orig_target1)\n",
    "                    if self.trend.get(self.target_cols[0]) == \"linear\":\n",
    "                        dfc[self.target_cols[0]] = dfc[self.target_cols[0]] - self.lr_model1.predict(np.arange(self.len).reshape(-1, 1))\n",
    "\n",
    "                if self.trend.get(self.target_cols[0]) in [\"ets\", \"feature_ets\"]:\n",
    "                    self.orig_target1 = dfc[self.target_cols[0]] # Store original values for later use during forecasting\n",
    "                    self.ses_model1 = ExponentialSmoothing(self.orig_target1, **self.ets_params[self.target_cols[0]][0]).fit(**self.ets_params[self.target_cols[0]][1])\n",
    "                    if self.trend.get(self.target_cols[0]) == \"ets\":\n",
    "                        dfc[self.target_cols[0]] = dfc[self.target_cols[0]] - self.ses_model1.fittedvalues.values\n",
    "\n",
    "                # If the second target column has a trend, apply the same logic\n",
    "                if self.trend.get(self.target_cols[1]) in [\"linear\", \"feature_lr\"]:\n",
    "                    self.orig_target2 = df[self.target_cols[1]] # Store original values for later use during forecasting\n",
    "                    self.lr_model2 = LinearRegression().fit(np.arange(self.len).reshape(-1, 1), self.orig_target2)\n",
    "                    if self.trend.get(self.target_cols[1]) == \"linear\":\n",
    "                        dfc[self.target_cols[1]] = dfc[self.target_cols[1]] - self.lr_model2.predict(np.arange(self.len).reshape(-1, 1))\n",
    "\n",
    "                if self.trend.get(self.target_cols[1]) in [\"ets\", \"feature_ets\"]:\n",
    "                    self.orig_target2 = df[self.target_cols[1]] # Store original values for later use during forecasting\n",
    "                    self.ses_model2 = ExponentialSmoothing(self.orig_target2, **self.ets_params[self.target_cols[1]][0]).fit(**self.ets_params[self.target_cols[1]][1])\n",
    "                    if self.trend.get(self.target_cols[1]) == \"ets\":\n",
    "                        dfc[self.target_cols[1]] = dfc[self.target_cols[1]] - self.ses_model2.fittedvalues.values\n",
    "\n",
    "            # Handle differencing if specified\n",
    "            if self.difference[self.target_cols[0]] is not None:\n",
    "                    self.orig1 = df[self.target_cols[0]].tolist()\n",
    "                    dfc[self.target_cols[0]] = np.diff(dfc[self.target_cols[0]], n=self.difference[self.target_cols[0]],\n",
    "                                                    prepend=np.repeat(np.nan, self.difference[self.target_cols[0]]))\n",
    "            if self.difference[self.target_cols[1]] is not None:\n",
    "                self.orig2 = df[self.target_cols[1]].tolist()\n",
    "                dfc[self.target_cols[1]] = np.diff(dfc[self.target_cols[1]], n=self.difference[self.target_cols[1]],\n",
    "                                                prepend=np.repeat(np.nan, self.difference[self.target_cols[1]]))\n",
    "            if self.season_diff[self.target_cols[0]] is not None:\n",
    "                self.orig_d1 = dfc[self.target_cols[0]].tolist()\n",
    "                dfc[self.target_cols[0]] = seasonal_diff(dfc[self.target_cols[0]], self.season_diff[self.target_cols[0]])\n",
    "            if self.season_diff[self.target_cols[1]] is not None:\n",
    "                self.orig_d2 = dfc[self.target_cols[1]].tolist()\n",
    "                dfc[self.target_cols[1]] = seasonal_diff(dfc[self.target_cols[1]], self.season_diff[self.target_cols[1]])\n",
    "\n",
    "            # Create lag features based on n_lag parameter\n",
    "            if self.n_lag is not None:\n",
    "                for target, lags in self.n_lag.items():\n",
    "                    for lag in lags:\n",
    "                        dfc[f\"{target}_lag_{lag}\"] = dfc[target].shift(lag)\n",
    "\n",
    "            # Create additional lag transformations if specified (check this later)\n",
    "            if self.lag_transform is not None:\n",
    "                for idx, (target, funcs) in enumerate(self.lag_transform.items()):\n",
    "                    for func in funcs:\n",
    "                        if isinstance(func, (expanding_std, expanding_mean)):\n",
    "                            dfc[f\"trg{idx}_{func.__class__.__name__}_shift_{func.shift}\"] = func(dfc[target])\n",
    "                        elif isinstance(func, expanding_quantile):\n",
    "                            dfc[f\"trg{idx}_{func.__class__.__name__}_shift_{func.shift}_q{func.quantile}\"] = func(dfc[target])\n",
    "                        elif isinstance(func, rolling_quantile):\n",
    "                            dfc[f\"trg{idx}_{func.__class__.__name__}_{func.window_size}_shift_{func.shift}_q{func.quantile}\"] = func(dfc[target])\n",
    "                        else:\n",
    "                            dfc[f\"trg{idx}_{func.__class__.__name__}_{func.window_size}_shift_{func.shift}\"] = func(dfc[target])\n",
    "            # Add trend features if specified\n",
    "            if self.trend is not None:\n",
    "                # if self.target_cols[0] in dfc.columns:\n",
    "                if self.trend.get(self.target_cols[0]) == \"feature_lr\":\n",
    "                    dfc[\"trend1\"] = self.lr_model1.predict(np.arange(self.len).reshape(-1, 1))\n",
    "                if self.trend.get(self.target_cols[0]) == \"feature_ses\":\n",
    "                    dfc[\"trend1\"] = self.ses_model1.fittedvalues.values\n",
    "                # if self.target_cols[1] in dfc.columns:\n",
    "                if self.trend.get(self.target_cols[1]) == \"feature_lr\":\n",
    "                    dfc[\"trend2\"] = self.lr_model2.predict(np.arange(self.len).reshape(-1, 1))\n",
    "                if self.trend.get(self.target_cols[1]) == \"feature_ses\":\n",
    "                    dfc[\"trend2\"] = self.ses_model2.fittedvalues.values\n",
    "\n",
    "        return dfc.dropna()\n",
    "            \n",
    "    def fit(self, df):\n",
    "        # Fit the model to the dataframe\n",
    "        model1_ = clone(self.model)\n",
    "        model2_ = clone(self.model)\n",
    "        if isinstance(self.model, (XGBRegressor, RandomForestRegressor, Cubist, HistGradientBoostingRegressor, AdaBoostRegressor, LinearRegression, Ridge, Lasso, ElasticNet)):\n",
    "            if self.cat_variables is not None:\n",
    "                self.cat_var = {c: sorted(df[c].drop_duplicates().tolist(), key=lambda x: x[0]) for c in self.cat_variables}\n",
    "                if isinstance(self.model, (LinearRegression, Ridge, Lasso, ElasticNet)):\n",
    "                    self.drop_categ= [sorted(df[i].drop_duplicates().tolist(), key=lambda x: x[0])[0] for i in self.cat_variables]\n",
    "        model_train = self.data_prep(df)\n",
    "        self.X = model_train.drop(columns=self.target_cols)\n",
    "        self.y1 = model_train[self.target_cols[0]]\n",
    "        self.y2 = model_train[self.target_cols[1]]\n",
    "        if isinstance(self.model, CatBoostRegressor):\n",
    "            self.model1_fit = model1_.fit(self.X, self.y1, cat_features=self.cat_variables, verbose = True)\n",
    "            self.model2_fit = model2_.fit(self.X, self.y2, cat_features=self.cat_variables, verbose = True)\n",
    "        elif isinstance(self.model, LGBMRegressor):\n",
    "            self.model1_fit = model1_.fit(self.X, self.y1, categorical_feature=self.cat_variables)\n",
    "            self.model2_fit = model2_.fit(self.X, self.y2, categorical_feature=self.cat_variables)\n",
    "        else:\n",
    "            self.model1_fit = model1_.fit(self.X, self.y1)\n",
    "            self.model2_fit = model2_.fit(self.X, self.y2)\n",
    "\n",
    "    def copy(self):\n",
    "        return copy.deepcopy(self)\n",
    "\n",
    "    def forecast(self, H, exog=None):\n",
    "        \"\"\"\n",
    "        Forecast future values for H periods.\n",
    "\n",
    "        Args:\n",
    "            H (int): Number of periods to forecast.\n",
    "            exog (pd.DataFrame, optional): Exogenous variables.\n",
    "\n",
    "        Returns:\n",
    "            np.array: Forecasted values.\n",
    "        \"\"\"\n",
    "        if isinstance(self.model, (XGBRegressor, RandomForestRegressor, Cubist, HistGradientBoostingRegressor, AdaBoostRegressor, LinearRegression, Ridge, Lasso, ElasticNet)):\n",
    "            if exog is not None:\n",
    "                exog = self.data_prep(exog)\n",
    "\n",
    "        target1_lags = self.y1.tolist()\n",
    "        target2_lags = self.y2.tolist()\n",
    "        tar1_forecasts = []\n",
    "        tar2_forecasts = []\n",
    "\n",
    "\n",
    "        if self.trend is not None:\n",
    "\n",
    "            if self.trend.get(self.target_cols[0]) in [\"ets\", \"feature_ets\"]:\n",
    "                trend_forecast1 = np.array(self.ses_model1.forecast(H)) # Forecasting H step\n",
    "            elif self.trend.get(self.target_cols[0]) in [\"linear\", \"feature_lr\"]:\n",
    "                future_time = np.arange(self.len, self.len + H).reshape(-1, 1)\n",
    "                trend_forecast1 = self.lr_model1.predict(future_time) # Predicting the next value trend\n",
    "\n",
    "            ## Second target variable\n",
    "\n",
    "            if self.trend.get(self.target_cols[1]) in [\"ets\", \"feature_ets\"]:\n",
    "                trend_forecast2 = np.array(self.ses_model2.forecast(H)) # Forecasting H step\n",
    "            elif self.trend.get(self.target_cols[1]) in [\"linear\", \"feature_lr\"]:\n",
    "                future_time = np.arange(self.len, self.len + H).reshape(-1, 1)\n",
    "                trend_forecast2 = self.lr_model2.predict(future_time) # Predicting the next value trend\n",
    "\n",
    "        # Forecast recursively one step at a time\n",
    "        for i in range(H):\n",
    "            if exog is not None:\n",
    "                x_var = exog.iloc[i, :].tolist()\n",
    "            else:\n",
    "                x_var = []\n",
    "                \n",
    "            inp_lag = []\n",
    "            if self.n_lag is not None:\n",
    "                # For the first target variable\n",
    "                for col, lags in self.n_lag.items():\n",
    "                    if col == self.target_cols[0]:\n",
    "                        inp_lag.extend([target1_lags[-lag] for lag in lags])\n",
    "                    else:\n",
    "                        inp_lag.extend([target2_lags[-lag] for lag in lags])\n",
    "\n",
    "            transform_lag = []\n",
    "            if self.lag_transform is not None:\n",
    "                for target, funcs in self.lag_transform.items():\n",
    "                    series_array = np.array(target1_lags if target == self.target_cols[0] else target2_lags)\n",
    "                    for func in funcs:\n",
    "                        transform_lag.append(func(series_array, is_forecast=True).to_numpy()[-1])\n",
    "\n",
    "            # Trend feature\n",
    "            trend_var = []\n",
    "\n",
    "            if self.trend is not None:\n",
    "                # First target variable\n",
    "                if self.trend.get(self.target_cols[0]) in [\"feature_lr\", \"feature_ets\"]:\n",
    "                    trend_var.append(trend_forecast1[i])\n",
    "\n",
    "                ## Second target variable\n",
    "                \n",
    "                if self.trend.get(self.target_cols[1]) in [\"feature_lr\", \"feature_ets\"]:\n",
    "                    trend_var.append(trend_forecast2[i])\n",
    "\n",
    "            inp = x_var + inp_lag + transform_lag + trend_var\n",
    "            # print(f\"len of x_var: {len(x_var)}, len of inp_lag1: {len(inp_lag1)}, len of inp_lag2: {len(inp_lag2)}, len of transform_lag: {len(transform_lag)}, len of inp: {len(inp)}\")\n",
    "\n",
    "            df_inp = pd.DataFrame(inp).T\n",
    "            df_inp.columns = self.X.columns\n",
    "            if isinstance(self.model, (LGBMRegressor, CatBoostRegressor)):\n",
    "                df_inp = df_inp.astype({col: 'category' if col in self.cat_variables else 'float64' for col in df_inp.columns})\n",
    "            pred1 = self.model1_fit.predict(df_inp)[0]\n",
    "            target1_lags.append(pred1)\n",
    "            pred2 = self.model2_fit.predict(df_inp)[0]\n",
    "            target2_lags.append(pred2)\n",
    "\n",
    "            tar1_forecasts.append(pred1)\n",
    "            tar2_forecasts.append(pred2)\n",
    "\n",
    "        forecasts1 = np.array(tar1_forecasts)\n",
    "        forecasts2 = np.array(tar2_forecasts)\n",
    "        # If trend is applied, add the trend forecast to the prediction\n",
    "        if self.trend is not None:\n",
    "            # Revert trend if applied\n",
    "            if self.trend.get(self.target_cols[0]) in [\"ets\", \"linear\"]:\n",
    "                forecasts1 += trend_forecast1\n",
    "            if self.trend.get(self.target_cols[1]) in [\"ets\", \"linear\"]:\n",
    "                forecasts2 += trend_forecast2\n",
    "        # Revert seasonal differencing if applied\n",
    "        if self.season_diff[self.target_cols[0]] is not None:\n",
    "            forecasts1 = invert_seasonal_diff(self.orig_d1, forecasts1, self.season_diff[self.target_cols[0]])\n",
    "        if self.season_diff[self.target_cols[1]] is not None:\n",
    "            forecasts2 = invert_seasonal_diff(self.orig_d2, forecasts2, self.season_diff[self.target_cols[1]])\n",
    "            \n",
    "        if self.difference[self.target_cols[0]] is not None:\n",
    "            forecasts1 = undiff_ts(self.orig1, forecasts1, self.difference[self.target_cols[0]])\n",
    "        if self.difference[self.target_cols[1]] is not None:\n",
    "            forecasts2 = undiff_ts(self.orig2, forecasts2, self.difference[self.target_cols[1]])\n",
    "\n",
    "        forecasts1 = np.array([max(0, x) for x in forecasts1])\n",
    "        forecasts2 = np.array([max(0, x) for x in forecasts2])\n",
    "        if self.box_cox[self.target_cols[0]]:\n",
    "            forecasts1 = back_box_cox_transform(y_pred=forecasts1,\n",
    "                                                lmda=self.lamda1,\n",
    "                                                shift=self.is_zero1,\n",
    "                                                box_cox_biasadj=self.biasadj[self.target_cols[0]])\n",
    "        if self.box_cox[self.target_cols[1]]:\n",
    "            forecasts2 = back_box_cox_transform(y_pred=forecasts2,\n",
    "                                                lmda=self.lamda2,\n",
    "                                                shift=self.is_zero2,\n",
    "                                                box_cox_biasadj=self.biasadj[self.target_cols[1]])\n",
    "        forecasts = {\n",
    "            self.target_cols[0]: forecasts1,\n",
    "            self.target_cols[1]: forecasts2\n",
    "        }\n",
    "        return forecasts\n",
    "\n",
    "# Hidden Markov Model with Regression\n",
    "    # def __init__(self, n_components, target_col, lags, method=\"posterior\",\n",
    "    #             startprob_prior=1e3, transmat_prior=1e5, add_constant=True,\n",
    "    #             difference=None, trend=None, ets_params = None, change_points=None,\n",
    "    #             cat_variables=None, lag_transform=None, n_iter=100, tol=1e-6,\n",
    "    #             coefficients=None, stds=None, init_state=None, trans_matrix=None,\n",
    "    #             box_cox=False, lamda=None, box_cox_biasadj=False, season_diff=None,\n",
    "    #             ridge=1e-5, var_floor=1e-5, \n",
    "    #             random_state=None, verbose=False):\n",
    "    \n",
    "class MsHmmRegression:\n",
    "    \"\"\"\n",
    "    Hidden Markov Model Regression for time series with EM parameter estimation.\n",
    "\n",
    "    Args:\n",
    "        n_components (int): Number of hidden states.\n",
    "        target_col (str): Name of the target variable.\n",
    "        lag_list (list): List of integer lags to include as features.\n",
    "        method (str): 'posterior' for soft state assignment, 'viterbi' for hard paths.\n",
    "        startprob_prior (float): Prior for initial state probabilities.\n",
    "        transmat_prior (float): Prior for transition matrix.\n",
    "        ets_params (tuple, optional): A tuple (model_params, fit_params) for exponential smoothing. Ex.g. ({'trend': 'add', 'seasonal': 'add'}, {'damped_trend': True}). If trend is \"ets\", this will be used.\n",
    "        change_points (list or None): List of change points for piecewise linear regression to handle trend\n",
    "        add_constant (bool): Whether to add constant to regressors.\n",
    "        difference (int or None): Order of differencing to apply to target.\n",
    "        trend (str or None): Type of trend to remove ('linear', 'ets', etc.). Default is None.\n",
    "        pol_degree (int): Degree of polynomial for trend if trend is 'linear'. Integer or list of integers. Default is 1.\n",
    "        cat_variables (list or None): List of categorical columns.\n",
    "        n_iter (int): Maximum number of EM iterations.\n",
    "        tol (float): Convergence tolerance for EM.\n",
    "        coefficients (np.ndarray or None): Initial regression coefficients.\n",
    "        stds (np.ndarray or None): Initial state std deviations.\n",
    "        init_state (np.ndarray or None): Initial state distribution.\n",
    "        trans_matrix (np.ndarray or None): Initial transition matrix.\n",
    "        random_state (int or None): Random seed.\n",
    "        switching_var (bool): If True, allows if variance to switch between states.\n",
    "        ridge (float): Ridge regularization parameter for regression.\n",
    "        verbose (bool): Print progress if True.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_components, target_col, lags, method=\"posterior\",\n",
    "                 startprob_prior=1e3, transmat_prior=1e5, add_constant=True,\n",
    "                 difference=None, trend=None, pol_degree=1, ets_params = None, change_points=None,\n",
    "                 cat_variables=None, lag_transform=None, n_iter=100, tol=1e-6,\n",
    "                 coefficients=None, stds=None, init_state=None, trans_matrix=None,\n",
    "                 box_cox=False, lamda=None, box_cox_biasadj=False, season_diff=None, \n",
    "                 random_state=None, switching_var=True, ridge=1e-5, verbose=False):\n",
    "        self.N = n_components\n",
    "        self.target_col = target_col\n",
    "        self.diff = difference\n",
    "        self.cons = add_constant\n",
    "        self.cat_variables = cat_variables\n",
    "        # lags must be a list of integers or an integer\n",
    "        if not isinstance(lags, (int, list)):\n",
    "            raise ValueError(\"Lags must be an integer or a list of integers.\")\n",
    "\n",
    "        self.lags = [i for i in range(1, lags + 1)] if isinstance(lags, int) else lags\n",
    "        self.method = method\n",
    "        self.box_cox = box_cox\n",
    "        self.lamda = lamda\n",
    "        self.biasadj = box_cox_biasadj\n",
    "        self.trend = trend\n",
    "        self.pol = pol_degree\n",
    "        if ets_params is not None:\n",
    "            self.ets_model = ets_params[0]\n",
    "            self.ets_fit = ets_params[1]\n",
    "        else:\n",
    "            self.ets_model = None\n",
    "            self.ets_fit = None\n",
    "        \n",
    "        self.cps = change_points\n",
    "        self.season_diff = season_diff\n",
    "        self.lag_transform = lag_transform\n",
    "        self.iter = n_iter\n",
    "        self.tol = tol\n",
    "        self.switching_var = switching_var\n",
    "        self.ridge = ridge\n",
    "        self.verb = verbose\n",
    "\n",
    "\n",
    "        # RNG for reproducibility\n",
    "        self.rng = np.random.default_rng(random_state)\n",
    "        if init_state is None:\n",
    "            self.sp = startprob_prior\n",
    "            self.alpha_p = np.repeat(self.sp, self.N)\n",
    "            self.pi = self.rng.dirichlet(self.alpha_p) # Initial state probabilities using Dirichlet distribution\n",
    "        else:\n",
    "            self.pi = np.array(init_state)\n",
    "        if trans_matrix is None:\n",
    "            self.tm = transmat_prior\n",
    "            self.alpha_t = np.repeat(self.tm, self.N) # \n",
    "            self.A = self.rng.dirichlet(self.alpha_t, size=self.N)\n",
    "        else:\n",
    "            self.A = np.array(trans_matrix)\n",
    "\n",
    "        self.coeffs = coefficients\n",
    "        self.stds = stds\n",
    "\n",
    "\n",
    "    def data_prep(self, df):\n",
    "        \"\"\"\n",
    "        Prepare the data: encode categoricals, add lags, trend, differencing.\n",
    "        \"\"\"\n",
    "        dfc = df.copy()\n",
    "        # Categorical variable encoding\n",
    "        if self.cat_variables is not None:\n",
    "            # if self.target_encode ==True:\n",
    "            #     for col in self.cat_variables:\n",
    "            #         encode_col = col+\"_target_encoded\"\n",
    "            #         dfc[encode_col] = kfold_target_encoder(dfc, col, self.target_col, 36)\n",
    "            #     self.df_encode = dfc.copy()\n",
    "            #     dfc = dfc.drop(columns = self.cat_variables)\n",
    "            #     # If target encoding is not used, convert categories to dummies    \n",
    "\n",
    "            # else:\n",
    "            for col, cat in self.cat_var.items():\n",
    "                dfc[col] = dfc[col].astype('category')\n",
    "                # Set categories for categorical columns\n",
    "                dfc[col] = dfc[col].cat.set_categories(cat)\n",
    "            dfc = pd.get_dummies(dfc, dtype=np.float64)\n",
    "\n",
    "            for i in self.drop_categ:\n",
    "                dfc.drop(list(dfc.filter(regex=i)), axis=1, inplace=True)\n",
    "        \n",
    "        if self.target_col in dfc.columns:\n",
    "            # Apply Box–Cox transformation if specified\n",
    "            if self.box_cox:\n",
    "                self.is_zero = np.any(np.array(dfc[self.target_col]) < 1) # check for zero or negative values\n",
    "                trans_data, self.lamda = box_cox_transform(x=dfc[self.target_col],\n",
    "                                                        shift=self.is_zero,\n",
    "                                                        box_cox_lmda=self.lamda)\n",
    "                dfc[self.target_col] = trans_data\n",
    "\n",
    "            if self.trend is not None:\n",
    "                self.len = len(dfc)\n",
    "                self.target_orig = dfc[self.target_col] # Store original values for later use during forecasting\n",
    "                if self.trend == \"linear\":\n",
    "                    # self.lr_model = LinearRegression().fit(np.arange(self.len).reshape(-1, 1), self.target_orig)\n",
    "                    # dfc[self.target_col] = dfc[self.target_col] - self.lr_model.predict(np.arange(self.len).reshape(-1, 1))\n",
    "                    if self.cps is not None:\n",
    "                        trend, self.lr_model = lr_trend_model(self.target_orig, degree=self.pol, breakpoints=self.cps, type='piecewise')\n",
    "                    else:\n",
    "                        trend, self.lr_model = lr_trend_model(self.target_orig, degree=self.pol)\n",
    "                    dfc[self.target_col] = dfc[self.target_col] - trend\n",
    "                if self.trend == \"ets\":\n",
    "                    self.ets_model_fit = ExponentialSmoothing(self.target_orig, **self.ets_model).fit(**self.ets_fit)\n",
    "                    dfc[self.target_col] = dfc[self.target_col] - self.ets_model_fit.fittedvalues.values\n",
    "\n",
    "\n",
    "            # Apply differencing if specified\n",
    "            if self.diff is not None or self.season_diff is not None:\n",
    "                self.orig = dfc[self.target_col].tolist()\n",
    "                if self.diff is not None:\n",
    "                    dfc[self.target_col] = np.diff(dfc[self.target_col], n=self.diff,\n",
    "                                                prepend=np.repeat(np.nan, self.diff))\n",
    "                if self.season_diff is not None:\n",
    "                    self.orig_d = dfc[self.target_col].tolist()\n",
    "                    dfc[self.target_col] = seasonal_diff(dfc[self.target_col], self.season_diff)\n",
    "\n",
    "            # Create lag features based on lags parameter\n",
    "            if self.lags is not None:\n",
    "                for lag in self.lags:\n",
    "                    dfc[f\"{self.target_col}_lag_{lag}\"] = dfc[self.target_col].shift(lag)\n",
    "            # Create additional lag transformations if specified\n",
    "            if self.lag_transform is not None:\n",
    "                for func in self.lag_transform:\n",
    "                    if isinstance(func, (expanding_std, expanding_mean)):\n",
    "                        dfc[f\"{func.__class__.__name__}_shift_{func.shift}\"] = func(dfc[self.target_col])\n",
    "                    elif isinstance(func, expanding_quantile):\n",
    "                        dfc[f\"{func.__class__.__name__}_shift_{func.shift}_q{func.quantile}\"] = func(dfc[self.target_col])\n",
    "                    elif isinstance(func, rolling_quantile):\n",
    "                        dfc[f\"{func.__class__.__name__}_{func.window_size}_shift_{func.shift}_q{func.quantile}\"] = func(dfc[self.target_col])\n",
    "                    else:\n",
    "                        dfc[f\"{func.__class__.__name__}_{func.window_size}_shift_{func.shift}\"] = func(dfc[self.target_col])\n",
    "                        \n",
    "            self.df = dfc.dropna()\n",
    "            self.X = self.df.drop(columns=self.target_col)\n",
    "            self.y = self.df[self.target_col]\n",
    "            if self.cons:\n",
    "                self.X = sm.add_constant(self.X)\n",
    "            self.col_names = self.X.columns.tolist() if hasattr(self.X, 'columns') else [f\"x{i}\" for i in range(self.X.shape[1])]\n",
    "            self.X = np.array(self.X)\n",
    "            self.y = np.array(self.y)\n",
    "            self.T = len(self.y)\n",
    "            if self.coeffs is None or self.stds is None:\n",
    "                # Initial fit: use unweighted least squares for all states\n",
    "                coeffs = []\n",
    "                stds = []\n",
    "                for i in range(self.N):\n",
    "                    # Least squares fit\n",
    "                    coeff_i = np.linalg.lstsq(self.X, self.y, rcond=None)[0]\n",
    "                    coeffs.append(coeff_i)\n",
    "                    y_pred = self.X @ coeff_i\n",
    "                    resid = self.y - y_pred\n",
    "                    var_i = np.mean(resid ** 2)\n",
    "                    stds.append(np.sqrt(var_i))\n",
    "                self.coeffs = np.row_stack(coeffs)\n",
    "                self.stds = np.array(stds)\n",
    "\n",
    "        else:\n",
    "            return dfc.dropna()\n",
    "\n",
    "\n",
    "    def compute_coeffs(self, var_floor=1e-5, w_floor=1e-5):\n",
    "\n",
    "        # Update regression coefficients and stds for each state\n",
    "\n",
    "        # If posterior probabilities are shorter than the number of observations, make self.X and self.y same length visa vis make posterier same as self.X and self.y length\n",
    "        if self.posterior.shape[1] < self.X.shape[0]:\n",
    "            # Truncate self.X and self.y to match the length of self.posterior[i]\n",
    "            self.X = self.X[:self.posterior.shape[1]]\n",
    "            self.y = self.y[:self.posterior.shape[1]]\n",
    "        if self.posterior.shape[1] > self.X.shape[0]:\n",
    "            # Truncate self.posterior to match the length of self.X and self.y\n",
    "            self.posterior = self.posterior[:, -self.X.shape[0]:]\n",
    "\n",
    "        coeffs = []\n",
    "        stds = []\n",
    "        weighted_resid_all = []\n",
    "        weights_all = []\n",
    "        X = self.X\n",
    "        for s in range(self.N):\n",
    "            # Add floor so state isn’t “killed”\n",
    "            w = self.posterior[s] + w_floor\n",
    "            w /= w.sum()\n",
    "            sw = np.sqrt(w)\n",
    "            Xw = X * sw[:, None]\n",
    "            yw = self.y * sw\n",
    "            XtX = Xw.T @ Xw + self.ridge*np.eye(X.shape[1])\n",
    "            Xty = Xw.T @ yw\n",
    "            beta_s = np.linalg.lstsq(XtX, Xty, rcond=None)[0]\n",
    "            coeffs.append(beta_s)\n",
    "            resid = self.y - X @ beta_s\n",
    "            # var_s = (w * resid**2).sum() / max((w.sum()-beta_s.shape[0]), 1.0)\n",
    "            # stds.append(np.sqrt(max(var_s, var_floor)))\n",
    "            weighted_resid_all.append(w * resid**2)\n",
    "            weights_all.append(w)\n",
    "\n",
    "            if self.switching_var:\n",
    "                var_s = (w * resid**2).sum() / max((w.sum()-beta_s.shape[0]), 1.0)\n",
    "                stds.append(np.sqrt(max(var_s, var_floor)))\n",
    "\n",
    "        if not self.switching_var:\n",
    "            # pooled variance across all states\n",
    "            pooled_var = (np.sum(weighted_resid_all)) / (np.sum(weights_all) - X.shape[1])\n",
    "            stds = [np.sqrt(max(pooled_var, var_floor))] * self.N\n",
    "\n",
    "        self.coeffs = np.row_stack(coeffs)\n",
    "        self.stds = np.array(stds)\n",
    "\n",
    "\n",
    "# Hidden Markov Model with Vector Autoregressive (VAR)\n",
    "\n",
    "    def _log_emissions(self):\n",
    "        # logB[s,t] = log p(y_t | state s)\n",
    "        N, T = self.N, self.T\n",
    "        logB = np.empty((N, T))\n",
    "        self.fitted = np.empty((N, T))\n",
    "        # self.compute_coeffs()\n",
    "        for s in range(N):\n",
    "            mu = self.X @ self.coeffs[s]           # (T,)\n",
    "            self.fitted[s, :] = mu\n",
    "            logB[s, :] = norm.logpdf(self.y, loc=mu, scale=self.stds[s])\n",
    "        return logB\n",
    "\n",
    "    def _e_step_log(self):\n",
    "        N, T = self.N, self.T\n",
    "        logA  = np.log(self.A + 1e-300)\n",
    "        logpi = np.log(self.pi + 1e-300)\n",
    "        logB  = self._log_emissions()\n",
    "\n",
    "        # Forward\n",
    "        log_alpha = np.empty((N, T))\n",
    "        log_alpha[:, 0] = logpi + logB[:, 0]\n",
    "        for t in range(1, T):\n",
    "            # log_alpha[:,t] = logB[:,t] + logsumexp_i( log_alpha[i,t-1] + logA[i,:] )\n",
    "            log_alpha[:, t] = logB[:, t] + logsumexp(log_alpha[:, t-1][:, None] + logA, axis=0)\n",
    "\n",
    "        # Log-likelihood\n",
    "        loglik = logsumexp(log_alpha[:, -1])\n",
    "\n",
    "        # Backward\n",
    "        log_beta = np.full((N, T), 0.0)\n",
    "        for t in range(T-2, -1, -1):\n",
    "            # log_beta[:,t] = logsumexp_j( logA + logB[:,t+1] + log_beta[:,t+1] , axis=1 )\n",
    "            log_beta[:, t] = logsumexp(logA + (logB[:, t+1] + log_beta[:, t+1])[None, :], axis=1)\n",
    "\n",
    "        # Gamma\n",
    "        log_gamma = log_alpha + log_beta - loglik\n",
    "        # normalize per time to kill rounding; columns sum to 1 after exp\n",
    "        log_gamma -= logsumexp(log_gamma, axis=0)\n",
    "        gamma = np.exp(log_gamma)\n",
    "\n",
    "        # Xi\n",
    "        log_xi = np.empty((N, N, T-1))\n",
    "        for t in range(T-1):\n",
    "            tmp = log_alpha[:, t][:, None] + logA + (logB[:, t+1] + log_beta[:, t+1])[None, :]\n",
    "            tmp -= logsumexp(tmp)      # normalize this slice\n",
    "            log_xi[:, :, t] = tmp\n",
    "        xi = np.exp(log_xi)\n",
    "\n",
    "        # print(\"logB min/max:\", np.min(logB), np.max(logB))\n",
    "        # print(\"Any NaN in logB?\", np.any(np.isnan(logB)))\n",
    "        # print(\"Any Inf in logB?\", np.any(np.isinf(logB)))\n",
    "        # sanity checks (soft)\n",
    "        assert np.allclose(gamma.sum(axis=0), 1.0, atol=1e-8)\n",
    "        assert np.allclose(xi.sum(axis=(0,1)), 1.0, atol=1e-8)\n",
    "\n",
    "        self.log_forward  = log_alpha\n",
    "        self.log_backward = log_beta\n",
    "        self.posterior    = gamma\n",
    "        self.loglik       = loglik\n",
    "        return loglik, gamma, xi\n",
    "\n",
    "\n",
    "    def _m_step(self, gamma, xi):\n",
    "        numer = xi.sum(axis=2)                         # (N,N)\n",
    "        denom = gamma[:, :-1].sum(axis=1, keepdims=True)  # (N,1)\n",
    "        A = numer / (denom + 1e-12)\n",
    "        A = np.maximum(A, 1e-12)\n",
    "        A /= A.sum(axis=1, keepdims=True)\n",
    "        self.A = A\n",
    "        self.pi = gamma[:, 0] / gamma[:, 0].sum()\n",
    "        self.compute_coeffs() \n",
    "\n",
    "\n",
    "    def EM(self):\n",
    "        loglik, gamma, xi = self._e_step_log()\n",
    "        self._m_step(gamma, xi)\n",
    "        self.LL = loglik\n",
    "        # return loglik\n",
    "        \n",
    "    def fit_em(self, df_train):\n",
    "        \"\"\"\n",
    "        Run EM iterations until convergence (log-domain version).\n",
    "        \"\"\"\n",
    "\n",
    "        # Handle categorical variable encoding\n",
    "        if self.cat_variables is not None:\n",
    "            self.cat_var = {c: sorted(df_train[c].drop_duplicates().tolist(), key=lambda x: str(x))\n",
    "                            for c in self.cat_variables}\n",
    "            self.drop_categ = [sorted(df_train[col].drop_duplicates().tolist(), key=lambda x: str(x))[0]\n",
    "                               for col in self.cat_variables]\n",
    "        self.data_prep(df_train)\n",
    "\n",
    "        prev_ll = -np.inf\n",
    "        # store intermediate log-likelihoods\n",
    "        self.log_likelihoods = []\n",
    "        for it in range(self.iter):\n",
    "            # loglik, gamma, xi = self._e_step_log()\n",
    "            # self._m_step(gamma, xi)\n",
    "            self.EM()\n",
    "            if self.verb:\n",
    "                print(f\"Iter {it}: loglik={self.LL:.4f}\")\n",
    "            if it > 10:\n",
    "                if abs(self.LL - prev_ll) < self.tol:\n",
    "                    if self.verb:\n",
    "                        print(\"Converged.\")\n",
    "                    break\n",
    "            self.log_likelihoods.append(self.LL)\n",
    "            prev_ll = self.LL\n",
    "        return self.LL\n",
    "\n",
    "    def fit(self, df, n_iter=1):\n",
    "        \"\"\"\n",
    "        Refit the HMM regression model on new training data (log-domain version).\n",
    "        \"\"\"\n",
    "        if n_iter < 1:\n",
    "            raise ValueError(\"n_iter must be at least 1.\")\n",
    "        \n",
    "        self.data_prep(df)\n",
    "        if n_iter > 1:\n",
    "            prev_ll = self.LL\n",
    "            for _ in range(n_iter):\n",
    "                self.EM()\n",
    "                if self.verb:\n",
    "                    print(f\"Iter {_}: loglik={self.LL:.4f}\")\n",
    "                if abs(self.LL - prev_ll) < self.tol:\n",
    "                    break\n",
    "                else:\n",
    "                    prev_ll = self.LL\n",
    "        else:\n",
    "            self.EM()\n",
    "\n",
    "        return self.LL\n",
    "    \n",
    "    def predict_states(self):\n",
    "        return np.argmax(self.posterior, axis=0)\n",
    "    def predict_proba(self):\n",
    "        return self.posterior\n",
    "    \n",
    "    # AIC computation    \n",
    "    @property\n",
    "    def aic(self):\n",
    "        # k = self.N * self.X.shape[1] + self.N ** 2 + self.N - 1 # number of parameters: regression coeffs + transition matrix + initial state probs\n",
    "        k = self.N * self.X.shape[1] + self.N * (self.N - 1) + (self.N - 1) + self.N # number of parameters: regression coeffs + transition matrix + initial state probs + variance per state\n",
    "        return 2 * k - 2 * self.LL\n",
    "\n",
    "    @property\n",
    "    def aicc(self):\n",
    "        k = self.N * self.X.shape[1] + self.N * (self.N - 1) + (self.N - 1) + self.N # number of parameters: regression coeffs + transition matrix + initial state probs\n",
    "        n = self.T  # effective number of observations\n",
    "        return self.aic + (2 * k * (k + 1)) / (n - k - 1)\n",
    "\n",
    "    @property\n",
    "    def bic(self):\n",
    "        \n",
    "        k = self.N * self.X.shape[1] + self.N * (self.N - 1) + (self.N - 1) + self.N\n",
    "        \n",
    "        n = self.T  # effective number of observations\n",
    "        return -2 * self.LL + k * np.log(n)\n",
    "    \n",
    "    @property\n",
    "    def hqc(self):\n",
    "        k = self.N * self.X.shape[1] + self.N * (self.N - 1) + (self.N - 1) + self.N\n",
    "        n = self.T  # effective number of observations\n",
    "        return -2 * self.LL + 2 * k * np.log(np.log(n))\n",
    "\n",
    "    def copy(self):\n",
    "        return copy.deepcopy(self)\n",
    "\n",
    "    def forecast(self, H, exog=None):\n",
    "        \"\"\"\n",
    "        Forecast H periods ahead using fitted HMM regression (log-domain version), with advanced post-processing.\n",
    "\n",
    "        Handles:\n",
    "        - Trend re-adjustment (linear/ETS)\n",
    "        - Seasonal differencing reversal\n",
    "        - Regular differencing reversal\n",
    "        - Box-Cox back-transform\n",
    "        - Exogenous variables\n",
    "        - Lag transformations\n",
    "        \"\"\"\n",
    "        y_list = self.y.tolist()\n",
    "        forecasts_ = []\n",
    "        N = self.N\n",
    "\n",
    "        # Prepare exogenous future regressors if provided\n",
    "        if exog is not None:\n",
    "            if self.cons:\n",
    "                if exog.shape[0] == 1:\n",
    "                    exog.insert(0, 'const', 1)\n",
    "                else:\n",
    "                    exog = sm.add_constant(exog)\n",
    "            exog = np.array(self.data_prep(exog))\n",
    "\n",
    "\n",
    "        # Init with last forward distribution (in log)\n",
    "        log_forward_last = self.log_forward[:, -1]\n",
    "        logA = np.log(self.A + 1e-300)\n",
    "\n",
    "        # Forward\n",
    "        log_alpha = np.empty((N, H))\n",
    "        log_alpha[:, 0] = logsumexp(log_forward_last[:, None] + logA, axis=0)\n",
    "        for t in range(1, H):\n",
    "            # log_alpha[:,t] = logB[:,t] + logsumexp_i( log_alpha[i,t-1] + logA[i,:] )\n",
    "            log_alpha[:, t] = logsumexp(log_alpha[:, t-1][:, None] + logA, axis=0)\n",
    "\n",
    "        log_alpha -= logsumexp(log_alpha, axis=0)\n",
    "        self.forecast_forward = np.exp(log_alpha)\n",
    "        self.state_forecasts = np.argmax(self.forecast_forward, axis=0)\n",
    "        #per state forecasts\n",
    "        self.forecast_ps = np.zeros((N, H))\n",
    "\n",
    "        # Prepare for trend adjustment\n",
    "        # This assumes you stored original target (pre-trend removal) in self.target_orig\n",
    "        if hasattr(self, 'target_orig') and self.trend is not None:\n",
    "            if self.trend == \"linear\":\n",
    "                # future_time = np.arange(len(self.target_orig), len(self.target_orig) + H).reshape(-1, 1)\n",
    "                # trend_forecast = np.array(self.lr_model.predict(future_time))\n",
    "                trend_forecast= forecast_trend(model = self.lr_model, H=H, start=self.len, degree=self.pol, breakpoints=self.cps)\n",
    "            elif self.trend == \"ets\":\n",
    "                trend_forecast = np.array(self.ets_model_fit.forecast(H))\n",
    "\n",
    "        for t in range(H):\n",
    "            if exog is not None:\n",
    "                exo_inp = exog[t].tolist()\n",
    "            else:\n",
    "                exo_inp = [1] if self.cons else []\n",
    "            lags = [y_list[-l] for l in self.lags]\n",
    "            transform_lag = []\n",
    "            if self.lag_transform is not None:\n",
    "                series_array = np.array(y_list)\n",
    "                for func in self.lag_transform:\n",
    "                    transform_lag.append(func(series_array, is_forecast=True).to_numpy()[-1])\n",
    "            inp = np.array(exo_inp + lags + transform_lag)\n",
    "\n",
    "            state_preds = np.zeros(N)\n",
    "            for j in range(N):\n",
    "                mu = np.dot(self.coeffs[j], inp)\n",
    "                state_preds[j] = mu\n",
    "            self.forecast_ps[:, t] = state_preds\n",
    "\n",
    "            # normalize to probabilities\n",
    "\n",
    "            # normalize to probabilities\n",
    "            pred_w = np.sum(self.forecast_forward[:, t] * state_preds)\n",
    "            forecasts_.append(pred_w)\n",
    "            y_list.append(pred_w)\n",
    "\n",
    "            # log_forward_last = log_f_t.copy()\n",
    "\n",
    "        forecasts = np.array(forecasts_)\n",
    "\n",
    "        if self.trend is not None:\n",
    "            forecasts += trend_forecast\n",
    "            self.forecast_ps += trend_forecast\n",
    "\n",
    "        # --- Revert seasonal differencing if applied ---\n",
    "        if self.season_diff is not None:\n",
    "            forecasts = invert_seasonal_diff(self.orig_d, forecasts, self.season_diff)\n",
    "            # Also revert seasonal differencing for per state forecasts\n",
    "            for s in range(self.N):\n",
    "                self.forecast_ps[s] = invert_seasonal_diff(self.orig_d, self.forecast_ps[s], self.season_diff)\n",
    "\n",
    "        # --- Revert regular differencing if applied ---\n",
    "        if self.diff is not None:\n",
    "            forecasts = undiff_ts(self.orig, forecasts, self.diff)\n",
    "            # Also revert differencing for per state forecasts\n",
    "            for s in range(self.N):\n",
    "                self.forecast_ps[s] = undiff_ts(self.orig, self.forecast_ps[s], self.diff)\n",
    "\n",
    "        # --- Box-Cox back-transform if applied ---\n",
    "        if self.box_cox:\n",
    "            forecasts = back_box_cox_transform(\n",
    "                y_pred=forecasts, lmda=self.lamda,\n",
    "                shift=self.is_zero, box_cox_biasadj=self.biasadj\n",
    "            )\n",
    "            for s in range(self.N):\n",
    "                self.forecast_ps[s] = back_box_cox_transform(\n",
    "                    y_pred=self.forecast_ps[s], lmda=self.lamda,\n",
    "                    shift=self.is_zero, box_cox_biasadj=self.biasadj\n",
    "                )\n",
    "\n",
    "        return forecasts\n",
    "\n",
    "\n",
    "    def get_param_spec(self):\n",
    "        \"\"\"\n",
    "        Get the model parameters: coefficients, stds, initial state distribution, transition matrix.\n",
    "        \"\"\"\n",
    "\n",
    "        # Weighted R2 calculation for each regime\n",
    "        y_weighted = self.y * self.posterior\n",
    "        y_wmean = y_weighted.mean(axis=1)\n",
    "        self.regime_fitted = (self.coeffs[:, None, :] * (self.posterior[:, :, None]*self.X)).sum(axis=2)\n",
    "\n",
    "        # Calculate TSS, RSS, and R2 for each regime\n",
    "        tss_w = ((y_weighted-y_wmean[:, None])**2).sum(axis=1)\n",
    "        rss_w = ((y_weighted - self.regime_fitted)**2).sum(axis=1)\n",
    "        self.regime_r2 = 1-rss_w/tss_w\n",
    "\n",
    "        # Overall R2 calculation\n",
    "        self.fitted = self.regime_fitted.sum(axis=0)\n",
    "        y_mean = np.mean(self.y)\n",
    "        rss = np.sum((self.y - self.fitted) ** 2)\n",
    "        tss = np.sum((self.y - y_mean) ** 2)\n",
    "        self.r2 = 1 - (rss / tss)\n",
    "\n",
    "        # coefs = self.coeffs # shape (s, p)\n",
    "        T = len(self.y)\n",
    "        # n_states = self.n_components\n",
    "        p = self.coeffs.shape[1]\n",
    "\n",
    "        arr = np.empty((p, 0)) \n",
    "        fit_ = 0\n",
    "        for s in range(self.N):\n",
    "            w = np.diag(self.posterior[s])\n",
    "            X_weight = np.dot(w, self.X)\n",
    "            y_weight = np.dot(w, self.y)\n",
    "            fit_ += (self.coeffs[s]*X_weight).sum(axis = 1)\n",
    "            resid = y_weight-(self.coeffs[s]*X_weight).sum(axis = 1)\n",
    "            # resid1 = hmm_model.y-(coefs[0]*hmm_model.X).sum(axis = 1)\n",
    "            s2 = resid.T @ resid / (T - p)\n",
    "            var_b = s2 * np.linalg.inv(self.X.T @ w @ self.X)\n",
    "            se_b = np.sqrt(var_b)\n",
    "            t_s = self.coeffs[s] / se_b.diagonal()\n",
    "            # Degrees of freedom\n",
    "            df = T - p  # degrees of freedom (n - number of coefficients)\n",
    "            # Calculate p-values for coefficients\n",
    "            p_values = (1 - t.cdf(np.abs(t_s), df)) * 2\n",
    "            # pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "            stats_b = np.column_stack((self.coeffs[s], se_b.diagonal(), t_s, p_values))\n",
    "            arr = np.concatenate((arr, stats_b), axis=1)\n",
    "        self.param_spec_df = pd.DataFrame(arr, index=self.col_names).reset_index().rename(columns={\"index\": \"variable\"})\n",
    "\n",
    "    def summary(self, font_size=12, row_padding=4, col_labels_padding=4):\n",
    "        \"\"\"\n",
    "        Print a summary of the fitted HMM regression model.\n",
    "        Args:\n",
    "            font_size (str): Font size for the summary table.\n",
    "            row_padding (str): Padding for data rows in the summary table.\n",
    "            col_labels_padding (str): Padding for column labels in the summary table.\n",
    "        Returns:\n",
    "            gt.Table: A gt table object containing the model summary.\n",
    "        \"\"\"\n",
    "        # self.stats()\n",
    "        self.get_param_spec()\n",
    "        # Create state probabilities dataframe for display\n",
    "        state_probs = pd.DataFrame(pd.Series(self.predict_states()).value_counts(normalize=True).sort_index())\n",
    "        state_probs.index = [f\"regime_{i+1}\" for i in state_probs.index]\n",
    "        state_probs[state_probs.select_dtypes(include='number').columns] = (\n",
    "            state_probs.select_dtypes(include='number')\n",
    "                .applymap(lambda x: f\"{x:.3f}\")\n",
    "        )\n",
    "        state_probs = state_probs.T\n",
    "        reg_vars = pd.DataFrame([{f\"regime_{i+1}\": f\"{v:.3f}\" for i, v in enumerate(np.round(self.stds**2, 3))}])\n",
    "        # Transition matrix dataframe\n",
    "        tm = pd.DataFrame(self.A)\n",
    "        tm.columns = [f\"regime_{i+1}\" for i in range(tm.shape[1])]\n",
    "        tm.index = [f\"regime_{i+1}\" for i in range(tm.shape[0])]\n",
    "        tm_df = tm.reset_index().rename(columns={\"index\": \"\"})\n",
    "        tm_df[tm_df.select_dtypes(include='number').columns] = (\n",
    "            tm_df.select_dtypes(include='number')\n",
    "                .applymap(lambda x: f\"{x:.3f}\"))      \n",
    "\n",
    "        ## Data info\n",
    "        data = {\"dep. Variable\": self.target_col, \"n_obs\": len(self.y), \"df_model\": self.coeffs.shape[1]}\n",
    "        data_df = pd.DataFrame([data]).T\n",
    "        data_df.rename(columns={0: \" \"}, inplace=True)\n",
    "        data_df = data_df.reset_index().rename(columns={\"index\": \"\"})\n",
    "\n",
    "        # diagnostics\n",
    "        diagn = {\"overall_R-squared\": round(self.r2, 3)}\n",
    "        regime_r2s = {f\"regime_{i+1}_R-squared\": round(j, 3) for i, j in enumerate(self.regime_r2)}\n",
    "        # append both dictionaries\n",
    "        diagn.update(regime_r2s)\n",
    "        pd.DataFrame([diagn])\n",
    "        how_fit = {f\"log-likelihood\": round(self.loglik, 1), \"AIC\": round(self.aic, 1), \"BIC\": round(self.bic, 1)}\n",
    "        diagn.update(how_fit)\n",
    "\n",
    "        diagnosis_df = pd.DataFrame([diagn]).T\n",
    "        diagnosis_df.rename(columns={0: \"diagnostics\"}, inplace=True)\n",
    "        diagnosis_df = diagnosis_df.reset_index().rename(columns={\"index\": \"\"})\n",
    "\n",
    "        # make_main_gt, gt_mini, inject_header_table_groups\n",
    "        gt_maint = make_main_gt(self.param_spec_df, n_regimes=self.N)\n",
    "\n",
    "        gt_final = inject_header_table_groups(\n",
    "            gt_maint,\n",
    "            columns=[\n",
    "                [(\"Regime probabilities\", state_probs, True), (\"Regime variances\", reg_vars, True)],\n",
    "                [(\"Transition probabilities\", tm_df, True)],\n",
    "                [(\"Data\", data_df, False)],                 # title only → one line        \n",
    "                [(\"Diagnostics\", diagnosis_df, False)],\n",
    "            ],\n",
    "            subtitle_text=\"Regime-switching hidden markov regression model results\").tab_options(\n",
    "        table_font_size=f\"{font_size}px\",          # shrink all text\n",
    "        data_row_padding=f\"{row_padding}px\",          # tighten row height\n",
    "        column_labels_padding=f\"{col_labels_padding}px\"  # tighten header band\n",
    "        )\n",
    "        return gt_final   \n",
    "\n",
    "# Hidden Markov Model with Vector Autoregressive (VAR)\n",
    "\n",
    "class MsHmmVar:\n",
    "    \"\"\"\n",
    "    Hidden Markov Model with Vector Autoregressive (VAR) emission distributions.\n",
    "\n",
    "    This model can be used for time series with multiple targets, capturing regime-switching dynamics\n",
    "    using multivariate Gaussian emissions whose means depend on lagged values (VAR).\n",
    "\n",
    "    Args:\n",
    "        n_components (int): Number of hidden states.\n",
    "        target_col (list of str): List of target variable names.\n",
    "        lags (dict): Dict mapping target column to list of lag values.\n",
    "        difference (dict): Dict mapping target column to integer difference order.\n",
    "        seasonal_diff (dict): Dict mapping target column to seasonal difference order.\n",
    "        lag_transform (dict): Dict mapping target column to lag transformation order.\n",
    "        trend (dict): Dict mapping target column to trend removal method.\n",
    "        pol_degree (int): Polynomial degree if trend is 'linear'. Default is 1 for all target columns. For example, {target1: 1, target2: [1,4]}.\n",
    "        ets_params : Optional[Dict[str, tuple]] (default=None)\n",
    "        Dictionary specifying params for ExponentialSmoothing per variable.\n",
    "        For example, {'target1': ({'trend': 'add', 'seasonal': 'add'}, {'damped_trend': True}), 'target2': ({'trend': 'add', 'seasonal': 'add'}, {'damped_trend': True})}.\n",
    "        change_points (list or None): List of change points for piecewise linear regression to handle trend\n",
    "        method (str): 'posterior' for soft state assignment, 'viterbi' for hard paths.\n",
    "        covariance_type (str): 'full' (default) or 'diag' for emission covariances. default is \"diag\".\n",
    "        startprob_prior, transmat_prior: Dirichlet prior values.\n",
    "        add_constant (bool): Whether to add constant/intercept to regressors.\n",
    "        cat_variables (list): List of categorical columns to encode.\n",
    "        n_iter (int): Maximum EM iterations.\n",
    "        tol (float): Convergence tolerance.\n",
    "        coefficients (list of np.ndarray): Initial state-wise VAR coefficient matrices.\n",
    "        init_state (np.ndarray): Initial state probabilities.\n",
    "        trans_matrix (np.ndarray): Initial transition matrix.\n",
    "        random_state (int): Seed.\n",
    "        switching_cov (bool): If True, allows covariance to switch between states.\n",
    "        verbose (bool): Print progress if True.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components, target_col, lags, difference=None, method=\"posterior\", covariance_type=\"full\",\n",
    "                 startprob_prior=1e3, transmat_prior=1e5, add_constant=True, cat_variables=None, lag_transform=None, seasonal_diff=None, trend=None,\n",
    "                 pol_degree=1, ets_params = None, change_points=None,\n",
    "                 n_iter=100, tol=1e-6, coefficients=None, init_state=None, trans_matrix=None, box_cox=None, lamda=None, box_cox_biasadj=False,\n",
    "                 random_state=None, switching_cov=True, verbose=False):\n",
    "\n",
    "        self.N = n_components\n",
    "        self.target_col = target_col\n",
    "        self.diffs = difference\n",
    "        self.season_diff = seasonal_diff\n",
    "        self.cons = add_constant\n",
    "        # make sure lags is a dict mapping target columns to lists of lags even the lag is integer\n",
    "        if not isinstance(lags, dict):\n",
    "            raise ValueError(\"Lags must be a dictionary mapping target columns to lists of lags or a single integer.\")\n",
    "        self.lags = {col: lags[col] if isinstance(lags[col], list) else list(range(1, lags[col] + 1)) for col in lags}\n",
    "        self.cat_variables = cat_variables\n",
    "        self.lag_transform = lag_transform\n",
    "        self.method = method\n",
    "        self.iter = n_iter\n",
    "        self.tol = tol\n",
    "        self.verb = verbose\n",
    "        self.cvr = covariance_type\n",
    "        self.switching_cov = switching_cov\n",
    "        self.coeffs = coefficients\n",
    "        self.box_cox = box_cox\n",
    "        if lamda is None:\n",
    "            self.lamda = {col: None for col in self.target_col}\n",
    "        else:\n",
    "            # must be dictionary mapping target columns to Box-Cox lambda values\n",
    "            if not isinstance(lamda, dict):\n",
    "                raise ValueError(\"Lambda must be a dictionary mapping target columns to Box-Cox lambda values.\")\n",
    "            self.lamda = lamda\n",
    "        if box_cox_biasadj == False:\n",
    "            self.biasadj = {col: False for col in self.target_col}\n",
    "        else:\n",
    "            # must be dictionary mapping target columns to boolean values\n",
    "            if not isinstance(box_cox_biasadj, dict):\n",
    "                raise ValueError(\"Box-Cox bias adjustment must be a dictionary mapping target columns to boolean values.\")\n",
    "            self.biasadj = box_cox_biasadj\n",
    "\n",
    "        # Handle trend default types\n",
    "        self.trend = trend\n",
    "        if isinstance(pol_degree, int):\n",
    "            self.pol = {col: pol_degree for col in self.target_col}\n",
    "        elif isinstance(pol_degree, dict):\n",
    "            self.pol = {col: pol_degree[col] if col in pol_degree else 1 for col in self.target_col}\n",
    "        else:\n",
    "            raise ValueError(\"pol_degree must be an integer or a dictionary mapping target columns to integer degrees.\")\n",
    "        if self.trend is not None:\n",
    "            if not isinstance(self.trend, dict):\n",
    "                raise TypeError(\"trend must be a dictionary of target values\")\n",
    "        self.ets_params = ets_params\n",
    "        # Initialization of state, transition, coefficients and covariances\n",
    "        self.cps = change_points\n",
    "        self.rng = np.random.default_rng(random_state)\n",
    "        if init_state is None:\n",
    "            self.alpha_p = np.repeat(startprob_prior, self.N)\n",
    "            self.pi = self.rng.dirichlet(self.alpha_p)\n",
    "        else:\n",
    "            self.pi = np.array(init_state)\n",
    "\n",
    "        if trans_matrix is None:\n",
    "            self.alpha_t = np.repeat(transmat_prior, self.N)\n",
    "            self.A = self.rng.dirichlet(self.alpha_t, size=self.N)\n",
    "        else:\n",
    "            self.A = np.array(trans_matrix)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Data preparation\n",
    "    # -----------------------------\n",
    "    def data_prep(self, df):\n",
    "        \"\"\"\n",
    "        Prepare data: encode categoricals, handle differencing, add lags.\n",
    "        Returns DataFrame with targets and features.\n",
    "        \"\"\"\n",
    "        dfc = df.copy()\n",
    "        if self.cat_variables is not None:\n",
    "            for col, cat in self.cat_var.items():\n",
    "                dfc[col] = pd.Categorical(dfc[col], categories=cat)\n",
    "            dfc = pd.get_dummies(dfc, dtype=np.float64)\n",
    "            for i in self.drop_categ:\n",
    "                dfc.drop(list(dfc.filter(regex=i)), axis=1, inplace=True)\n",
    "        if all(elem in dfc.columns for elem in self.target_col):\n",
    "\n",
    "            # Apply Box–Cox transformation if specified\n",
    "            if self.box_cox is not None:\n",
    "                self.is_zero = {col: np.any(np.array(dfc[col]) < 1) for col in self.box_cox}  # check for zero or negative values\n",
    "                for col in self.box_cox:\n",
    "                    if self.box_cox[col]:\n",
    "                        self.is_zero[col] = np.any(np.array(dfc[col]) < 1)  # check for zero or negative values\n",
    "                        trans_data, self.lamda[col] = box_cox_transform(x=dfc[col],\n",
    "                                                                shift=self.is_zero[col],\n",
    "                                                                box_cox_lmda=self.lamda[col])\n",
    "                        dfc[col] = trans_data\n",
    "\n",
    "\n",
    "            if self.trend is not None:\n",
    "                self.len = df.shape[0]\n",
    "                self.orig_targets = {i: dfc[i] for i in self.trend.keys()}  # Store original values for later use during forecasting\n",
    "                self.trend_models = {}\n",
    "                for k, v in self.trend.items():\n",
    "                    if v == \"linear\":\n",
    "                        # model_fit = LinearRegression().fit(np.arange(self.len).reshape(-1, 1), self.orig_targets[k])\n",
    "                        # dfc[k] = dfc[k] - model_fit.predict(np.arange(self.len).reshape(-1, 1))\n",
    "                        # self.trend_models[k] = model_fit\n",
    "                            # self.lr_model = LinearRegression().fit(np.arange(self.len).reshape(-1, 1), dfc[self.target_col])\n",
    "                        if self.cps is not None:\n",
    "                            if k in self.cps and self.cps[k]:\n",
    "                                trend, model_fit = lr_trend_model(self.orig_targets[k], degree=self.pol[k], breakpoints=self.cps[k], type='piecewise')\n",
    "                        else:\n",
    "                            trend, model_fit = lr_trend_model(self.orig_targets[k], degree=self.pol[k])\n",
    "\n",
    "                        dfc[k] = dfc[k] - trend\n",
    "                        self.trend_models[k] = model_fit\n",
    "\n",
    "                    elif v == \"ets\": # ets\n",
    "                        model_fit = ExponentialSmoothing(self.orig_targets[k], **self.ets_params[k][0]).fit(**self.ets_params[k][1])\n",
    "                        dfc[k] = dfc[k] - model_fit.fittedvalues.values\n",
    "                        self.trend_models[k] = model_fit\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unknown trend type: {v}\")\n",
    "\n",
    "            # Apply differencing if specified\n",
    "            if self.diffs is not None:\n",
    "                # Save original values for inverse-differencing later for forecasting\n",
    "                self.origs = {}\n",
    "                for col in self.diffs.keys():\n",
    "                    if self.diffs[col] is not None:\n",
    "                        self.origs[col] = dfc[col].tolist()\n",
    "                        dfc[col] = np.diff(dfc[col], n=self.diffs[col],\n",
    "                                                    prepend=np.repeat(np.nan, self.diffs[col]))\n",
    "            # Apply seasonal differencing if specified\n",
    "            if self.season_diff is not None:\n",
    "                # Save original values for inverse-differencing later for forecasting\n",
    "                self.orig_d = {}\n",
    "                for col in self.season_diff.keys():\n",
    "                    if self.season_diff[col] is not None:\n",
    "                        self.orig_d[col] = dfc[col].tolist()\n",
    "                        dfc[col] = seasonal_diff(dfc[col], self.season_diff[col])\n",
    "\n",
    "            # Add lags for each target\n",
    "            if self.lags is not None:\n",
    "                for col, lags in self.lags.items():\n",
    "                    for lag in lags:\n",
    "                        dfc[f\"{col}_lag_{lag}\"] = dfc[col].shift(lag)\n",
    "            # Add lag transformations if specified\n",
    "            if self.lag_transform is not None:\n",
    "                for idx, (target, funcs) in enumerate(self.lag_transform.items()):\n",
    "                    for func in funcs:\n",
    "                        if isinstance(func, (expanding_std, expanding_mean)):\n",
    "                            dfc[f\"trg{idx}_{func.__class__.__name__}_shift_{func.shift}\"] = func(dfc[target])\n",
    "                        elif isinstance(func, expanding_quantile):\n",
    "                            dfc[f\"trg{idx}_{func.__class__.__name__}_shift_{func.shift}_q{func.quantile}\"] = func(dfc[target])\n",
    "                        elif isinstance(func, rolling_quantile):\n",
    "                            dfc[f\"trg{idx}_{func.__class__.__name__}_{func.window_size}_shift_{func.shift}_q{func.quantile}\"] = func(dfc[target])\n",
    "                        else:\n",
    "                            dfc[f\"trg{idx}_{func.__class__.__name__}_{func.window_size}_shift_{func.shift}\"] = func(dfc[target])\n",
    "\n",
    "            self.df = dfc.dropna()\n",
    "            self.X = self.df.drop(columns=self.target_col)\n",
    "            self.y = self.df[self.target_col]\n",
    "            if self.cons:\n",
    "                self.X = sm.add_constant(self.X)\n",
    "            self.col_names = self.X.columns.tolist() if hasattr(self.X, 'columns') else [f\"x{i}\" for i in range(self.X.shape[1])]\n",
    "            self.X = np.array(self.X)\n",
    "            self.y = np.array(self.y)\n",
    "            self.T = len(self.y)\n",
    "\n",
    "            # Init coeffs/stds if None\n",
    "            if self.coeffs is None:\n",
    "                coeffs, covs = [], []\n",
    "                for _ in range(self.N):\n",
    "                    beta = np.linalg.lstsq(self.X, self.y, rcond=None)[0]\n",
    "                    coeffs.append(beta)\n",
    "                    resid = self.y - self.X @ beta\n",
    "                    eff_df = len(resid) - self.X.shape[1]\n",
    "                    eff_df = max(eff_df, 1)\n",
    "                    cov_i = (resid.T @ resid) / eff_df\n",
    "                    covs.append(cov_i)\n",
    "                self.coeffs = np.stack(coeffs, axis=0)\n",
    "                self.covs = np.array(covs)\n",
    "\n",
    "        else:\n",
    "            return dfc.dropna()\n",
    "\n",
    "    # -----------------------------\n",
    "    # Emissions\n",
    "    # -----------------------------\n",
    "    def _log_emissions(self):\n",
    "        N, T = self.N, self.T\n",
    "        logB = np.empty((N, T))\n",
    "        self.fitted = np.empty((N, T, self.y.shape[1]))   # (N, T, m)\n",
    "        for s in range(N):\n",
    "            mus = self.X @ self.coeffs[s]\n",
    "            self.fitted[s, :, :] = mus\n",
    "            for t in range(T):\n",
    "                logB[s, t] = multivariate_normal(mean=mus[t], cov=self.covs[s]).logpdf(self.y[t])\n",
    "        return logB\n",
    "    \n",
    "    # -----------------------------\n",
    "    # E-step: perform expectation step: forward and backward and posterior update\n",
    "    # -----------------------------\n",
    "\n",
    "    def _e_step_log(self):\n",
    "        N, T = self.N, self.T\n",
    "        logA  = np.log(self.A + 1e-300)\n",
    "        logpi = np.log(self.pi + 1e-300)\n",
    "        logB  = self._log_emissions()\n",
    "\n",
    "        # Forward\n",
    "        log_alpha = np.empty((N, T))\n",
    "        log_alpha[:, 0] = logpi + logB[:, 0]\n",
    "        for t in range(1, T):\n",
    "            # log_alpha[:,t] = logB[:,t] + logsumexp_i( log_alpha[i,t-1] + logA[i,:] )\n",
    "            log_alpha[:, t] = logB[:, t] + logsumexp(log_alpha[:, t-1][:, None] + logA, axis=0)\n",
    "\n",
    "        # Log-likelihood\n",
    "        loglik = logsumexp(log_alpha[:, -1])\n",
    "\n",
    "        # Backward\n",
    "        log_beta = np.full((N, T), 0.0)\n",
    "        for t in range(T-2, -1, -1):\n",
    "            # log_beta[:,t] = logsumexp_j( logA + logB[:,t+1] + log_beta[:,t+1] , axis=1 )\n",
    "            log_beta[:, t] = logsumexp(logA + (logB[:, t+1] + log_beta[:, t+1])[None, :], axis=1)\n",
    "\n",
    "        # Gamma\n",
    "        log_gamma = log_alpha + log_beta - loglik\n",
    "        # normalize per time to kill rounding; columns sum to 1 after exp\n",
    "        log_gamma -= logsumexp(log_gamma, axis=0)\n",
    "        gamma = np.exp(log_gamma)\n",
    "\n",
    "        # Xi\n",
    "        log_xi = np.empty((N, N, T-1))\n",
    "        for t in range(T-1):\n",
    "            tmp = log_alpha[:, t][:, None] + logA + (logB[:, t+1] + log_beta[:, t+1])[None, :]\n",
    "            tmp -= logsumexp(tmp)      # normalize this slice\n",
    "            log_xi[:, :, t] = tmp\n",
    "        xi = np.exp(log_xi)\n",
    "\n",
    "        # print(\"logB min/max:\", np.min(logB), np.max(logB))\n",
    "        # print(\"Any NaN in logB?\", np.any(np.isnan(logB)))\n",
    "        # print(\"Any Inf in logB?\", np.any(np.isinf(logB)))\n",
    "        # sanity checks (soft)\n",
    "        assert np.allclose(gamma.sum(axis=0), 1.0, atol=1e-8)\n",
    "        assert np.allclose(xi.sum(axis=(0,1)), 1.0, atol=1e-8)\n",
    "\n",
    "        self.log_forward  = log_alpha\n",
    "        self.log_backward = log_beta\n",
    "        self.posterior    = gamma\n",
    "        self.loglik       = loglik\n",
    "        return loglik, gamma, xi\n",
    "    \n",
    "    # -----------------------------\n",
    "    # M-step: perform expectation step: forward and backward and posterior update\n",
    "    # -----------------------------\n",
    "\n",
    "    def _m_step(self, gamma, xi):\n",
    "        numer = xi.sum(axis=2)                         # (N,N)\n",
    "        denom = gamma[:, :-1].sum(axis=1, keepdims=True)  # (N,1)\n",
    "        A = numer / (denom + 1e-12)\n",
    "        A = np.maximum(A, 1e-12)\n",
    "        A /= A.sum(axis=1, keepdims=True)\n",
    "        self.A = A\n",
    "        self.pi = gamma[:, 0] / gamma[:, 0].sum()\n",
    "        self.compute_coeffs() \n",
    "\n",
    "\n",
    "    def EM(self):\n",
    "        loglik, gamma, xi = self._e_step_log()\n",
    "        self._m_step(gamma, xi)\n",
    "        self.LL = loglik\n",
    "        # return loglik\n",
    "    \n",
    "    # -----------------------------\n",
    "    # perform iterations using EM\n",
    "    # -----------------------------\n",
    "        \n",
    "    def fit_em(self, df_train):\n",
    "        \"\"\"\n",
    "        Run EM iterations until convergence (log-domain version).\n",
    "        \"\"\"\n",
    "\n",
    "        # Handle categorical variable encoding\n",
    "        if self.cat_variables is not None:\n",
    "            self.cat_var = {c: sorted(df_train[c].drop_duplicates().tolist(), key=lambda x: str(x))\n",
    "                            for c in self.cat_variables}\n",
    "            self.drop_categ = [sorted(df_train[col].drop_duplicates().tolist(), key=lambda x: str(x))[0]\n",
    "                               for col in self.cat_variables]\n",
    "        self.data_prep(df_train)\n",
    "\n",
    "        prev_ll = -np.inf\n",
    "        # store intermediate log-likelihoods\n",
    "        self.log_likelihoods = []\n",
    "        for it in range(self.iter):\n",
    "            # loglik, gamma, xi = self._e_step_log()\n",
    "            # self._m_step(gamma, xi)\n",
    "            self.EM()\n",
    "            if self.verb:\n",
    "                print(f\"Iter {it}: loglik={self.LL:.4f}\")\n",
    "            if it > 10:\n",
    "                if abs(self.LL - prev_ll) < self.tol:\n",
    "                    if self.verb:\n",
    "                        print(\"Converged.\")\n",
    "                    break\n",
    "            self.log_likelihoods.append(self.LL)\n",
    "            prev_ll = self.LL\n",
    "        # self.LL = self.LL\n",
    "        return self.LL\n",
    "\n",
    "    # -----------------------------\n",
    "    # Compute coefficients and covariances\n",
    "    # -----------------------------\n",
    "\n",
    "    def compute_coeffs(self, ridge=1e-5, var_floor=1e-5, w_floor=1e-6):\n",
    "\n",
    "        # Update regression coefficients and stds for each state\n",
    "\n",
    "        # If posterior probabilities are shorter than the number of observations, make self.X and self.y same length visa vis make posterier same as self.X and self.y length\n",
    "        if self.posterior.shape[1] < self.X.shape[0]:\n",
    "            # Truncate self.X and self.y to match the length of self.posterior[i]\n",
    "            self.X = self.X[:self.posterior.shape[1]]\n",
    "            self.y = self.y[:self.posterior.shape[1]]\n",
    "        if self.posterior.shape[1] > self.X.shape[0]:\n",
    "            # Truncate self.posterior to match the length of self.X and self.y\n",
    "            self.posterior = self.posterior[:, -self.X.shape[0]:]\n",
    "\n",
    "        covs, coeffs = [], []\n",
    "        X, y = self.X, self.y\n",
    "        for s in range(self.N):\n",
    "            w = self.posterior[s] + w_floor\n",
    "            w /= w.sum()\n",
    "            sw = np.sqrt(w)\n",
    "            self.sw = sw\n",
    "            Xw = X * sw[:, None]\n",
    "            yw = y * sw[:, None]  # <-- THIS IS THE FIX\n",
    "            XtX = Xw.T @ Xw + ridge * np.eye(X.shape[1]) # regularization\n",
    "            Xty = Xw.T @ yw\n",
    "            beta_s = np.linalg.lstsq(XtX, Xty, rcond=None)[0]\n",
    "            coeffs.append(beta_s)\n",
    "            resid = y - X @ beta_s\n",
    "            eff_df = np.sum(w) - X.shape[1]\n",
    "            eff_df = max(eff_df, 1)\n",
    "            cov_i = (w[:, None] * resid).T @ resid / eff_df\n",
    "            # Regularize covariance matrix\n",
    "            # cov_i += var_floor * np.eye(cov_i.shape[0])\n",
    "            eigvals, eigvecs = np.linalg.eigh(cov_i)\n",
    "            eigvals = np.clip(eigvals, var_floor, None)\n",
    "            cov_i = eigvecs @ np.diag(eigvals) @ eigvecs.T\n",
    "\n",
    "            covs.append(cov_i)\n",
    "            # var_s = (w * resid**2).sum() / max(w.sum(), 1.0)\n",
    "        self.coeffs = np.stack(coeffs)\n",
    "        # if self.cvr == \"full\":\n",
    "        #     self.covs = covs\n",
    "        # elif self.cvr == \"diag\":\n",
    "        #     self.covs = [np.diag(np.diag(cov_i)) for cov_i in covs]\n",
    "\n",
    "            # Decide covariance structure\n",
    "        if not self.switching_cov:  # single covariance across all regimes\n",
    "            mean_cov = sum(covs) / self.N\n",
    "            if self.cvr == \"diag\":\n",
    "                mean_cov = np.diag(np.diag(mean_cov))\n",
    "            self.covs = [mean_cov for _ in range(self.N)]\n",
    "        else:  # different covariance per regime\n",
    "            if self.cvr == \"full\":\n",
    "                self.covs = covs\n",
    "            elif self.cvr == \"diag\":\n",
    "                self.covs = [np.diag(np.diag(cov_i)) for cov_i in covs]\n",
    "\n",
    "    # -----------------------------\n",
    "    # Fit model with learned parameters\n",
    "    # -----------------------------\n",
    "\n",
    "    def fit(self, df, n_iter=1):\n",
    "        \"\"\"\n",
    "        Refit the HMM-VAR model on new training data\n",
    "        \"\"\"\n",
    "        if n_iter < 1:\n",
    "            raise ValueError(\"n_iter must be at least 1.\")\n",
    "        \n",
    "        self.data_prep(df)\n",
    "        if n_iter > 1:\n",
    "            prev_ll = self.LL\n",
    "            for _ in range(n_iter):\n",
    "                self.EM()\n",
    "                if self.verb:\n",
    "                    print(f\"Iter {_}: loglik={self.LL:.4f}\")\n",
    "                if abs(self.LL - prev_ll) < self.tol:\n",
    "                    break\n",
    "                else:\n",
    "                    prev_ll = self.LL\n",
    "        else:\n",
    "            self.EM()\n",
    "\n",
    "        return self.LL\n",
    "\n",
    "    def predict_states(self):\n",
    "        return np.argmax(self.posterior, axis=0)\n",
    "    def predict_proba(self):\n",
    "        return self.posterior\n",
    "\n",
    "    # AIC computation    \n",
    "    @property\n",
    "    def aic(self):\n",
    "        d = self.y.shape[1]     # number of dependent variables\n",
    "        r = self.X.shape[1]     # number of predictors (lags + exogenous, intercept included)\n",
    "        \n",
    "        # Parameters per regime (state):\n",
    "        # regression coefficients + covariance parameters\n",
    "        per_regime = r * d + d * (d + 1) / 2  \n",
    "        \n",
    "        # Total parameters:\n",
    "        # - regime-specific: N * per_regime\n",
    "        # - transition matrix: N*(N-1)\n",
    "        # - initial distribution: (N-1)\n",
    "        n_params = self.N * per_regime + self.N * (self.N - 1) + (self.N - 1)\n",
    "        \n",
    "        return 2 * n_params - 2 * self.LL\n",
    "    \n",
    "    @property\n",
    "    def aicc(self):\n",
    "        d = self.y.shape[1]  # number of dependent variables\n",
    "        r = self.X.shape[1]  # number of predictors (lags + exogenous, intercept included)\n",
    "        \n",
    "        per_regime = r * d + d * (d + 1) / 2  # regression + covariance per state\n",
    "        k = self.N * per_regime + self.N*(self.N-1) + (self.N-1)  # total params\n",
    "        \n",
    "        n = self.T  # effective number of observations\n",
    "        # aic = -2 * self.LL + 2 * k\n",
    "        correction = (2 * k * (k + 1)) / (n - k - 1) if n - k - 1 > 0 else 0\n",
    "        return self.aic + correction\n",
    "    \n",
    "    @property\n",
    "    def bic(self):\n",
    "        d = self.y.shape[1]  # number of dependent variables\n",
    "        r = self.X.shape[1]  # number of predictors (lags + exogenous, intercept included)\n",
    "        \n",
    "        per_regime = r * d + d * (d + 1) / 2  # regression + covariance per state\n",
    "        k = self.N * per_regime + self.N*(self.N-1) + (self.N-1)  # total params\n",
    "        \n",
    "        n = self.T  # effective number of observations\n",
    "        return -2 * self.LL + k * np.log(n)\n",
    "\n",
    "    @property\n",
    "    def hqc(self):\n",
    "        d = self.y.shape[1]  # number of dependent variables\n",
    "        r = self.X.shape[1]  # number of predictors (lags + exogenous, intercept included)\n",
    "        \n",
    "        per_regime = r * d + d * (d + 1) / 2  # regression + covariance per state\n",
    "        k = self.N * per_regime + self.N*(self.N-1) + (self.N-1)  # total params\n",
    "        \n",
    "        n = self.T  # effective number of observations\n",
    "        return -2 * self.LL + 2 * k * np.log(np.log(n))\n",
    "    \n",
    "    def copy(self):\n",
    "        return copy.deepcopy(self)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Forecasting\n",
    "    # -----------------------------\n",
    "    def forecast(self, H, exog=None):\n",
    "        \"\"\"\n",
    "        Forecast H periods ahead using fitted HMM-VAR model (log-domain version).\n",
    "        Includes trend adjustment, differencing reversal, seasonal differencing,\n",
    "        and Box-Cox back-transform.\n",
    "        \"\"\"\n",
    "        if exog is not None:\n",
    "            if self.cons:\n",
    "                if exog.shape[0] == 1:\n",
    "                    exog.insert(0, 'const', 1)\n",
    "                else:\n",
    "                    exog = sm.add_constant(exog)\n",
    "            exog = np.array(self.data_prep(exog))\n",
    "        y_dict = {col: self.y[:, i].tolist() for i, col in enumerate(self.target_col)}\n",
    "        forecasts = {col: [] for col in self.target_col}\n",
    "\n",
    "        # Init with last forward distribution (in log)\n",
    "        log_forward_last = self.log_forward[:, -1]\n",
    "        logA = np.log(self.A + 1e-300)\n",
    "        N = self.N\n",
    "        # Forward\n",
    "        log_alpha = np.empty((N, H))\n",
    "        log_alpha[:, 0] = logsumexp(log_forward_last[:, None] + logA, axis=0)\n",
    "        for t in range(1, H):\n",
    "            # log_alpha[:,t] = logB[:,t] + logsumexp_i( log_alpha[i,t-1] + logA[i,:] )\n",
    "            log_alpha[:, t] = logsumexp(log_alpha[:, t-1][:, None] + logA, axis=0)\n",
    "\n",
    "        log_alpha -= logsumexp(log_alpha, axis=0)\n",
    "        self.forecast_forward = np.exp(log_alpha)\n",
    "        self.state_forecasts = np.argmax(self.forecast_forward, axis=0)\n",
    "\n",
    "        # Keep original targets if trend adjustments are needed\n",
    "        if self.trend is not None:\n",
    "            # orig_targets = {col: self.orig_targets[col].tolist() for col in self.trend.keys()}\n",
    "\n",
    "            # --- Trend re-addition ---\n",
    "            trend_forecasts = {}\n",
    "            for col in self.trend:\n",
    "                if self.trend[col] == \"linear\":\n",
    "                    if (self.cps is not None) and (col in self.cps):\n",
    "                        trend_forecasts[col]= forecast_trend(model = self.trend_models[col], H=H, start=self.len, degree=self.pol[col], breakpoints=self.cps[col])\n",
    "                    else:\n",
    "                        trend_forecasts[col]= forecast_trend(model = self.trend_models[col], H=H, start=self.len, degree=self.pol[col])\n",
    "                elif self.trend[col] == \"ets\":\n",
    "                    trend_forecasts[col] = np.array(self.trend_models[col].forecast(H))\n",
    "\n",
    "        for t in range(H):\n",
    "            if exog is not None:\n",
    "                exo_inp = exog[t].tolist()\n",
    "            else:\n",
    "                exo_inp = [1] if self.cons else []\n",
    "\n",
    "            # Construct lag inputs\n",
    "            lags = []\n",
    "            for col in y_dict.keys():\n",
    "                ys = [y_dict[col][-x] for x in self.lags[col]]\n",
    "                lags.extend(ys)\n",
    "\n",
    "            # Lag transforms\n",
    "            transform_lag = []\n",
    "            if self.lag_transform is not None:\n",
    "                for target, funcs in self.lag_transform.items():\n",
    "                    series_array = np.array(y_dict[target])\n",
    "                    for func in funcs:\n",
    "                        transform_lag.append(func(series_array, is_forecast=True).to_numpy()[-1])\n",
    "\n",
    "            inp = np.array(exo_inp + lags + transform_lag)\n",
    "            state_preds = {col: np.zeros(self.N) for col in self.target_col}\n",
    "\n",
    "            for j in range(self.N):\n",
    "                mus = inp @ self.coeffs[j]  # shape: (n_targets,)\n",
    "                for i, col in enumerate(self.target_col):\n",
    "                    state_preds[col][j] = mus[i]\n",
    "\n",
    "\n",
    "            # Weighted prediction per target\n",
    "            for col in self.target_col:\n",
    "                pred = np.sum(self.forecast_forward[:, t] * state_preds[col])\n",
    "                forecasts[col].append(pred)\n",
    "                y_dict[col].append(pred)\n",
    "\n",
    "        # trend adjustments\n",
    "        if self.trend is not None:\n",
    "            for col in self.trend.keys():\n",
    "                forecasts[col] += trend_forecasts[col]\n",
    "\n",
    "        # --- Post-processing transforms ---\n",
    "        if self.season_diff is not None:\n",
    "            for col in self.season_diff.keys():\n",
    "                forecasts[col] = invert_seasonal_diff(self.orig_d[col], forecasts[col], self.season_diff[col])\n",
    "\n",
    "        if self.diffs is not None:\n",
    "            for col in self.diffs.keys():\n",
    "                forecasts[col] = undiff_ts(self.origs[col], forecasts[col], self.diffs[col])\n",
    "\n",
    "        if self.box_cox is not None:\n",
    "            for col in self.box_cox:\n",
    "                if self.box_cox[col]:\n",
    "                    forecasts[col] = back_box_cox_transform(\n",
    "                        y_pred=forecasts[col],\n",
    "                        lmda=self.lamda[col],\n",
    "                        shift=self.is_zero[col],\n",
    "                        box_cox_biasadj=self.biasadj[col]\n",
    "                    )\n",
    "\n",
    "        return forecasts\n",
    "    \n",
    "    def get_param_spec(self):\n",
    "        \"\"\"\n",
    "        Get the model parameters: coefficients, stds, initial state distribution, transition matrix.\n",
    "        \"\"\"\n",
    "\n",
    "        T = len(self.y)\n",
    "        p = sum(self.coeffs.shape)\n",
    "        df = T - p  # degrees of freedom\n",
    "        # create empty NxY.shape[1]x1 array to store results\n",
    "        results = np.empty((self.y.shape[1], self.N, 4, self.coeffs.shape[1]))\n",
    "        for s in range(self.N):\n",
    "            xw = self.posterior[s][:, None] * self.X\n",
    "            yw = self.posterior[s][:, None]* self.y\n",
    "            yw_fit = self.coeffs[s, :, :].T @ xw.T\n",
    "            resid = yw - yw_fit.T\n",
    "\n",
    "            res_cov = (resid.T @ resid)/df # calculate residual covariance matrix\n",
    "            cov_diag = res_cov.diagonal()\n",
    "            w_d = np.diag(self.posterior[s]) # weight diagonal matrix\n",
    "            XtWX_1 = np.linalg.inv(self.X.T @ w_d @ self.X)\n",
    "            var_b = cov_diag[:, None, None] * XtWX_1[None, :, :]\n",
    "\n",
    "            se_b = np.sqrt(var_b)\n",
    "            t_s = self.coeffs[s, :, :].T / np.diagonal(se_b, axis1=1, axis2=2)\n",
    "\n",
    "            from scipy.stats import t\n",
    "            p_values = (1 - t.cdf(np.abs(t_s), df)) * 2\n",
    "            for i in range(self.y.shape[1]):\n",
    "                results[i, s, 0, :] = self.coeffs[s, :, i]   # (13,)\n",
    "                results[i, s, 1, :] = np.diagonal(se_b, axis1=1, axis2=2)[i]                     # (13,)\n",
    "                results[i, s, 2, :] = t_s[i]                         # (13,)\n",
    "                results[i, s, 3, :] = p_values[i]                    # (13,)\n",
    "\n",
    "        arrs = np.vstack(\n",
    "            [\n",
    "                np.concatenate([results[i, j, :, :].T for j in range(results.shape[1])], axis=1)\n",
    "                for i in range(self.y.shape[1])\n",
    "            ]\n",
    "        )\n",
    "        multi_vr = pd.DataFrame(arrs, index=self.col_names*self.y.shape[1]).reset_index().rename(columns={\"index\": \"variable\"})\n",
    "\n",
    "        grp = [f\"Results for equation {i}\" for i in self.target_col\n",
    "                                for _ in range(self.coeffs.shape[1])]\n",
    "        multi_vr.columns.get_loc('variable') + 1\n",
    "        multi_vr.insert(loc=1, column='group', value=grp)\n",
    "        self.param_spec_df = multi_vr\n",
    "\n",
    "        state_probs = pd.DataFrame(pd.Series(self.predict_states()).value_counts(normalize=True).sort_index())\n",
    "        state_probs.index = [f\"regime_{i+1}\" for i in range(self.N)]\n",
    "        state_probs[state_probs.select_dtypes(include='number').columns] = (\n",
    "            state_probs.select_dtypes(include='number')\n",
    "                .applymap(lambda x: f\"{x:.3f}\")\n",
    "        )\n",
    "        self.regime_probs = state_probs.T\n",
    "\n",
    "        tm = pd.DataFrame(self.A)\n",
    "        tm.columns = [f\"regime_{i+1}\" for i in range(self.A.shape[1])]\n",
    "        tm.index = [f\"regime_{i+1}\" for i in range(self.A.shape[1])]\n",
    "\n",
    "        tm_df = tm.reset_index().rename(columns={\"index\": \"\"})\n",
    "        tm_df[tm_df.select_dtypes(include='number').columns] = (\n",
    "            tm_df.select_dtypes(include='number')\n",
    "                .applymap(lambda x: f\"{x:.3f}\"))\n",
    "        self.tm_df = tm_df\n",
    "\n",
    "        how_fit = {f\"log-likelihood\": round(self.loglik, 1), \"AIC\": round(self.aic, 1), \"BIC\": round(self.bic, 1)}\n",
    "\n",
    "        ## other information\n",
    "        data_info = {\"dep. Variable\": self.target_col, \"n_obs\": len(self.y), \"df_model\": sum(self.coeffs.shape)}\n",
    "        data_info = pd.DataFrame(data_info)\n",
    "        data_info.iloc[0, 0] = \", \".join(self.target_col)\n",
    "        data_inf = pd.DataFrame(data_info.T.iloc[:, 0]).reset_index().rename(columns={\"index\": \"\"})\n",
    "        self.data_inf = data_inf.rename(columns={0: \" \"})\n",
    "\n",
    "        data_fit = pd.DataFrame(how_fit, index=[0]).T.reset_index().rename(columns={\"index\": \"\"})\n",
    "        self.data_fit = data_fit.rename(columns={0: \" \"})\n",
    "\n",
    "        def create_cov_df(s):\n",
    "            covs = pd.DataFrame(self.covs[s])\n",
    "            covs.columns = [t for t in self.target_col]\n",
    "            covs.index = [t for t in self.target_col]\n",
    "            covs_df = covs.reset_index().rename(columns={\"index\": \"\"})\n",
    "            covs_df[covs_df.select_dtypes(include='number').columns] = (\n",
    "                covs_df.select_dtypes(include='number')\n",
    "                    .applymap(lambda x: f\"{x:.3f}\")) \n",
    "            return covs_df\n",
    "        \n",
    "        self.cov_matrixes = [(f\"Covariance matrix for regime {i}\", create_cov_df(i)) for i in range(self.N)]\n",
    "\n",
    "    def summary(self, font_size=10, row_padding=4, col_labels_padding=4, cov_font_size=350):\n",
    "        \"\"\"\n",
    "        Generate a summary table of the model results using gt.\n",
    "        Args:\n",
    "            font_size (str): Font size for the summary table text.\n",
    "            row_padding (str): Padding for the data rows in the summary table.\n",
    "            col_labels_padding (str): Padding for the column labels in the summary table.\n",
    "            cov_font_size (int): Font size for the covariance matrix tables.\n",
    "        \"\"\"\n",
    "        self.get_param_spec()\n",
    "        gt_maint = make_var_gt_regimes(self.param_spec_df, n_regimes=self.N)\n",
    "\n",
    "        gt_final = inject_header_table_groups(\n",
    "            gt_maint,\n",
    "            columns=[\n",
    "                [(\"Regime probabilities\", self.regime_probs, True), (\"Transition probabilities\", self.tm_df, True)],\n",
    "                [(\"Data\", self.data_inf, False), (\"Diagnostics\", self.data_fit, False)]\n",
    "            ],\n",
    "            subtitle_text=\"Regime-switching hidden markov VAR model results\"\n",
    "        ).tab_options(\n",
    "        table_font_size=f\"{font_size}px\",          # shrink all text\n",
    "        data_row_padding=f\"{row_padding}px\",          # tighten row height\n",
    "        column_labels_padding=f\"{col_labels_padding}px\"  # tighten header band\n",
    "        )\n",
    "\n",
    "        return gt_final\n",
    "    \n",
    "    def cov_results(self, cov_font_size=350):\n",
    "        \"\"\"\n",
    "        Generate covariance matrix tables for each regime using gt.\n",
    "        Args:\n",
    "            cov_font_size (int): Font size for the covariance matrix tables.\n",
    "        \"\"\"\n",
    "        return cov_table(self.cov_matrixes, font_size_px=cov_font_size)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
